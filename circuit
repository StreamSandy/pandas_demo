2:11 PM
>>>>>>>>>>>>>>>>>

Got it â€” letâ€™s break this down systematically.

Youâ€™re writing a **zero-shot instruction prompt** whose goal is:
âž¡ï¸ Guide the most capable AI agent to deeply analyze the internal Python library `ent_pycore`, understand its functions, and propose logic/designs that can be built using it.

---

## ðŸ”Ž Analysis of Your Current Prompt

**Your version:**

> Pip install ent_pycore.
> We have ent_pycore inbuilt library in our org, and I want to analyse all the functions in the library ent_pycore and help me to achieve this technically and see what all logic is possible with the internaly python library.
> Add if I have missed anything like Like-a Google engineer do or someone work in billion dollar startup.
> And I want you to think like high performing software engineer.

### Characteristics

1. **Direct but unstructured** â€“ youâ€™re stating the goal, but not guiding *how* the agent should analyze, organize, or present results.
2. **High ambition tone** â€“ asking it to â€œthink like a Google engineer / billion-dollar startupâ€ sets quality expectations, but itâ€™s vague.
3. **Task ambiguity** â€“ "analyze all functions" could mean: generate documentation, usage examples, dependencies, design best practices, or even building new patterns (circuit breaker, retry, etc.).
4. **Missing structured output request** â€“ you donâ€™t specify how the answer should be organized (tables, code snippets, summaries, examples).
5. **No reasoning chain request** â€“ you want deep reasoning, but the current text doesnâ€™t explicitly demand step-by-step thought.

---

## âš¡ Suggestions to Improve Prompt

Hereâ€™s how to make it sharper for **zero-shot instruction tasks**:

### 1. Explicit Task Breakdown

Instead of just â€œanalyze all functions,â€ tell the agent *how* to analyze:

* Extract all function names and signatures.
* Categorize by purpose (e.g., networking, retry, circuit-breaker, monitoring).
* Show possible use cases.
* Suggest best practices for enterprise-scale systems.

### 2. Require Reasoning Chain

Ask the agent to:

* Explain reasoning behind classification.
* Compare different function design patterns.
* Show why a certain function is suitable for a use case.

### 3. Structured Output Format

Specify you want output in sections/tables, for clarity:

* **Function Catalog** (name, signature, short description).
* **Category/Use Case Mapping**.
* **Design Patterns Possible** (e.g., circuit breaker, retry logic, caching).
* **Code Examples** (with explanations).
* **Improvement Suggestions** (what is missing, how a Google-level engineer would extend it).

### 4. Strengthen the "high-performance engineer" framing

Instead of â€œthink like Google engineer,â€ say:

> â€œThink like a senior engineer at a FAANG-scale company: prioritize scalability, fault-tolerance, and clean abstractions.â€

---

## ðŸ“ Refined Prompt (Improved Version)

Hereâ€™s a stronger **zero-shot instruction prompt** you could use:

---

**Prompt:**
You are acting as a **high-performing senior software engineer at a FAANG-scale company**.

We have an internal Python library called **`ent_pycore`**. Your task is to:

1. **Analyze all functions** in the library and extract their purpose, signatures, and dependencies.
2. **Categorize functions** by use case (e.g., retries, error handling, monitoring, networking, data processing).
3. **Propose enterprise-grade design patterns** (e.g., circuit breaker, retry with backoff, observability hooks) that can be built using these functions.
4. **Show code examples** that demonstrate how to combine these functions into robust, production-ready solutions.
5. **Explain reasoning step-by-step**, highlighting trade-offs and why a design is scalable or resilient.
6. **Suggest improvements** or missing capabilities â€” what else a world-class engineering team would add.

**Output format:**

* Section 1: Function Catalog (table of functions with short description).
* Section 2: Categorization & Use Cases.
* Section 3: Proposed Designs & Patterns (with code).
* Section 4: Step-by-Step Reasoning.
* Section 5: Improvement & Future Enhancements.

---

ðŸ‘‰ This way, the agent knows:

* the **goal** (analyze & apply functions),
* the **method** (structured reasoning),
* the **output style** (tables + code + explanation),
* the **quality bar** (world-class engineer thinking).

---

Would you like me to **rewrite your exact original wording** into this improved form (keeping your style but sharpening structure), or do you prefer a **more concise minimalistic version** you can drop directly into your system as a one-liner?


Short answer: I canâ€™t install your internal package here or reach your private registry. I also canâ€™t see `ent_pycore` unless you share the code (repo/zip/wheel) or paste docs/stubs.

That said, I can still help in two solid ways:

## Option A â€” You provide artifacts, I analyze now

Share **any one** of these and Iâ€™ll proceed immediately:

* The repo (or a zip/tarball) of `ent_pycore`
* A wheel (`.whl`) or sdist (`.tar.gz`)
* Generated docs or `.pyi` stubs
* Even a single module you care about most

### My analysis plan (once I have it)

1. **Inventory:** Parse modules â†’ list functions/classes, signatures, docstrings.
2. **Categorize:** Group by capability (I/O, retries, CB, caching, telemetry, auth, utils).
3. **Pattern map:** Identify what enterprise patterns are already enabled (circuit breaker, exponential backoff, bulkhead, idempotency, tracing) and whatâ€™s missing.
4. **Usage recipes:** Produce production-grade examples (with error handling, logging, metrics, configuration via env/flags, test doubles).
5. **Gaps & proposals:** Concrete PR-style suggestions (APIs to add, breaking-change risks, test matrix, SLO/observability hooks).

Iâ€™ll deliver:

* A **Function Catalog table**
* **Design patterns** you can implement today using `ent_pycore`
* **Sample code** (prod-ready) + **pytest** tests
* A short **â€œWhat a FAANG-level team would add nextâ€** section

## Option B â€” Run this locally, then paste results back

If you can run Python in your environment, this script will generate a JSON inventory I can analyze without needing the code itself:

```python
# save as scan_ent_pycore.py and run: python scan_ent_pycore.py
import importlib, inspect, json, pkgutil, sys

PKG = "ent_pycore"
mod = importlib.import_module(PKG)

def walk(pkg_name):
    seen = set()
    for m in pkgutil.walk_packages(mod.__path__, mod.__name__ + "."):
        name = m.name
        if name in seen: 
            continue
        seen.add(name)
        try:
            module = importlib.import_module(name)
        except Exception as e:
            yield {"module": name, "error": repr(e)}
            continue
        items = []
        for n, obj in inspect.getmembers(module):
            if inspect.isfunction(obj) or inspect.isclass(obj):
                try:
                    sig = str(inspect.signature(obj))
                except Exception:
                    sig = "(signature unavailable)"
                doc = (inspect.getdoc(obj) or "").splitlines()[0:3]
                items.append({
                    "name": n,
                    "kind": "class" if inspect.isclass(obj) else "function",
                    "signature": sig,
                    "doc_head": " ".join(doc)
                })
        yield {"module": name, "items": items}

result = list(walk(PKG))
print(json.dumps(result, indent=2))
```

* Run it where `ent_pycore` is installed.
* Paste the JSON output here. Iâ€™ll turn it into the Function Catalog + patterns + code examples.

## Option C â€” Prompt for an internal agent that *can* install

If you have an internal runner/agent with network access, use this zero-shot prompt there:

> **Role:** Senior Staff Engineer.
> **Task:** Install and analyze the internal Python library `ent_pycore`.
> **Steps:**
>
> 1. `pip install ent_pycore` (using the corporate index if required).
> 2. Enumerate modules, functions, classes, and signatures; capture first-line docstrings.
> 3. Categorize capabilities (retries/backoff, circuit breaker, caching, I/O, telemetry, auth, utils).
> 4. Propose enterprise patterns implementable with current APIs (circuit breaker, bulkhead, idempotency, request hedging, structured logging/metrics/traces).
> 5. Produce production-ready Python examples + pytest tests for each pattern.
> 6. Identify gaps and propose API changes (backwards-compatible where possible), include observability & configurability (env vars/flags).
>    **Output format:**
>
> * Section 1: Function Catalog (table: module, name, kind, signature, doc_head).
> * Section 2: Category â†’ Functions mapping.
> * Section 3: Design patterns with code + tests.
> * Section 4: Reasoning & trade-offs.
> * Section 5: Gaps & roadmap (MVP, nice-to-have, risks).
>   **Quality bar:** FAANG-scale reliability, clarity, and testability; align with SLOs, tracing, and error-budget thinking.

---

Pick the path that fits your environment. If you can share the repo/zip or the JSON inventory, Iâ€™ll dive straight into the analysis and produce the catalog, designs, and code.




9:38
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Got itâ€”your goal is a **zero-shot, do-it-all prompt** that reliably guides a capable agent to (1) understand your repo + `ent_pycore`, (2) design a production-grade **circuit breaker** for critical LLM calls, and (3) deliver code + tests + rationale at a â€œGoogle-ish/billion-$ startupâ€ bar.

Below is a tight upgrade: first, a quick critique so you know whatâ€™s missing, then a **drop-in prompt template** you can paste into your system with strong structure, guardrails, and outputs.

---

# What to improve (fast critique)

1. **Ambiguity about inputs & scope**
   â€œ30+ scriptsâ€ and â€œanalyze all functionsâ€ is open-ended. Agents wander. Anchor scope: target directories, file patterns, key entrypoints.

2. **Tooling/Capabilities not explicit**
   If the agent can inspect files, run code, or only reason statically? Say it. If it must infer `ent_pycore` APIs, tell it how (read docs, grep usage).

3. **Outcome format not enforced**
   You want design, code, and tests. Force **structured sections** + acceptance criteria.

4. **Prod-readiness criteria missing**
   Define SLIs/SLOs, failure modes, fallback policy, telemetry, config flags, and rollback plan.

5. **Test strategy underspecified**
   Require unit + integration tests (happy/timeout/error/open/half-open/closed), fault injection, and deterministic seed.

6. **Reviewability**
   Ask for a **PR-style patch** (diffs), `README.md` updates, and CHANGELOG notes.

---

# Upgraded Zero-Shot Prompt (paste this as your instruction)

> **Role**: You are a senior platform/SRE-minded software engineer designing a production-grade **circuit breaker** for mission-critical LLM API calls. Think rigorously, write reviewable code, and justify decisions with measurable reliability goals.
>
> **Context**:
>
> * Codebase: ~30+ scripts in a GitHub repo (Python).
> * Internal library: `ent_pycore` (installed via `pip install ent_pycore`). Treat it as first-class; prefer its primitives over ad-hoc code.
> * Objective: Protect user experience during LLM outages/timeouts while meeting reliability goals and preserving throughput under normal load.
>
> **Repository Scope to Analyze**:
>
> * Focus on files under `src/` and `services/` containing LLM calls (search terms: `openai`, `llm`, `completion`, `chat`, `inference`, `client.*create`, `requests.post` to model endpoints).
> * Identify: call sites, retry logic, timeouts, error handling, and any existing rate limiting/backoff.
> * Map the call-graph for top 2 critical entrypoints (by traffic or business priority).
>
> **What you must deliver** (strict structure):
>
> 1. **Executive Summary (â‰¤10 bullets)**
>
>    * Problem statement, constraints, proposed pattern (circuit breaker), expected impact.
> 2. **Requirements & Reliability Goals**
>
>    * Define SLIs (success rate, end-to-end latency p90/p95, fallback hit-rate).
>    * Propose SLOs (e.g., success â‰¥ 99.5%, p95 latency â‰¤ 2.5s under nominal load).
>    * Non-functionals: observability, configurability, blast-radius limits.
> 3. **Design (with diagrams-in-text)**
>
>    * Circuit breaker states (Closed, Open, Half-open) and transitions.
>    * **Trip conditions** (e.g., rolling window error-rate > X%, consecutive timeouts > N, upstream 5xx > M).
>    * **Recovery probe** policy (half-open sample size, jittered probe interval).
>    * **Fallbacks** in priority order (cached response â†’ narrowed model â†’ stub/template â†’ user-facing graceful degradation).
>    * **Configuration** (env vars or config file): timeouts, thresholds, window size, max concurrent probes, allowlist endpoints.
>    * **Thread/async model** and concurrency safety.
> 4. **Implementation Plan (PR-Checklist)**
>
>    * New module(s) (paths, filenames) and changes to existing files.
>    * Backward compatibility and kill-switch/feature flag (`CB_ENABLED`).
>    * Telemetry events (names, fields) and log levels.
> 5. **Code**
>
>    * A self-contained `circuit_breaker.py` (or package) using **`ent_pycore`** where appropriate.
>    * Adapters/wrappers for current LLM clients (minimal surface change at call sites).
>    * Config loader + sane defaults.
> 6. **Tests**
>
>    * Unit tests covering: Closedâ†’Open, Openâ†’Half-open, Half-openâ†’Closed; timeout bursts; mixed failures; fallback correctness; concurrency.
>    * Integration test with a stubbed LLM endpoint (fast, deterministic).
>    * Property-style test for rolling-window math.
> 7. **Operations**
>
>    * Dashboards (metrics list), alerts (burn-rate style), SLO doc snippet.
>    * Runbook: how to enable/disable, tune thresholds, interpret metrics, rollback plan.
> 8. **STAR Rationale**
>
>    * Situation, Task, Actions (trade-offs), Results (expected metrics deltas, e.g., p95 â†“, error budget burn â†“).
> 9. **Patch Output**
>
>    * Present changes as a **single PR-style diff** (filenames + unified diff).
>    * Include `README.md` section â€œCircuit Breaker Overviewâ€ and `CHANGELOG.md` entry.
>
> **Constraints & Guardrails**:
>
> * **No breaking public APIs** unless adapter provided.
> * Prefer **idempotent** operations; document any non-idempotent side-effects.
> * All constants are configurable (env or `config.yaml`), with safe defaults.
> * Add **metrics** (counter/gauge/histogram) and **structured logs** at state transitions.
> * Write code in **Python 3.10+**, type-annotated, lint-clean (ruff/flake8), and documented (docstrings).
>
> **Assumptions re: `ent_pycore`** (adapt if real APIs differ):
>
> * If `ent_pycore` has utilities for:
>
>   * **circuit breakers**, **rate limiters**, **rolling windows**, **metrics/logging**, or **feature flags**, **use them** and show how.
>   * If unknown, **introspect by reading its package modules** and **grep existing repo usage**; then state the chosen functions/classes and why.
>
> **Exact Output Format** (important):
>
> * Use the section headers 1â€“9 above.
> * All code blocks must be runnable and complete.
> * Tests must run via `pytest -q`. Provide a short command list to execute locally.
>
> **Acceptance Criteria** (the work is â€œdoneâ€ when):
>
> * Unit + integration tests pass locally.
> * A demo script shows tripping and recovery behavior deterministically.
> * Metrics names + labels are listed and mapped to dashboards/alerts.
> * A kill-switch exists and works (`CB_ENABLED=false`).
> * Risk analysis + rollback steps are documented.

---

## Ready-to-use â€œmicroâ€ variant (if you need a shorter prompt)

> Design and implement a production-grade **circuit breaker** for our LLM calls in this Python repo, prioritizing: (1) reliability SLOs, (2) graceful degradation, (3) observability, (4) configurability. Analyze `src/` and `services/` for LLM call sites; prefer **`ent_pycore`** primitives. Deliver: (1) executive summary, (2) SLIs/SLOs, (3) design with states/thresholds/fallbacks, (4) PR-style patch (code + tests + config), (5) ops guide (metrics, alerts, runbook), (6) STAR rationale. Code must be typed, lint-clean, pytest-covered, include a kill-switch, and expose metrics + logs at state transitions.

---

## Optional code skeleton (helps the agent land consistently)

```python
# src/platform/circuit_breaker.py
from dataclasses import dataclass
from typing import Callable, Any, Tuple
import time
# from ent_pycore.observability import metrics, log
# from ent_pycore.controls import RollingWindow, FeatureFlag, CircuitState  # hypothetical

@dataclass
class CBConfig:
    error_rate_threshold: float = 0.25
    min_samples: int = 50
    rolling_window_sec: int = 60
    open_cooldown_sec: int = 20
    half_open_probe_count: int = 5
    request_timeout_sec: float = 8.0
    enabled: bool = True

class CircuitBreaker:
    # implement Closed/Open/Half-open with rolling window and timed cooldown
    ...
```

Tests should simulate: steady success, burst failuresâ†’trip, cooldownâ†’probeâ†’recover, and verify metrics/logs.

---

## Quick reasoning chain the agent should follow (baked into the structure above)

1. Identify hot paths â†’ quantify failure/latency patterns.
2. Choose thresholds grounded in SLIs/SLOs.
3. Implement breaker with safe defaults + feature flag.
4. Wrap LLM calls; add fallbacks for degraded mode.
5. Add metrics/logs; write tests (unit/integration/fault-injection).
6. Provide PR diff + runbook + STAR results.

---

If you want, share a couple of filenames that actually call the LLM (e.g., `services/llm_client.py`, `src/handlers/generate.py`) and Iâ€™ll tailor the design thresholds and a concrete wrapper that uses your real `ent_pycore` primitives.


9:15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

awesome problem. hereâ€™s a sharper, production-ready way to brief a capable agentâ€”plus a filled-in version for your two use cases, and a quick sanity checklist.

# 1) whatâ€™s missing in your current prompt (and why it matters)

* **Clear objective & scope**: say exactly what â€œdoneâ€ looks like (PRs, tests, dashboards, runbook).
* **Assumptions policy**: forbid clarifying questions; require the agent to state assumptions & proceed.
* **Non-functional constraints**: SLOs, latency budgets, token/cost budgets, concurrency limits, PII/compliance.
* **Observability & rollback**: metrics, alerts, chaos tests, feature flags, rollback steps.
* **Testing matrix**: unit, integ, fault-injection, load, resilience.
* **STAR narrative**: ask for an explicit Situationâ†’Taskâ†’Actionâ†’Result write-up.
* **Review criteria**: a checklist/rubric the PE team can use to accept/reject.
* **Change safety**: canary plan, blast radius, and backout commands.

---

# 2) zero-shot instruction prompt (template)

> copy/paste and fill <> where needed. this keeps the agent decisive and output-oriented.

**ROLE & MINDSET**
Act as a senior software engineer/SRE at a high-scale company. Do not ask questions. If information is missing, make conservative, clearly labeled assumptions and proceed.

**OBJECTIVE**
Design and implement a **circuit breaker** for the specified services to protect user experience, control LLM spend, and prevent cascading failures. Deliver production-grade code and an end-to-end plan.

**CODEBASE CONTEXT**

* Repo: `<org>/<repo>` (monorepo; primary languages: `<languages>`)
* Services in scope: `<service names / paths>`
* Environment: Dev/UAT/Prod with feature flags via `<flag system>`
* Dependencies: LLM API `<provider(s)>`, DB `<type>`, queue `<type>`

**CONSTRAINTS & SLOs**

* User-facing latency P95 â‰¤ `<X ms>`; error budget `<Y %>` per 30 days
* LLM spend: max `<tokens/min>` and `<$/day>`; shed load if exceeded
* Timeouts: `<client timeout>`; retries: capped with exponential backoff + jitter
* Concurrency caps: `<value>` per instance; global rate limit `<value>`
* PII/compliance: no PII in logs; enable structured logging

**DELIVERABLES**

1. **Design doc** (â‰¤2 pages) with: state machine, thresholds, fallback paths, backpressure, and recovery policy. Include Mermaid diagram.
2. **Implementation**: PR(s) that add circuit breaker middleware/util(s) + configuration (env-driven).
3. **Tests**: unit + integration + fault-injection (simulate timeouts, 5xx, time-window failure %).
4. **Observability**: metrics names, dashboards, and alert rules.
5. **Runbook**: ops steps, feature-flag toggles, rollback commands.
6. **STAR** summary of the work (Situation, Task, Actions, Results).
7. **Review note for PE**: risks, tradeoffs, and how to validate locally & in canary.

**EVALUATION / ACCEPTANCE CRITERIA**

* Breaker transitions correctly (Closedâ†’Openâ†’Half-Open) using rolling window failure rate or consecutive errors.
* No request storms: retries are bounded, with jitter; concurrency & rate limits enforced.
* For write-paths: durability guaranteed (queue + idempotency key); zero data loss on transient DB outage.
* Observability: metrics and alerts fire in controlled chaos tests; dashboards show breaker state & rates.
* Cost guardrails: breaker opens on spend spikes or token rate breach.
* All tests pass; canary shows â‰¤ `<delta>` regression on latency/error rate.

**PROCESS**

* Make and list assumptions.
* Produce: (a) design doc, (b) code diff, (c) tests, (d) ops assets, (e) STAR summary.
* Output everything inline, with code blocks and filenames.

---

# 3) filled-in prompt for your two use cases

**ROLE & MINDSET**
Act as a senior SWE/SRE. Do not ask questions. If details are missing, assume safe defaults and record them.

**IN-SCOPE USE CASES**
A) **LLM Completion (read path)** â€“ Core UX; timeouts/outages break workflows and burn error budget.
B) **Feedback/Analytics Write (write path)** â€“ Compliance/audit-critical; DB outages risk data loss.

**OBJECTIVE**
Design and implement circuit breakers for A & B that:

* protect UX and SLOs,
* cap cost & token usage,
* prevent cascading failures,
* preserve/queue writes and alert on failures.

**CODEBASE**

* Repo: `<org>/genai-platform`
* Services: `api/llm_gateway` (TypeScript/Node) and `svc/telemetry_writer` (Python/FastAPI)
* Env: Dev/UAT/Prod; feature flags via `FF_CIRCUIT_BREAKER_{LLM,WRITE}`

**KEY CONSTRAINTS**

* SLOs: P95 latency â‰¤ 1200 ms for LLM completions; monthly availability â‰¥ 99.9%
* LLM budgets: â‰¤ 60k tokens/min per tenant; daily spend cap `<$>` configurable
* Timeouts: upstream LLM 2.0 s; client 2.5 s; retry: max 2 with exp backoff (100â€“600 ms) + jitter
* Concurrency: 64 per instance; global qps per tenant set via token bucket
* DB writes: at-least-once via durable queue (e.g., SQS/Kafka) with idempotency keys
* No PII in logs; trace with `trace_id`, `tenant_id`, `breaker_state`

**DELIVERABLES**

1. **Design doc** (â‰¤2 pages) including Mermaid diagrams of both breakers.
2. **Code**

   * `api/llm_gateway`: CircuitBreaker middleware (Closed/Open/Half-Open), fallback: cached/template response for non-critical prompts, plus shed-load when cost/token budgets breach.
   * `svc/telemetry_writer`: CircuitBreaker guarding DB client; on Open, enqueue to durable queue; consumer drains with idempotency & DLQ.
3. **Tests**

   * Unit: state transitions, rolling window failure %, budget gating, retry caps.
   * Integration: fault-injection (LLM 5xx/timeout spikes; DB connection refused).
   * Load/chaos: verify alerts, ensure no thundering herd, measure P95.
4. **Observability** (Prometheus/Grafana or equivalent)

   * Metrics: `breaker_state{service}`, `breaker_open_total`, `failure_rate`, `timeout_total`, `retry_total`, `queued_writes`, `dlq_depth`, `token_rate`, `spend_rate_usd_per_min`
   * Alerts:

     * LLM: breaker Open > 1 min OR failure_rate â‰¥ 50% over 2 min OR token_rate/spend breach for 3 consecutive mins.
     * Writes: queue lag > 2 min OR DLQ depth > 0 OR Open > 30 s.
5. **Runbook**: flag toggles, raising limits, draining DLQ, recovering DB, backout steps.
6. **STAR**: concise narrative of the problem and results.
7. **PE Review Note**: risk, tradeoffs (e.g., hedging cost vs latency), and validation commands.

**ACCEPTANCE CRITERIA**

* LLM: Under induced 50% upstream failure for 2 min, breaker opens within 5â€“10 s, caps retries, P95 â‰¤ 1.5Ã— baseline, error rate â‰¤ budget, spend spike is contained.
* Writes: On DB outage for 5 min, 0 lost records; queue builds then drains to 0 within 10 min post-recovery; duplicates suppressed via idempotency key.
* All tests pass; dashboards accurate; alerts verified in chaos runs.

**OUTPUT FORMAT**
Produce in this order:

1. Assumptions list
2. Design doc (with Mermaid diagrams)
3. Code diffs (filenames + code blocks)
4. Test code & how to run
5. Observability spec (metrics, dashboards JSON if applicable)
6. Runbook & rollback
7. STAR summary
8. PE review checklist mapping

---

# 4) pocket designs (for the agent to implement)

**LLM breaker (TypeScript sketch)**

```ts
// api/llm_gateway/middleware/circuitBreaker.ts
type State = "CLOSED" | "OPEN" | "HALF_OPEN";
export function createBreaker(opts: {
  failureThresholdPct: number; // e.g., 50 over window
  windowSecs: number;          // e.g., 20
  openCooldownMs: number;      // e.g., 5000
  halfOpenProbe: number;       // e.g., 5 requests
  maxRetries: number;          // e.g., 2 w/ jittered backoff
  tokenBudgetPerMin: number;   // per-tenant
  spendBudgetPerMinUSD: number;
}) { /* implement rolling counters, budgets, and state machine */ }
```

**Write-path breaker + queue (Python sketch)**

```python
# svc/telemetry_writer/breaker.py
class CircuitBreaker:
    # CLOSED -> OPEN (on failure rate or consecutive errors)
    # OPEN -> HALF_OPEN after cooldown
    # HALF_OPEN -> CLOSED on probe success; else OPEN
    ...
# On OPEN: push record to durable queue with idempotency_key = sha256(tenant_id|event_id|ts)
# Consumer: reads queue, writes with upsert on idempotency_key; on permanent failure -> DLQ.
```

**Mermaid (example)**

```mermaid
stateDiagram-v2
    [*] --> CLOSED
    CLOSED --> OPEN: failure_rate>=threshold OR budget_breach
    OPEN --> HALF_OPEN: cooldown elapsed
    HALF_OPEN --> CLOSED: probeSuccess>=N
    HALF_OPEN --> OPEN: probeFailure>0
```

---

# 5) review checklist (paste into PR)

* [ ] Assumptions documented; no open questions.
* [ ] Breaker thresholds, windows, cooldowns are config-driven.
* [ ] Retries capped + jitter; no unbounded fan-out.
* [ ] Cost/token budgets enforced; load-shedding in place.
* [ ] Write-path durability: queue, idempotency, DLQ; zero data loss in tests.
* [ ] Metrics/alerts wired; dashboards render breaker state & lag.
* [ ] Chaos/fault-injection scripts included; alerts verified.
* [ ] Feature-flagged; canary & rollback steps documented.

If you want, I can now generate the exact design doc + code stubs + tests following this prompt format.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Awesome brief. Hereâ€™s a battle-tested, zero-shot prompt you can drop into your agentâ€”plus add-ons and a tiny example of the expected answer shape. Itâ€™s written to elicit Google-level design depth without forcing the model to reveal private reasoning.

---

# Zero-Shot Instruction Prompt (Final)

**Role & Context**
You are a principal software engineer designing **production-grade circuit breakers** for a large LLM application with ~30 Python scripts (micro-tools, services, CLIs). Assume an elite engineering bar (Google/billion-dollar startup).

**Inputs**

* Repository snapshot (files, tree, or links)
* Runtime context: Python 3.x, Streamlit/FastAPI workers, LLM provider(s) (e.g., Gemini/OpenAI), pptx rendering, caching/queue layers (if any)

**Primary Objective**

1. **Discover the top two business-critical use cases** in the repo and justify why theyâ€™re top priority (traffic, revenue/user impact, SLO/SLA/error-budget risk, fan-out).
2. **Design circuit breakers for each use case** with detailed, production-ready guidance.

**Deliverables (must follow exactly)**
A. **Birdâ€™s-eye map (1â€“2 pages max)**

* System diagram of major scripts/services and call graph to external dependencies (LLM API, storage, DB, network).
* Hot paths and shared libraries that concentrate risk.
* Error taxonomy: timeouts, 5xx/429, transport errors, parser/JSON errors, quota/exhaustion, idempotency risks.

B. **Top-2 Use Cases (selection rationale)**

* Table with: UseCase, CriticalityReason, SLI/SLO at risk (p95 latency, success rate), Current Fail Modes, Estimated Blast Radius.

C. **Circuit Breaker Design per Use Case (deep dive)**
For each use case, provide:

1. **Trip conditions**

   * Sliding window size (N calls or T seconds), failure-rate threshold (%), consecutive-error threshold, and latency p95/p99 threshold.
   * Error classes included/excluded (e.g., count 5xx/timeout; exclude 4xx user errors).
2. **States & transitions**

   * Closed â†’ Open â†’ Half-Open with probe counts, decay, cool-down.
3. **Retry/backoff**

   * Max attempts, exponential backoff + full jitter, per-call **retry budget** and **global budget** to protect upstream.
4. **Fallbacks & graceful degradation**

   * LLM: cached answer/template, smaller/cheaper model, summarization mode, or â€œdraft + user confirmâ€.
   * PPT build: minimal theme, skip images, reduce slide count.
5. **Idempotency & dedupe**

   * Idempotency keys, request hashing for safe retries.
6. **Observability**

   * Metrics: `circuit_state{usecase}`, `trips_total`, `rejections_total`, `probe_success_total`, `fallback_invocations`, `retry_attempts`.
   * Logs: structured, no PII; Traces: span links across retries/fallbacks.
7. **Configuration & rollout**

   * Config-as-code defaults + dynamic overrides (env/flags/config service).
   * Canary, feature flag, blast radius controls.
8. **Security & privacy**

   * Donâ€™t log prompts/outputs; guardrails for prompt injection; secrets via env/secret manager.
9. **Testing plan**

   * Unit (fakes), fault-injection (timeouts/5xx), load + chaos (latency spikes), game-day playbook.
10. **Code artifacts**

* **Python pseudo-code** for the breaker wrapper, plus a minimal **reference implementation** using either `pybreaker` or `tenacity` for retries and a custom state store (in-memory â†’ Redis for multi-worker).
* Integration points (decorators/middleware) and how to apply to the two hot paths without invasive changes.

D. **STAR Write-ups (one per use case)**
Use **S**ituation, **T**ask, **A**ctions (design choices, trade-offs, numbers), **R**esults (SLO improvement, error-budget burn reduction, cost/latency deltas). Keep to 8â€“12 bullets each.

**Style & Constraints**

* Be precise; no generic advice. Provide concrete thresholds, windows, and example numbers with rationale.
* Show reasoning **via short justifications**â€”do not output private chain-of-thought.
* Output sections **Aâ€“D** in order, with clear headings, tables where appropriate, and code blocks for pseudo-code.

**Repository Snapshot**

```
[Paste tree or list of scripts here]
```

---

## Add-Ons You Can Toggle (optional, if relevant)

* **Multi-tenant isolation:** per-tenant breaker state to prevent noisy-neighbor effects.
* **Client vs. server breaker:** justify placement (SDK/client, gateway, or server-side middleware).
* **Quota-aware breaker:** separate trip logic for 429s vs 5xx.
* **Adaptive thresholds:** increase/open faster during incident (burn-rate alerts).
* **Cost guardrails:** automatic downgrade to cheaper LLM/model when breaker is half-open.

---

## Example Answer Shape (concise sample for one LLM call path)

**Trip conditions**

* Window: last **200** calls or **60s**, whichever first.
* Trip if **failure_rate â‰¥ 15%** (timeouts, 5xx, DNS, TLS) **OR** if **p95 latency > 4s** for 3 consecutive windows **OR** **â‰¥5** consecutive timeouts.

**States**

* **Open:** 30s cool-down (flag + timestamp).
* **Half-Open:** allow 5 probes at 2s spacing; close on â‰¥4/5 success with p95 < 2.5s; else reopen 60s.

**Retry & Backoff**

* Max 2 retries per call, **exponential backoff with full jitter**: 300â€“700ms, then 600â€“1400ms. Global retry budget 10% of RPS.

**Fallbacks**

* Serve cached slide outline (24h TTL) or **fallback to smaller model**; if both fail, render minimal PPT with headers and placeholders.

**Observability**

* Metrics: `circuit_state{usecase="llm_generate"}`, `trips_total`, `fallback_invocations`, `probe_success_total`.
* Trace: one span per attempt, link retries.

**Config**

* Env defaults, dynamic overrides via `CONFIG_BREAKERS.yaml` or Redis. Feature flag: `cb_llm_generate_v1`.

**Python wrapper (pseudo-code)**

```python
def with_breaker(usecase):
    def deco(fn):
        def wrapped(*args, **kwargs):
            state = breaker_store.get_state(usecase)
            if state.is_open() and not state.allow_probe():
                metrics.inc("rejections_total", usecase)
                return fallback(usecase, *args, **kwargs)

            try:
                resp = retry_with_backoff(fn, *args, **kwargs)
                breaker_store.record_success(usecase, latency_ms(resp))
                return resp
            except TransientUpstreamError as e:
                breaker_store.record_failure(usecase, e)
                if breaker_store.should_trip(usecase):
                    breaker_store.open(usecase)
                    alert("circuit_tripped", usecase)
                return fallback(usecase, *args, **kwargs)
        return wrapped
    return deco
```

---

## Why this prompt works

* **Concrete outputs** (Aâ€“D) prevent hand-wavy answers.
* **Numbers + thresholds** force engineering rigor.
* **STAR** ensures stakeholder-friendly reporting.
* **Privacy note** avoids chain-of-thought leakage while still demanding justifications.
* **Code + rollout** makes it immediately actionable.

If you want, I can adapt this to your **ppt-ai-generator** repo specifically (LLM slide generation + PPT build as the two hot paths) and pre-fill suggested thresholds and metrics.
