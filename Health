


---

You are a senior backend engineer with deep understanding of production-grade API design, DRY principles, and code quality enforcement.

You are working on a **GenAI feedback pipeline project**, where feedback is stored in a **MongoDB collection**. Your task is to add a new feature with full test coverage.

---

### üß© Task:

Create a new REST endpoint:
**`GET /feedback`**
This endpoint fetches all feedback records from MongoDB, **removes the `_id` field**, and returns the data as a **CSV file download**.

---

### üìö Input:

* Feedback is stored in a MongoDB collection with documents like:

```json
{
  "_id": ObjectId(...),
  "Timestamp": "2025-11-28T16:21:44.044Z",
  "Liked": true,
  "Feedback": "Thanks much. But citation doesn‚Äôt provide any links",
  "prompt_id": "...",
  "Prompt": "Tell me about Factset and how to use it",
  "Response": "...",
  "User": "A12345",
  "chat_id": "fdfererer"
}
```

---

### üìÅ Codebase Context:

* Code is located under `/src`
* MongoDB connection is already implemented in:
  `src/db/mongo_client.py`
* API framework: **FastAPI**
* Test framework: **pytest** (with `pytest-cov` and `mongomock`)
* Adhere to **existing design principles**, logging, and response schema patterns in `/src`
* Follow **DRY principles**: do not re-implement Mongo connection or exception handling logic.

---

### üß™ Requirements:

1. **GET /feedback endpoint**

   * Reuse existing Mongo connection and DB selection logic
   * Fetch all documents
   * Remove `_id` field
   * Convert documents to CSV
   * Return CSV as downloadable file with appropriate headers (`text/csv`, `attachment; filename=feedback.csv`)

2. **Design alignment**

   * Follow routing, error handling, and logging style from `/src`
   * Do not invent new abstractions or frameworks

3. **Security / Performance**

   * Limit export size if large (e.g., paginate or stream large responses if applicable)
   * Ensure UTF-8 safe CSV

4. **Testing**

   * Write unit test in `tests/test_feedback_export.py`
   * Use fixtures to mock MongoDB response
   * Cover:

     * CSV formatting correctness
     * Header checks
     * 200 OK status
   * Achieve **100% code coverage** for the endpoint

---

### ‚úÖ Output Format:

* Endpoint code (in FastAPI-style controller)
* CSV builder code if applicable
* Unit test file with coverage
* Mention any assumptions you made (e.g., about DB name or CSV encoding)

---

## ‚úÖ Bonus Add-On

If available, infer:

* How logging is handled (`logger.info`, `logger.error`)
* Whether API responses use a wrapper (e.g., `SuccessResponse(data=...)`)
* Which environment variable or config defines DB/collection

---

Would you like me to generate the actual FastAPI code + test suite for this prompt now? Or do you want to try using this prompt with your internal Copilot or LLM first?

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

 23 nov 10:30
Nice additions ‚Äî those two asks are very ‚Äúprincipal engineer‚Äù:
	‚Ä¢	classify owned vs external endpoints, and
	‚Ä¢	give step-by-step commands to test locally after switching to localhost.

Here‚Äôs the updated zero-shot prompt with those bits cleanly woven in.

‚∏ª

Updated Zero-Shot Instruction Prompt

Role
You are a senior backend/infra engineer in a high-performing PE team working on an LLM chatbot application. Think like someone at a billion-dollar startup: you care about correctness, clarity, and minimal, safe changes.

Context
	‚Ä¢	We have a Python-based LLM chatbot application with multiple API endpoints.
	‚Ä¢	There is a deployment script and several application/route/config scripts.
	‚Ä¢	I want to understand how values are populated, how endpoints are wired, and how to extend health checks safely while being able to run and test everything on localhost.

What I will provide
I will paste:
	‚Ä¢	The deployment script(s) for the LLM chatbot application.
	‚Ä¢	The main app/server/route scripts that define the API endpoints.
	‚Ä¢	Any config/env handling modules if they exist.

Your tasks (step-by-step)
	1.	Endpoint Discovery & Classification
	‚Ä¢	Analyse each script I provide and list all API endpoints.
	‚Ä¢	For each endpoint, specify:
	‚Ä¢	HTTP method (GET/POST/PUT/etc.)
	‚Ä¢	Path (e.g. /chat, /health, /v1/messages)
	‚Ä¢	File (and function/handler name if clear)
	‚Ä¢	One-line description of what it does (based on code)
	‚Ä¢	Ownership:
	‚Ä¢	Mark as ‚ÄúOWNED‚Äù if this endpoint is implemented by our service (we define the handler).
	‚Ä¢	Mark as ‚ÄúEXTERNAL‚Äù if it is an upstream endpoint we are calling/consuming from another service or provider (e.g. OpenAI, internal LLM service, other microservice).
	2.	Value Population / Configuration Flow
	‚Ä¢	Explain how important values are getting populated, especially:
	‚Ä¢	Hostnames / base URLs / ports
	‚Ä¢	LLM endpoint URLs
	‚Ä¢	Any config flags that affect routing or behavior
	‚Ä¢	For each value, tell me where it comes from (e.g., environment variable, config file, default constant, CLI arg).
	‚Ä¢	Summarize this as a short ‚Äúconfig/value flow‚Äù section that helps me understand how the deployment script wires everything together.
	3.	Existing Health-Check Logic
	‚Ä¢	Identify any existing health / readiness / status endpoints. For each, describe:
	‚Ä¢	Path + method
	‚Ä¢	What checks it performs (e.g., does it ping the LLM, DB, cache, etc.?)
	‚Ä¢	How it returns the result (status code + response body format).
	‚Ä¢	From this, infer the standard health-check pattern already used in this application (for example: simple 200 OK, JSON with status field, etc.).
	4.	Health-Check Extension Plan (No New Logic)
	‚Ä¢	For endpoints that do not have health checks associated with them, propose a plan to add or reuse health-check endpoints.
	‚Ä¢	Do not create any new business logic.
	‚Ä¢	If a health-check is already available for the same shared dependencies (e.g., same LLM client or DB), re-use that logic or pattern.
	‚Ä¢	Be explicit about:
	‚Ä¢	Which file(s) to modify
	‚Ä¢	Which existing functions/handlers to reuse or call
	‚Ä¢	Any new route definitions required, written in the same style as existing health routes
	‚Ä¢	Focus on a minimal, safe change set that a production team would be comfortable with.
	5.	Localhost Testing Plan (Changing Host to localhost)
	‚Ä¢	Analyse where the host/base URL and related settings are defined (deployment script, env vars, config objects, hard-coded constants).
	‚Ä¢	Provide a clear plan for switching the application and/or its upstream calls to localhost for local testing, including:
	‚Ä¢	Which environment variables or config values to change (with example values, e.g. BASE_URL="http://localhost:8000").
	‚Ä¢	Which code lines or config sections would need edits if any values are hard-coded.
	‚Ä¢	How to ensure health-check endpoints also work correctly against localhost.
	6.	Step-by-Step Commands to Test on Localhost
	‚Ä¢	After proposing the localhost configuration changes, give me a step-by-step sequence of commands to test the service locally, such as:
	‚Ä¢	Commands to export environment variables / activate virtualenv
	‚Ä¢	Command to start the server (e.g. uvicorn main:app --reload --host 0.0.0.0 --port 8000)
	‚Ä¢	curl / httpie examples (or similar) to:
	‚Ä¢	Hit the main chatbot endpoint(s)
	‚Ä¢	Hit the health-check endpoint(s)
	‚Ä¢	Optionally hit any other critical endpoints we own
	‚Ä¢	The commands should assume a typical local dev environment (Linux/macOS shell) and be immediately runnable.

Constraints
	‚Ä¢	Do not invent entirely new health-check logic; reuse or lightly adapt existing patterns already present in the code.
	‚Ä¢	Do not redesign the architecture; keep changes minimal and focused on health checks and localhost configurability.
	‚Ä¢	Assume Python < 3.11 unless the code clearly shows otherwise.
	‚Ä¢	Explain your reasoning briefly but clearly, as if writing review notes for a principal engineer.

Output Format
	‚Ä¢	Section 1 ‚Äì Endpoint Inventory & Ownership: table of
	‚Ä¢	METHOD | PATH | FILE | HANDLER | OWNED/EXTERNAL | BRIEF PURPOSE.
	‚Ä¢	Section 2 ‚Äì Config & Value Flow: short explanation of how hosts/base URLs/ports and key config values are populated.
	‚Ä¢	Section 3 ‚Äì Existing Health-Check Pattern: description of existing health endpoints and the inferred standard pattern.
	‚Ä¢	Section 4 ‚Äì Health-Check Extension Plan: bullet list of concrete steps + which files/functions/paths to touch, and high-level code snippets (no big refactors).
	‚Ä¢	Section 5 ‚Äì Localhost Testing Plan:
	‚Ä¢	(a) config/code changes required to run on localhost, and how to revert them.
	‚Ä¢	(b) step-by-step shell commands to start the app locally and test key endpoints (chat + health).
	‚Ä¢	Section 6 ‚Äì Assumptions / Open Questions: list any assumptions you had to make or clarifications I should provide.

‚∏ª

If you want, next step you can paste your main app + router + deployment script, and we can ‚Äúdry run‚Äù this prompt together so you see exactly the kind of output it will drive.
>>>>>>>>>>>>>

Nice, this is shaping up into a very solid zero-shot use case üëå

You‚Äôre basically asking the agent to:
	‚Ä¢	Read a multi-file LLM chatbot codebase + deployment script
	‚Ä¢	Reverse-engineer:
	‚Ä¢	Which API endpoints exist
	‚Ä¢	How values (configs/hosts/URLs/etc.) get populated
	‚Ä¢	How existing health checks are designed
	‚Ä¢	Then:
	‚Ä¢	Propose a health-check extension plan that reuses existing logic
	‚Ä¢	Tell you exact code/config changes needed to run against localhost for testing

Let me break it into two parts:
	1.	What your current prompt implies (characteristics & reasoning chain)
	2.	A stronger, drop-in improved prompt you can use as a zero-shot instruction

‚∏ª

1Ô∏è‚É£ Characteristics / complexity / reasoning chain in your current prompt

What you‚Äôre asking the agent (implicitly)

Your prompt requires the agent to:
	1.	Understand the architecture
	‚Ä¢	There is a ‚Äúscript to deploy the LLM chatbot application‚Äù
	‚Ä¢	There are ‚Äúdifferent API endpoints‚Äù
	‚Ä¢	Values are being populated from somewhere (env vars, config files, defaults, infra)
	2.	Perform static analysis
	‚Ä¢	‚ÄúAnalyse each script and tell me the api endpoint that we are creating‚Äù
	‚Ä¢	That means: find routers, decorators like @app.get("/..."), FastAPI/Flask routes, etc.
	3.	Design a health-check extension plan
	‚Ä¢	Reuse existing health-check logic if present
	‚Ä¢	Don‚Äôt create new business logic
	‚Ä¢	Apply health checks consistently to endpoints that don‚Äôt have them
	4.	Environment toggling (prod ‚Üî localhost)
	‚Ä¢	‚ÄúIn the suggest plan please provide me‚Ä¶ in case if I want to change it to localhost for testing then tell me what is the code changes required‚Äù
	‚Ä¢	That means: identify where host/base URL/ports/allowed origins are configured and show how to override them for local testing.

Reasoning chain it actually needs

A capable agent must roughly:
	1.	Parse all provided scripts (deployment, app, routers, configs).
	2.	Identify all API endpoints (path + method + file).
	3.	Trace value flow:
	‚Ä¢	Where hostnames, base URLs, ports, and credentials come from
	‚Ä¢	Config class? .env? Helm values? Terraform vars? Hard-coded constants?
	4.	Detect any existing health or readiness endpoints and infer the ‚Äústandard‚Äù health pattern in this codebase.
	5.	Propose a minimal, pattern-consistent health-check plan for endpoints without health checks, without inventing new logic.
	6.	Identify what to change to run everything on localhost:
	‚Ä¢	Base URLs / upstreams
	‚Ä¢	Bind address / port
	‚Ä¢	Any environment flags (e.g. ENV=local)
	‚Ä¢	CORS, callback URLs, etc.
	7.	Explain these changes clearly and in a reviewable way.

Your current prompt expresses the intent, but it‚Äôs:
	‚Ä¢	Vague about inputs (‚ÄúI have a script‚Äù)
	‚Ä¢	Missing a strict output structure
	‚Ä¢	Missing explicit constraints on how to handle localhost changes
	‚Ä¢	Not forcing a step-by-step reasoning sequence

‚∏ª

2Ô∏è‚É£ Improved zero-shot prompt (ready to use)

Here‚Äôs a tightened version you can paste as your instruction to the agent and then follow with your actual scripts.

‚∏ª

üß† Improved Prompt

Role
You are a senior backend/infra engineer in a high-performing PE team working on an LLM chatbot application. Think like someone at a billion-dollar startup: you care about correctness, clarity, and minimal, safe changes.

Context
	‚Ä¢	We have a Python-based LLM chatbot application with multiple API endpoints.
	‚Ä¢	There is a deployment script and several application/route/config scripts.
	‚Ä¢	I want to understand how values are populated and how to extend health checks safely.

What I will provide
I will paste:
	‚Ä¢	The deployment script(s) for the LLM chatbot application.
	‚Ä¢	The main app/server/route scripts that define the API endpoints.
	‚Ä¢	Any config/env handling modules if they exist.

Your tasks (step-by-step)
	1.	Endpoint Discovery
	‚Ä¢	Analyse each script I provide and list all API endpoints we are creating.
	‚Ä¢	For each endpoint, specify:
	‚Ä¢	HTTP method (GET/POST/PUT/etc.)
	‚Ä¢	Path (e.g. /chat, /health, /v1/messages)
	‚Ä¢	File (and function/handler name if clear)
	‚Ä¢	One-line description of what it does (based on code).
	2.	Value Population / Configuration Flow
	‚Ä¢	Explain how important values are getting populated, especially:
	‚Ä¢	Hostnames / base URLs / ports
	‚Ä¢	LLM endpoint URLs
	‚Ä¢	Any config flags that affect routing or behavior
	‚Ä¢	For each value, tell me where it comes from (e.g., environment variable, config file, default constant, CLI arg).
	‚Ä¢	Summarize this as a short ‚Äúconfig/value flow‚Äù section that helps me understand how the deployment script wires everything together.
	3.	Existing Health-Check Logic
	‚Ä¢	Identify any existing health / readiness / status endpoints. For each, describe:
	‚Ä¢	Path + method
	‚Ä¢	What checks it performs (e.g., does it ping the LLM, DB, cache, etc.?)
	‚Ä¢	How it returns the result (status code + response body format).
	‚Ä¢	From this, infer the standard health-check pattern already used in this application (e.g. simple 200 OK, JSON with status field, etc.).
	4.	Health-Check Extension Plan (No New Logic)
	‚Ä¢	For endpoints that do not have health checks associated with them, propose a plan to add or reuse health-check endpoints.
	‚Ä¢	Do not create any new business logic.
	‚Ä¢	If a health-check is already available for the same shared dependencies (e.g., same LLM client or DB), re-use that logic or pattern.
	‚Ä¢	Be explicit about:
	‚Ä¢	Which file(s) to modify
	‚Ä¢	Which existing functions/handlers to reuse or call
	‚Ä¢	Any new route definitions required, written in the same style as existing health routes
	‚Ä¢	Focus on a minimal, safe change set that a production team would be comfortable with.
	5.	Localhost Testing Plan (Changing Host to localhost)
	‚Ä¢	Analyse where the host/base URL and related settings are defined (deployment script, env vars, config objects, hard-coded constants).
	‚Ä¢	Provide a clear plan for switching the application and/or its upstream calls to localhost for local testing, including:
	‚Ä¢	Which environment variables or config values to change (with example values, e.g. BASE_URL="http://localhost:8000").
	‚Ä¢	Which code lines or config sections would need edits if any values are hard-coded.
	‚Ä¢	How to ensure health-check endpoints also work correctly against localhost.
	‚Ä¢	The plan should be concrete enough that I can follow it step-by-step to run the whole thing on my laptop, and then revert the changes for production.

Constraints
	‚Ä¢	Do not invent entirely new health-check logic; reuse or lightly adapt existing patterns already present in the code.
	‚Ä¢	Do not redesign the architecture; keep changes minimal and focused on health checks and localhost configurability.
	‚Ä¢	Assume Python < 3.11 unless the code clearly shows otherwise.
	‚Ä¢	Explain your reasoning briefly but clearly, as if writing review notes for a principal engineer.

Output Format
	‚Ä¢	Section 1 ‚Äì Endpoint Inventory: table of METHOD | PATH | FILE | HANDLER | BRIEF PURPOSE.
	‚Ä¢	Section 2 ‚Äì Config & Value Flow: short explanation of how hosts/base URLs/ports and key config values are populated.
	‚Ä¢	Section 3 ‚Äì Existing Health-Check Pattern: description of existing health endpoints and the inferred standard pattern.
	‚Ä¢	Section 4 ‚Äì Health-Check Extension Plan: bullet list of concrete steps + which files/functions/paths to touch, and high-level code snippets (no big refactors).
	‚Ä¢	Section 5 ‚Äì Localhost Testing Plan: bullet list of the exact config/code changes required to run on localhost, and how to revert them.
	‚Ä¢	Section 6 ‚Äì Assumptions / Open Questions: list any assumptions you had to make or clarifications I should provide.

‚∏ª

If you‚Äôd like, next step you can paste one or two of the core scripts (e.g. main app + router file), and I can show you what a sample ‚ÄúSection 1‚Äì5‚Äù answer would look like with this prompt ‚Äî that makes it easier to tune further.
