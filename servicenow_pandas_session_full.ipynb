{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udc3c Pandas ServiceNow Session \u2014 End-to-End Demo\n", "This notebook covers **read \u2192 wrangle \u2192 analyze \u2192 visualize \u2192 export** using a ServiceNow-style dataset."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udce6 Imports + Quick Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Read CSVs (adjust paths if needed)\n", "tickets = pd.read_csv(\"servicenow_demo_tickets.csv\", parse_dates=[\"opened_at\",\"closed_at\"])\n", "users   = pd.read_csv(\"servicenow_demo_users.csv\")\n", "\n", "tickets.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> **Note:** Think of `tickets` as your **Incident** table, and `users` as a lookup for assignee metadata."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd17 1) Join vs Merge \u2014 What's the difference?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- **`merge`**: SQL-style join on columns (explicit & most common)\n", "- **`join`**: Convenience join on **index** (or a column via `on=`), handy after you set an index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Merge (left join)\n", "enriched = tickets.merge(\n", "    users[[\"assignee\",\"assignee_full_name\",\"location\",\"manager\"]],\n", "    on=\"assignee\", how=\"left\", validate=\"m:1\"\n", ")\n", "enriched.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Add human-friendly assignee details to tickets.\n", "\n", "**Tip:** `validate=\"m:1\"` guards against accidental many-to-many explosions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Join (after setting index)\n", "enriched2 = (\n", "    tickets.set_index(\"assignee\")\n", "           .join(users.set_index(\"assignee\"), how=\"left\")\n", "           .reset_index()\n", ")\n", "enriched2.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** A bit shorter once keys are indices; functionally similar to `merge`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\uddd1\ufe0f 2) Deleting rows with null values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop rows where assignee OR priority is missing\n", "clean_rows = tickets.dropna(subset=[\"assignee\", \"priority\"])\n", "clean_rows.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Enforce critical fields before KPIs.\n", "\n", "**Analogy:** Reject incomplete forms before processing."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\uddd1\ufe0f 3) Deleting columns with (mostly) null values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop columns that are 80% or more missing\n", "threshold = 0.8\n", "cols_to_drop = tickets.columns[tickets.isna().mean() >= threshold]\n", "tickets_dropcols = tickets.drop(columns=cols_to_drop)\n", "tickets_dropcols.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Remove noisy columns that won\u2019t help analysis."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcd1 4) Read Excel (specific sheet/tab)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Read \"Tickets\" sheet\n", "tickets_xlsx = pd.read_excel(\"servicenow_demo.xlsx\", sheet_name=\"Tickets\", parse_dates=[\"opened_at\",\"closed_at\"])\n", "\n", "# Read \"Users\" sheet\n", "users_xlsx = pd.read_excel(\"servicenow_demo.xlsx\", sheet_name=\"Users\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Enterprise data often lands as Excel with multiple tabs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf10 5) Read JSON and Parquet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# JSON (records)\n", "tickets_json = pd.read_json(\"servicenow_demo_tickets.json\")\n", "\n", "# Parquet (fast, typed)\n", "tickets_parquet = pd.read_parquet(\"servicenow_demo_tickets.parquet\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** JSON is common for APIs; Parquet is best for speed + schema."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\uddf9 6) Imputing missing values (fillna, per-type strategies)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 6.1 Simple fills\n", "tickets[\"priority\"] = tickets[\"priority\"].fillna(\"P3\")  # fill categorical with a default\n", "tickets[\"assignment_group\"] = tickets[\"assignment_group\"].fillna(\"Unassigned\")\n", "tickets[\"time_to_resolve_hours\"] = tickets[\"time_to_resolve_hours\"].fillna(0.0)\n", "\n", "# 6.2 Group-wise imputation (e.g., fill priority by category mode)\n", "mode_by_cat = (tickets.groupby(\"category\")[\"priority\"]\n", "                      .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else \"P3\"))\n", "tickets[\"priority\"] = tickets[\"priority\"].fillna(tickets[\"category\"].map(mode_by_cat))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Missing-value policy should follow business rules (defaults, mode/median, or group-wise fills)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd0d 7) Selecting and filtering"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Select a few columns\n", "core = tickets[[\"ticket_id\",\"opened_at\",\"state\",\"priority\",\"assignment_group\",\"assignee\",\"customer\"]]\n", "\n", "# Filter high-priority open tickets\n", "mask = (tickets[\"priority\"].isin([\"P1\",\"P2\"])) & (~tickets[\"state\"].isin([\"Resolved\",\"Closed\"]))\n", "open_high = tickets.loc[mask, [\"ticket_id\",\"priority\",\"state\",\"assignment_group\",\"assignee\"]]\n", "open_high.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Analogy:** Think slicers/filters in Excel\u2014only scripted and repeatable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcca 8) Grouping and aggregating (KPIs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Average TTR and count by assignment group\n", "kpi_group = (tickets\n", "             .groupby(\"assignment_group\", as_index=False)\n", "             .agg(\n", "                 tickets_count=(\"ticket_id\",\"count\"),\n", "                 avg_ttr_hours=(\"time_to_resolve_hours\",\"mean\"),\n", "                 p90_ttr=(\"time_to_resolve_hours\", lambda s: s.quantile(0.9))\n", "             )\n", "             .sort_values(\"tickets_count\", ascending=False))\n", "kpi_group.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Monthly volume by priority\n", "tickets[\"month\"] = tickets[\"opened_at\"].dt.to_period(\"M\").astype(str)\n", "vol_by_month_priority = (tickets\n", "    .groupby([\"month\",\"priority\"], as_index=False)\n", "    .agg(count=(\"ticket_id\",\"count\"))\n", "    .sort_values([\"month\",\"priority\"]))\n", "vol_by_month_priority.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** These rollups become dashboard tiles and SLO inputs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\uddc2\ufe0f 9) Sort and deduplicate (keep latest per ticket or per key)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Keep the latest record per ticket_id by closed_at (or opened_at if still open)\n", "tickets_sorted = tickets.sort_values([\"ticket_id\",\"closed_at\",\"opened_at\"], ascending=[True, False, False])\n", "latest_per_ticket = tickets_sorted.drop_duplicates(subset=\"ticket_id\", keep=\"first\")\n", "latest_per_ticket.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Real tables often have multiple snapshots; pick the canonical one."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\ude79 10) Handling missing values (overview)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Missingness profile (top 5)\n", "missing_report = tickets.isna().mean().sort_values(ascending=False).head(5)\n", "missing_report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace booleans and flags\n", "tickets[\"sla_breach\"] = tickets[\"sla_breach\"].fillna(False)\n", "\n", "# Drop rows missing BOTH assignee and assignment_group\n", "drop_both = tickets.dropna(subset=[\"assignee\", \"assignment_group\"], how=\"all\")\n", "drop_both.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Know where holes are; decide: fill, drop, or model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcc8 11) Graphs using matplotlib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run each cell separately to generate one plot at a time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bar chart: Tickets by priority\n", "count_by_priority = tickets[\"priority\"].value_counts().sort_index()\n", "plt.figure()\n", "count_by_priority.plot(kind=\"bar\")\n", "plt.title(\"Ticket Count by Priority\")\n", "plt.xlabel(\"Priority\")\n", "plt.ylabel(\"Count\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Line chart: Monthly volume\n", "vol_month = tickets.groupby(\"month\")[\"ticket_id\"].count().sort_index()\n", "plt.figure()\n", "vol_month.plot(kind=\"line\", marker=\"o\")\n", "plt.title(\"Monthly Ticket Volume\")\n", "plt.xlabel(\"Month\")\n", "plt.ylabel(\"Tickets\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Histogram: Time to resolve (hours)\n", "plt.figure()\n", "tickets[\"time_to_resolve_hours\"].dropna().plot(kind=\"hist\", bins=15)\n", "plt.title(\"Distribution of Time to Resolve (Hours)\")\n", "plt.xlabel(\"Hours\")\n", "plt.ylabel(\"Frequency\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd17 Extra: a clean, readable pipeline (method chaining)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# From raw tickets \u2192 enrich with users \u2192 KPI rollup\n", "kpi = (\n", "    pd.read_csv(\"servicenow_demo_tickets.csv\", parse_dates=[\"opened_at\",\"closed_at\"])\n", "      .assign(\n", "          month=lambda d: d[\"opened_at\"].dt.to_period(\"M\").astype(str),\n", "          time_to_resolve_hours=lambda d: d[\"time_to_resolve_hours\"].fillna(0.0),\n", "          priority=lambda d: d[\"priority\"].fillna(\"P3\"),\n", "      )\n", "      .merge(pd.read_csv(\"servicenow_demo_users.csv\")[['assignee','location']], on=\"assignee\", how=\"left\")\n", "      .groupby([\"month\",\"priority\",\"location\"], as_index=False)\n", "      .agg(tickets=(\"ticket_id\",\"count\"),\n", "           avg_ttr=(\"time_to_resolve_hours\",\"mean\"))\n", "      .sort_values([\"month\",\"priority\",\"location\"])\n", ")\n", "kpi.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Why:** Reads like a recipe you can re-run every day."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcbe 12) Writing / Exporting Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# To CSV\n", "tickets.to_csv(\"tickets_clean.csv\", index=False)\n", "\n", "# To Excel with multiple sheets\n", "with pd.ExcelWriter(\"tickets_output.xlsx\", engine=\"xlsxwriter\") as writer:\n", "    tickets.to_excel(writer, sheet_name=\"Tickets\", index=False)\n", "    users.to_excel(writer, sheet_name=\"Users\", index=False)\n", "\n", "# To JSON\n", "tickets.to_json(\"tickets.json\", orient=\"records\", indent=2, date_format=\"iso\")\n", "\n", "# To Parquet\n", "tickets.to_parquet(\"tickets.parquet\", index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- `index=False` prevents row numbers from being written as a column\n", "- `orient=\"records\"` makes row-wise JSON objects; `date_format=\"iso\"` keeps ISO timestamps\n", "- Parquet is fast/typed and ideal for big-data pipelines (Spark/Dask/Athena)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 Tips for your session", "- Call out **why** each step matters (e.g., imputation tied to SLA reporting)\n", "- Encourage participants to tweak thresholds (e.g., P1/P2 filter) and re-run\n", "- Show a merge with `validate=` to demonstrate safe joins\n", "- Close by saving outputs with `to_csv` / `to_parquet`"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}