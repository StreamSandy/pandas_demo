

11:00 pm
>>>>>>>>>>>>>>>>>>>>

Love it. Here’s a copy-paste, battle-tested zero-shot prompt that makes a top agent produce a step-by-step, code-anchored explainer of every circuit breaker in your repo—covering design, failure detection, fallbacks, observability, edge cases, and PE-grade recommendations.

⸻

Zero-Shot Prompt — “Explain Every Circuit Breaker in Our Codebase (Step-by-Step, PE-Ready)”

Role
Act as a Staff/Principal engineer at a billion-dollar startup: concise, accurate, SRE-aligned, and documentation-first.

Context
	•	We use an in-house circuit breaker pattern (e.g., CircuitBreaker(fail_max=5, reset_timeout=60, success_threshold=2)), possibly via ent_pycore or local equivalents.
	•	Codebase layout: src/ (code), tests/ (pytest).
	•	Primary files likely include src/circuit_breaker.py and call sites across services/clients.
	•	Objective: explain, not redesign—produce a precise, code-referenced walkthrough of how each breaker works, how failure is detected, and what the fallback is.

Inputs You Can Assume
	•	Repo root: <<REPO_ROOT>> (default: current working tree)
	•	Key modules: <<PRIMARY_FILES_OR_GLOBS>> (e.g., src/**/circuit_breaker*.py, src/**/*client*.py, src/**/*service*.py)
	•	Runtime config sources in order of precedence (override if code differs): ENV → config file → code defaults.
	•	If ent_pycore is a distribution/namespace, use AST + safe import guards to find its exported CB pieces.

Task
Produce a single Markdown report that explains every circuit breaker instance and its behavior, with code pointers and runnable reproductions.

Method (required)
	1.	Discover all CBs
	•	Statically (AST) find CircuitBreaker(...) and wrappers; include custom factories or decorators.
	•	For each instance, resolve thresholds/timeouts/success criteria and where they come from (ENV, config, defaults).
	2.	Trace usage
	•	Build a concise call graph from entrypoint → CB → upstream dependency (e.g., LLM API).
	•	Note sync vs async, thread/process safety, and whether the breaker is per-dependency or global.
	3.	Explain behavior step-by-step
	•	Show state machine (Closed → Open → Half-Open), with what triggers transitions (consecutive failures vs error-rate window), and timers (cool-down/reset).
	•	Clarify what counts as “failure”: exception types, status codes, timeouts, circuit open rejections.
	•	Describe fallbacks (cached value, degraded response, null-object, fast-fail with user-friendly message).
	4.	Observability mapping
	•	List emitted metrics/logs/traces, names & labels; sample queries (Prometheus) and suggested alerts (flapping, open too long, probe failures).
	5.	Edge cases & hazards
	•	Double-wrapping, async + blocking mix, global state, multi-process workers, config hot-reload, high-cardinality metrics, retry storms.
	6.	Mini reproduction harness
	•	Provide simple, runnable code snippets or pytest examples that trigger: normal success, consecutive failures to Open, Half-Open probe success/fail, timeout path.
	7.	Prioritized recommendations
	•	P0/P1 suggestions to harden behavior, improve observability, or fix pitfalls—without changing public APIs unless strongly justified.

Output Format (strict, in order)
	1.	# Executive Summary
	•	One paragraph: how many CBs found, where, and headline risks or confidence level.
	2.	# Inventory of Circuit Breakers (table)

ID	Module:Line	Breaker Var	Upstream/Callsite	Policy (fail_max / reset_timeout / success_threshold)	Failure Signals Counted	Fallback



	3.	# State Machine & Semantics
	•	Mermaid stateDiagram-v2 with transitions & timers.
	•	Bulleted rules for what opens/closes; how Half-Open probes work; what’s ignored (e.g., 4xx vs 5xx).
	4.	# Per-Breaker Deep Dive (repeat per CB instance)
	•	Path & code refs: src/.../file.py:L123 (show small, focused snippet).
	•	Config resolution: ENV keys, config keys, defaults; resolved effective values.
	•	Call flow (Mermaid sequence diagram) from endpoint/function → CB → upstream.
	•	Failure taxonomy: exceptions/status codes/timeouts that trip the CB.
	•	Fallback behavior: exact return shape/examples; user impact.
	•	Concurrency model: thread/process safety notes; async peculiarities.
	•	Instrumentation: metric/log names & sample entries.
	5.	# Reproduction Recipes
	•	Pytest examples (or curl/script) to demonstrate:
	•	normal path,
	•	N consecutive failures → Open,
	•	cool-down → Half-Open,
	•	success_threshold successes → Closed,
	•	timeout vs error differentiation.
	•	Show expected outputs and /cb/state (if available).
	6.	# Observability & Alerting
	•	Metrics list, low-cardinality labels, example PromQL (or pseudo):
	•	open duration SLI, state transition counter, probe success rate.
	•	Alert sketches: breaker flapping, breaker stuck Open, sudden rise in open rejections.
	7.	# Edge Cases & Known Gaps
	•	E.g., double wrap, per-tenant breakers, retries causing thundering herd, multi-worker desync.
	8.	# Recommendations (P0/P1/P2)
	•	Actionable, scoped changes; risk & effort notes; suggested owners.
	9.	# Appendix
	•	AST/grep queries used, config key map, longer code excerpts if needed.

Acceptance Criteria
	•	Every circuit breaker in the codebase is listed with module:line references and resolved config values.
	•	Clear, step-by-step narrative of how failure is detected and exactly what fallback returns.
	•	Includes state diagram + sequence diagrams, runnable reproduction snippets, and concrete alert ideas.
	•	Uses precise language (no vague “may/should” unless truly unknown; then call it out as an assumption).
	•	No third-party CB design theory unless tied back to our code.

House Rules
	•	Be concise but specific. Quote only the minimal code needed to explain.
	•	Do not expose secrets; keep example logs/metrics PII-safe.
	•	If something is unknown or inferred, label it “Assumption” and proceed.

Optional Inputs (fill if known)
	•	<<ENV_KEYS>> for CB config (e.g., CB_FAIL_MAX, CB_RESET_TIMEOUT, CB_SUCCESS_THRESHOLD).
	•	<<UPSTREAMS>> names and SLIs/SLOs to anchor recommendations.
	•	Whether the repo exposes /cb/state//metrics; if not, note how to emulate.

⸻

Pro tips to strengthen your prompt even more
	•	Pin scope: add explicit file globs or entrypoints the agent must analyze.
	•	Give SLOs: target success rate/latency/error budget window to contextualize fallbacks.
	•	Name env keys: so config resolution is deterministic.
	•	Ask for risk-ranked recos with estimated effort (S/M/L) to help prioritization.
	•	Require runnable recipes: reviewers can verify behavior in minutes.

If you want, paste your repo paths and ENV keys and I’ll pre-fill the placeholders for you.


8:00 PM
>>>>>>>>>>>>>>>

Awesome—here’s a tight, copy-paste-ready **zero-shot prompt** you can give to the “most capable agent.” It forces runnable code, tests, and docs, all **under `src/temp/livetest`**, to A/B test your circuit breaker **before vs after** via FastAPI on localhost.

---

# Zero-Shot Prompt — FastAPI “Before vs After” Circuit Breaker (localhost, under `src/temp/livetest`)

**Role**
Act as a Staff/Principal backend & platform engineer. Ship minimal, production-grade, fully runnable code with tests and docs.

**Context**
We’re evaluating a Python **circuit breaker (CB)** for an LLM API call. Current usage pattern in our app:

```python
cb = CircuitBreaker(fail_max=5, reset_timeout=60, success_threshold=2)

def call_llm_api(...):
    return cb.call(llm_api, ...)
```

We need a **dummy FastAPI service** to compare behavior **before (no CB)** vs **after (with CB)** on **localhost**.

**Hard constraints**

* **All files must live under:** `src/temp/livetest/...` (use these exact paths).
* Use **FastAPI + uvicorn**.
* **Do not** use any third-party circuit-breaker libraries. Implement a small CB that mirrors the above semantics (Closed → Open → Half-Open with `fail_max`, `reset_timeout`, `success_threshold`).
* Keep dependencies minimal (FastAPI, pydantic, httpx (if needed), prometheus-client (optional), pytest).
* Python **3.10+**. No ellipses in code; provide complete files.

**What to build**

1. **Endpoints**

   * `GET /v1/before/complete` → calls simulated upstream **without** CB (control).
   * `GET /v1/after/complete` → same call **with** CB.
   * `GET /simulate/upstream` → simulated upstream that can:

     * return success,
     * return 5xx,
     * or timeout/sleep.
       Control via query params or ENV: `fail_rate` (0.0–1.0), `consec_fail` (int), `timeout_ms` (int).
   * `GET /cb/state` → JSON with `state` (`closed|open|half_open`), counters (attempts/success/failure/opens), timestamps.
   * `GET /healthz` → 200 OK.
   * `GET /metrics` → Prometheus metrics (optional but preferred).

2. **Config (ENV with sane defaults)**

   * `CB_FAIL_MAX=5`, `CB_RESET_TIMEOUT=60`, `CB_SUCCESS_THRESHOLD=2`
   * `UPSTREAM_FAIL_RATE=0.0`, `UPSTREAM_CONSEC_FAIL=0`, `UPSTREAM_TIMEOUT_MS=0`
   * `SERVER_HOST=127.0.0.1`, `SERVER_PORT=8080`
   * Respect ENV → .env → defaults precedence (simple, documented).

3. **Code organization (exact paths)**

```
src/temp/livetest/requirements.txt
src/temp/livetest/requirements-dev.txt            # optional (pytest, coverage, ruff/black)
src/temp/livetest/app/__init__.py
src/temp/livetest/app/config.py
src/temp/livetest/app/circuit_breaker.py
src/temp/livetest/app/upstream.py
src/temp/livetest/app/main.py
src/temp/livetest/tests/test_cb_api.py
src/temp/livetest/Makefile
src/temp/livetest/README.md
src/temp/livetest/.env.example
# Optional:
# src/temp/livetest/Dockerfile
# src/temp/livetest/pyproject.toml (ruff/black), mypy.ini
```

4. **Circuit breaker requirements (must)**

   * States: **Closed → Open → Half-Open**.
   * **Open** on either `fail_max` consecutive failures **or** (if you implement rate windows) an error rate threshold—choose one consistent policy and document it (default: consecutive failures only).
   * **Half-Open** allows limited probes; **Close** after `success_threshold` consecutive successes; otherwise **Open** again.
   * Thread/process-safe state changes (use locks for shared state).
   * Optional bounded retry with jitter **only** inside the “after” path; avoid retry storms; document the policy.
   * Structured JSON logs for state transitions (no secrets/PII).
   * Minimal, low-cardinality Prometheus metrics if `/metrics` is included.

5. **Tests (pytest)**

   * Use FastAPI TestClient or httpx AsyncClient.
   * Cover:

     * Success path (no failures).
     * Failing upstream: breaker **opens** by/before `fail_max`.
     * Cool-down elapses → **half-open**; on `success_threshold` successes → **closed** again.
     * Timeout behavior vs explicit 5xx.
     * `/cb/state` accuracy.
     * (Nice-to-have) simple concurrency test (parallel calls) showing safe transitions.
   * Include coverage run (target ≥ **85%** for CB module).

6. **Dev UX (localhost)**

   * `Makefile` targets: `run`, `test`, `lint`, `fmt`, `coverage`.
   * `README.md` quickstart for **localhost only** (no Docker required), plus optional Docker instructions.
   * Example `curl` commands to trigger scenarios:

     * random failures: `curl 'http://127.0.0.1:8080/v1/after/complete?fail_rate=0.7'`
     * consecutive failures then success: `...&consec_fail=5`
     * timeouts: `...&timeout_ms=1200`
   * Show how to read `/cb/state` and `/metrics`.

**Acceptance criteria**

* With `UPSTREAM_CONSEC_FAIL=5`, calling `/v1/after/complete` opens the breaker **by/before the 5th failure**, stays **open** for `CB_RESET_TIMEOUT` seconds, then transitions to **half-open**.
* During **half-open**, after `CB_SUCCESS_THRESHOLD` successful probes, the breaker **closes**.
* `/v1/before/complete` reflects raw upstream behavior (control).
* `/cb/state` returns correct state & counters; logs include transition entries.
* `pytest` passes locally; coverage target met.
* Service runs on **[http://127.0.0.1:8080](http://127.0.0.1:8080)** with `make run` or the README command.

**Output format (strict; in this order)**

1. **Project tree** (exact paths rooted at `src/temp/livetest/`)
2. `requirements.txt` (+ optional `requirements-dev.txt`)
3. `app/config.py`
4. `app/circuit_breaker.py`
5. `app/upstream.py`
6. `app/main.py`
7. `tests/test_cb_api.py` (and any helpers)
8. `Makefile`
9. `README.md`
10. `.env.example`
11. *(Optional)* `Dockerfile` and any lint/format config

> Provide **full file contents** for each (no placeholders or ellipses).

**House rules**

* No new complex deps; keep it lean.
* Small, readable functions; docstrings + type hints.
* Consistent logging format; redact sensitive values.
* If you must make an assumption, **state it in README** and proceed.

**Starter defaults (use unless overridden)**

* `CB_FAIL_MAX=5`, `CB_RESET_TIMEOUT=60`, `CB_SUCCESS_THRESHOLD=2`
* `SERVER_HOST=127.0.0.1`, `SERVER_PORT=8080`

---

## Bonus (things “a Google-level engineer” wouldn’t miss)

* Clear **state transition diagram** (Mermaid) in the README.
* **Alerting ideas** for `/metrics`: breaker flapping, open too long.
* **Operational switches**: env flag to disable CB, and a safe default when config is absent.
* Minimal **security hygiene**: no secrets in logs; avoid high-cardinality labels; validate/limit user-supplied params.

Want me to generate the **reference code** for this prompt next so you can run it immediately?

6:50PM
>>>>>>>>>>>>>>>>>>>>>.

Got it—great raw brief. Here’s a sharper, copy-paste-ready zero-shot prompt that sets the “most capable agent” up for success, plus a quick checklist of why these upgrades matter.

# What to improve (why your current prompt underdelivers)

* **Scope + guardrails:** Be explicit about allowed code changes (which files, what not to touch), dependencies (only in-house libs), and non-goals.
* **Inputs & assumptions:** Tell the agent exactly what you’re providing (repo paths, module list, target SLOs) and how to proceed if something’s missing.
* **Deliverables & formats:** Require deterministic outputs: a design ADR, a unified diff, test files, coverage target, and a PE-facing report.
* **Quality bars:** Bake in acceptance criteria, failure modes to simulate, and observability requirements.
* **Reasoning signal (without chain-of-thought):** Ask for decision records and trade-off summaries instead of hidden step-by-step thoughts.
* **Enterprise specifics:** Thread/process safety, sync/async, config precedence, feature flagging, rollback plan, and security/compliance notes.
* **ent_pycore reality check:** If it’s a distribution/namespace package, include a fallback way to discover submodules.

---

# Upgraded Zero-Shot Prompt (drop-in)

**System / Role**
You are a senior platform engineer embedded on a high-performing team. Operate like a Staff/Principal engineer at a billion-dollar startup: pragmatic, security-minded, SRE-aligned, and documentation-first.

**Context**

* We must implement/extend a **circuit breaker** using our **in-house library `ent_pycore`**, reusing the enterprise patterns already present in **`src/circuit_breaker.py`**.
* Codebase layout: `src/` (code), `tests/` (pytest). About 30+ scripts exist.
* You will receive the **use case(s)** (see `<<USE_CASES>>`) and a **list of ent_pycore modules relevant to circuit breaking**. Primary file of record: **`src/circuit_breaker.py`**.
* **Non-goals:** Do not introduce third-party CB libraries. Do not change public APIs outside the designated module(s) unless explicitly justified and documented.

**Inputs you have**

1. `<<USE_CASES>>`
2. List of relevant `ent_pycore` modules/subpackages and the file `src/circuit_breaker.py`.
3. SLO targets & constraints (fill if known; otherwise propose defaults and justify):

   * Target success rate: `<<SUCCESS_RATE>>`
   * p95 latency: `<<P95_MS>> ms`
   * Error budget window: `<<WINDOW>>`

**If imports are tricky (ent_pycore is a distribution/namespace)**

* Prefer **in-repo references** already used in `src/circuit_breaker.py`.
* If discovery is required, attempt safe introspection (e.g., `importlib.metadata` for top-level names, guarded imports). If still unknown, **document assumptions** and proceed with the best-aligned design to `src/circuit_breaker.py`.

**Task**
Design and implement a resilient, observable circuit breaker for the provided use case(s), **only** by extending/refactoring existing enterprise logic from `src/circuit_breaker.py` and related in-house modules.

**Functional requirements (must)**

* Support **Closed → Open → Half-Open** states with configurable thresholds:

  * **Open** when failure rate or consecutive failures exceed thresholds within a rolling window.
  * **Half-Open** with limited probe requests and success criteria to close.
* **Sync & Async** call paths (if codebase supports async): ensure consistent behavior and thread/process safety.
* **Timeouts + retries with jitter** integrated with breaker state (no retry storms; respect backoff).
* **Config precedence:** env → service config → sane defaults. Hot-reload safe if present in repo patterns.
* **Observability:** counters/timers for attempts, successes, failures, state transitions; log correlation IDs; OpenTelemetry hooks if used; minimal cardinality labels.
* **Feature flags/kill switch:** enable/disable breaker quickly at runtime (enterprise pattern).
* **Fallbacks / graceful degradation:** return cached/partial/placeholder responses where the use case allows.
* **Security/compliance:** no secrets in logs; PII-safe telemetry; adhere to existing logging levels and scrubbers.

**Non-functional requirements (must)**

* Deterministic behavior under concurrency; no race conditions on state transitions.
* Avoid global mutable state unless protected; prefer scoped breakers per external dependency.
* Clear, minimal public surface; internal helpers documented and tested.
* No new external dependencies.

**Design & Implementation steps (what to produce)**

1. **Short ADR (Architecture Decision Record)**

   * Problem statement, constraints, options considered, and the chosen approach with trade-offs.
   * State machine diagram (ASCII or Mermaid).
2. **Proposed config schema** with defaults and examples (YAML/ENV).
3. **Code changes** limited to `src/circuit_breaker.py` and necessary adjacent internal modules.
4. **Tests (pytest)** in `tests/` covering:

   * Threshold crossings, half-open probes, backoff/jitter, timeouts vs errors, concurrency, async (if applicable), idempotent retries, feature flag on/off, and flapping prevention.
   * Include property-based tests (if Hypothesis is already used in repo; else keep to pytest).
   * **Coverage target:** ≥ 90% for changed lines; report coverage summary.
5. **Observability plan**: emitted metrics/logs/traces with names, types, and example queries/dashboards.
6. **PE-facing change report** (see “Final deliverables” format) with before/after snippets and rationale.
7. **Rollback plan**: flag/Config switch + revert commands; documented blast radius and owner runbook pointers.

**Acceptance criteria (verify via tests & local runs)**

* Under `<<FAILURE_PATTERN_1>>` (e.g., 5xx burst), breaker **opens** within `<<N>>` requests and **stays open** for the cool-down interval.
* Under `<<RECOVERY_PATTERN>>`, half-open allows `<<M>>` probes; on `>=<<K>>` successful probes, breaker **closes**.
* p95 latency stays within `<<P95_MS>> ms` during normal load; does not exceed `<<DEGRADED_P95_MS>> ms` during Open state with fallback.
* Metrics/logs contain **state_transition** events and can drive alerts (burn-rate or SLO alerts).
* No data races in concurrent test harness (document evidence or include a concurrency test).

**Output formatting (strict)**
Return a **single Markdown document** with the following top-level sections **in order**:

1. `# ADR – Circuit Breaker for <<USE_CASE_NAME>>`
2. `# Design`

   * State machine diagram
   * Config schema
   * Thread/async model
   * Failure & recovery policy
3. `# Patch (Unified Diff)`

   * Only include diffs for files you modify/create. Use unified diff format with paths rooted at repo root (e.g., `src/circuit_breaker.py`, `tests/test_circuit_breaker.py`).
4. `# Test Plan & Coverage`

   * Test matrix table; commands to run; expected coverage summary
5. `# Observability`

   * Metrics/logs/traces spec; sample alert rules or queries
6. `# PE Review Notes`

   * Before/after code snippets (small, focused), rationale, trade-offs, risk, rollback
7. `# TODO / Follow-ups`

   * Small backlog items explicitly out of scope

**House rules**

* Do **not** add third-party deps.
* Preserve repository coding style and lint rules.
* Prefer small, composable functions; thorough docstrings.
* If an assumption is made (e.g., missing SLOs), **state it clearly** and continue.

**Self-check rubric (brief)**

* Correctness (states/thresholds/time)
* Concurrency safety
* Observability completeness
* Test depth & coverage
* Configurability & ops ergonomics
* Simplicity & maintainability

---

## Optional starter you can fill now

* `<<USE_CASES>>`: e.g., “Protect LLM completion API calls to `provider_X` from cascading failures/timeouts; fallback to cached answer or graceful message.”
* Threshold sketch: “Open on ≥50% error rate over last 20 requests or ≥5 consecutive failures; half-open after 30s with 3 probes; close on 3/3 success.”

---

### Why this works

* Forces **enterprise-grade outputs** (ADR, diff, tests, observability, rollback) your PE expects.
* Constrains the agent to **reuse in-house patterns** (no scope creep).
* Makes results **auditable and runnable** (unified diff + pytest commands).
* Anticipates **ent_pycore** import quirks with a safe fallback approach.

If you want, paste your actual `<<USE_CASES>>`, SLOs, and any ent_pycore module list you have, and I’ll slot them into the template for you.


7:15
>>>>>>>>>>>>>>>>

Great start. Your prompt has the right intent (“design a circuit breaker using the in-house library + update tests + PR for review”), but it’s missing the scaffolding that lets a capable agent act deterministically: concrete inputs/outputs, constraints, acceptance criteria, and a step plan.

Below I’ll (1) pinpoint the gaps, (2) give you a copy-paste upgraded prompt template tailored to your situation (ent_pycore + LLM circuit breaker), and (3) add a short checklist you can keep using across tasks.

---

# 1) What’s strong vs. what’s missing

**Strong**

* Clear objective: design a circuit breaker for a top-priority use case.
* Mentions internal lib (`ent_pycore`) and existing 30+ scripts.
* Requires tests and a review-ready output.

**Missing / ambiguous**

* No explicit input artifacts (where is the JSON, which repo branch?).
* No target language/runtime/test framework versions.
* No SLOs/thresholds (how to decide open/half-open/closed? which error taxonomy?).
* No output format (PR layout, folders, filenames).
* No acceptance criteria or evaluation plan (benchmarks, chaos tests, pass/fail).
* No guardrails (observability, idempotency, config flags, rollback).
* No UX/behavior definition during degrade mode.
* No security/compliance constraints.

---

# 2) Copy-paste upgraded zero-shot instruction (fill the placeholders)

> **Title:** Design & integrate an LLM circuit breaker using `ent_pycore` + test + PR
> **Audience:** Staff/Principal-level agent; produce review-ready work, not notes.

**Role & Mission**
You are a senior backend engineer. Your job: design and implement a production-grade circuit breaker for our highest-priority use case using our internal library **`ent_pycore`**. You must integrate it into the existing codebase, add tests, and produce a review-ready PR.

**Context**

* We depend on an LLM Completion API for core user workflows. Outages/timeouts currently cause cascading failures and SLO breaches.
* We have ~30 existing Python scripts in this repo that call the LLM.
  `REPO:` `<link or path>` (branch: `<branch>`).
* We have **JSON manifests** listing all available `ent_pycore` functions (module path, name, signature, docstring).
  `ARTIFACTS:` `<path/to/functions_index.json>` `<path/to/any_additional.json>`
  If runtime import of `ent_pycore` is not possible (e.g., distribution-style package), **work from the JSON** to map capabilities.

**Top-Priority Use Case**
LLM Completion call (sync path).

* **Trigger:** user submit → backend → LLM.
* **Pain:** timeouts/5xx spikes cause request storms and poor UX.
* **Goal:** protect upstream and preserve UX via circuit breaker + graceful degrade.
  Fill any details you need from code and manifests; where unknown, propose sensible defaults and call them out.

**Non-Negotiable Requirements**

* **Language/Runtime:** Python `X.Y`, pytest `Z`.
* **Observability:** emit metrics/traces/logs for: state transitions, failure counts, rolling windows, short-circuits, half-open probes, fallback activations.
* **Config:** thresholds/timeouts must be **configurable** (env or YAML), with sane defaults.
* **Safety:** idempotent operations, no sensitive data in logs, follow org logging policy.
* **Compatibility:** do not break existing public functions; add feature-flag `CB_ENABLED` default `false` (we’ll canary).
* **Docs:** create/update a `README.md` and a runbook section.

**Design Constraints & Targets**

* **States:** Closed → Open → Half-Open with exponential backoff + jitter for re-probe.
* **Trip policy:** open when any of:

  * rolling error-rate ≥ **`ERROR_RATE_THRESHOLD`** over last **`N`** calls,
  * consecutive failures ≥ **`CONSEC_FAILS`**,
  * p95 latency ≥ **`P95_LATENCY_MS`** for **`M`** windows.
* **Recovery:** allow **`K`** trial calls in Half-Open; if success-rate ≥ **`RECOVERY_RATE`**, close else re-open.
* **Partitioning:** maintain breakers per **service + tenant (or API key)** to avoid noisy-neighbor effects.
* **Fallbacks:** when open, do (in order):

  1. return cached last-good response (if TTL valid),
  2. route to a cheaper/backup model (configurable),
  3. return a minimal “degraded” response with user-safe messaging.
* **SLO alignment:** define & report: availability, p95 latency, error budget burn.

**Inputs You Can Use**

1. `functions_index.json` describing `ent_pycore` callable surface.
2. Source tree under `<repo path>`.
3. Test data samples under `<tests/data>` (or create fixtures if absent).

**Tasks (do them in order)**

1. **Inventory Mapping:** Parse the JSON and produce a table: module path, function name, signature, doc summary, probable category (metrics/timing/retries/circuit-breaker helpers). Identify the most relevant `ent_pycore` utilities for:

   * rolling windows, counters, timers, atomic state, locks, monotonic time, retry/backoff, config.
     If something is missing, propose minimal adapters.
2. **Design Spec (short):**

   * Architecture diagram (ASCII or Mermaid) showing call path and breaker boundaries.
   * Config schema (`config/circuit_breaker.yml`) with defaults and comments.
   * Error taxonomy (timeout, 429/5xx, connection errors) and which should count toward tripping.
   * Observability plan (metrics names, labels, sampling for traces).
3. **Implementation:**

   * Add a `circuit_breaker/` module with a thin, reusable wrapper (`@with_circuit_breaker` or context manager) that integrates with `ent_pycore`.
   * Wire it into the top-priority LLM call sites via dependency injection / wrapper.
   * Respect feature flag `CB_ENABLED`.
4. **Tests:**

   * Unit tests for state transitions, thresholds, backoff/jitter, half-open probes, partitioning.
   * Property-based or table-driven tests for rolling windows.
   * Integration tests that simulate spike of timeouts/5xx and verify correct degrade behavior and metrics.
   * Basic chaos test (loop: fail N% randomly) to validate stability and observability.
5. **Docs & Ops:**

   * Update `README.md` with quickstart, config, and examples.
   * Add `RUNBOOK.md` section: how to flip the flag, safe rollback, and dashboard links.
   * Provide a one-page PR description with risks and canary plan.

**Deliverables (exact output format)**

* `design/circuit_breaker_spec.md` (≤ 2 pages, includes thresholds table & Mermaid diagram)
* `circuit_breaker/` Python module (well-documented)
* Modified call sites under `<paths>` guarded by `CB_ENABLED`
* `config/circuit_breaker.yml` (defaults + comments)
* `tests/test_circuit_breaker_*.py` (pytest)
* `docs/RUNBOOK.md#circuit-breaker`
* **PR text** (markdown): summary, screenshots of metrics if possible, rollout plan, rollback plan.

**Acceptance Criteria**

* All tests pass in CI.
* In a 5-minute fault-injection run (≥50% timeouts), breaker must:

  * open within **X seconds** or **Y failed calls**,
  * cap outbound calls to ≤ **RPS limit** while open,
  * emit metrics: `cb_state{service,tenant}`, `cb_open_total`, `cb_short_circuits_total`, `cb_half_open_probe_total`, `cb_fallback_total`.
* With `CB_ENABLED=false`, behavior is unchanged (backward compatibility).

**Output Style**

* Provide concise artifacts and code blocks.
* Show rationale as **bulleted summaries only** (no raw chain-of-thought).
* Where you make assumptions, call them out at the top of each file as `ASSUMPTIONS`.

**If Something Blocks You**

* If `ent_pycore` cannot be imported, operate solely from the JSON manifest and generate shims with TODOs.
* If critical data is missing, propose defaults and proceed; list gaps in the PR.

---

### (Optional) Ready-to-use skeletons the agent can emit

**Mermaid (architecture)**

```mermaid
flowchart LR
Client-->App[App Service]
App-->CB{Circuit Breaker}
CB -->|Closed/Half-Open| LLM[LLM API]
CB -.Open/Short-circuit.-> Fallback[Cache/Backup Model/Degraded]
App-->Obs[Metrics/Tracing Logs]
```

**Example config (`config/circuit_breaker.yml`)**

```yaml
cb_enabled: false
window_size: 100            # rolling window calls
error_rate_threshold: 0.25  # 25%+ errors trip
consecutive_failures: 10
latency_p95_ms_threshold: 3000
half_open_max_probes: 5
recovery_success_rate: 0.8
open_state_backoff_ms: [1000, 2000, 4000, 8000]  # with jitter
partition_keys: ["service", "tenant"]
fallbacks:
  enable_cache: true
  cache_ttl_s: 300
  enable_backup_model: true
  backup_model_name: "gpt-lite"
```

**Pytest outline**

```python
def test_trips_on_error_rate(...): ...
def test_half_open_probes_and_reclose(...): ...
def test_short_circuit_and_fallback_cache(...): ...
def test_partitioning_isolated_states(...): ...
def test_flag_off_is_noop(...): ...
```

---

# 3) Quick quality checklist (use this every time)

* Inputs specified (repo, branch, JSON paths)?
* Runtime/toolchain versions fixed?
* SLOs/thresholds defined & justified?
* Error taxonomy + which count toward tripping?
* Configurable with safe defaults? Feature flag?
* Observability: metrics names, labels, traces, logs?
* Tests: unit + integration + chaos; pass/fail criteria?
* Canary & rollback plan documented?
* Backward compatibility guaranteed?
* Security & privacy logging rules respected?

---

If you want, paste your actual JSON manifest path + repo path and I’ll tailor the thresholds and emit a first pass of `config/circuit_breaker.yml`, the metrics schema, and a pytest starter file that matches your tree.


5:44 PM
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


Great call — if `ent_pycore` is a *distribution* with many subpackages, you’ll want a sturdier scanner. Here’s a **drop-in upgraded script** with:

* **Two scan modes**:

  1. **Import mode (default)** — imports modules and uses `inspect` (fast, accurate signatures, but may run import side-effects).
  2. **AST mode (`--ast`)** — parses `.py` files without importing (safer; signatures may be partial).
* **Top-level package scan** (not just submodules).
* **Public/private filtering** (hide names starting with `_` unless `--include-private`).
* **Markdown + JSON outputs** (to stdout and optional files).
* **Package metadata** (version, location, distribution name).
* **Graceful error capture** per module.

> Run: `python scan_ent_pycore.py`
> Safer (no imports): `python scan_ent_pycore.py --ast`
> Save files: `python scan_ent_pycore.py --json out.json --md out.md`

---

### `scan_ent_pycore.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scan a Python package (default: ent_pycore) and produce:
- JSON inventory of modules, functions, classes, signatures, first-line docs
- Markdown catalog (grouped by module)
Supports two modes:
  * Import-based inspection (default)
  * AST-only scanning (--ast) without importing modules
"""

import argparse
import importlib
import inspect
import json
import os
import pkgutil
import sys
import textwrap
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any

# Python 3.8+: importlib.metadata is stdlib (fallback for older)
try:
    from importlib import metadata as ilmd  # type: ignore
except Exception:
    ilmd = None  # pragma: no cover

@dataclass
class Item:
    name: str
    kind: str  # "function" | "class"
    signature: str
    doc_head: str
    defined_in: Optional[str] = None  # file path if known
    line_no: Optional[int] = None

@dataclass
class ModuleReport:
    module: str
    error: Optional[str] = None
    items: List[Item] = None  # type: ignore

def get_package_metadata(pkg_name: str) -> Dict[str, Any]:
    meta = {"package": pkg_name}
    try:
        mod = importlib.import_module(pkg_name)
        meta["location"] = getattr(mod, "__file__", None)
        meta["package_dir"] = os.path.dirname(meta["location"]) if meta["location"] else None
    except Exception as e:
        meta["import_error"] = repr(e)
        return meta

    # Distribution metadata (best-effort)
    if ilmd is not None:
        try:
            # Some packages use different "distribution" names than import name
            # Try direct match; if fails, scan distributions and match top-level files
            dist_version = None
            dist_name = None
            try:
                dist_version = ilmd.version(pkg_name)
                dist_name = pkg_name
            except Exception:
                # fallback: pick first distribution whose files include the package directory name
                for dist in ilmd.distributions():
                    name = dist.metadata.get("Name") or dist.metadata.get("name")
                    if not name:
                        continue
                    try:
                        files = list(dist.files or [])
                    except Exception:
                        continue
                    for f in files:
                        parts = str(f).split("/")
                        if parts and parts[0] == pkg_name:
                            dist_name = name
                            dist_version = dist.version
                            break
                    if dist_name:
                        break
            if dist_name:
                meta["distribution"] = dist_name
            if dist_version:
                meta["version"] = dist_version
        except Exception:
            pass
    return meta

def should_keep(name: str, include_private: bool) -> bool:
    if include_private:
        return True
    return not name.startswith("_")

def doc_head(obj) -> str:
    d = inspect.getdoc(obj) or ""
    # First 1–2 lines (short)
    lines = d.strip().splitlines()
    return " ".join(lines[:2]).strip()

def inspect_module(module, include_private: bool) -> List[Item]:
    out: List[Item] = []
    for n, obj in inspect.getmembers(module):
        if not should_keep(n, include_private):
            continue
        if inspect.isfunction(obj) or inspect.isbuiltin(obj):
            k = "function"
        elif inspect.isclass(obj):
            k = "class"
        else:
            continue
        try:
            try:
                sig = str(inspect.signature(obj))
            except Exception:
                sig = "(signature unavailable)"
            try:
                srcfile = inspect.getsourcefile(obj) or inspect.getfile(obj)
            except Exception:
                srcfile = None
            try:
                _, lineno = inspect.getsourcelines(obj)
            except Exception:
                lineno = None
            out.append(Item(
                name=n,
                kind=k,
                signature=sig,
                doc_head=doc_head(obj),
                defined_in=srcfile,
                line_no=lineno
            ))
        except Exception:
            # Continue collecting even if one member fails
            continue
    return out

def walk_import(pkg_name: str, include_private: bool) -> List[ModuleReport]:
    reports: List[ModuleReport] = []

    # Include the top-level package itself
    try:
        root = importlib.import_module(pkg_name)
        items = inspect_module(root, include_private)
        reports.append(ModuleReport(module=pkg_name, items=items))
    except Exception as e:
        reports.append(ModuleReport(module=pkg_name, error=repr(e), items=[]))

    # Walk submodules
    try:
        root = importlib.import_module(pkg_name)
        for m in pkgutil.walk_packages(root.__path__, root.__name__ + "."):
            name = m.name
            try:
                mod = importlib.import_module(name)
                items = inspect_module(mod, include_private)
                reports.append(ModuleReport(module=name, items=items))
            except Exception as e:
                reports.append(ModuleReport(module=name, error=repr(e), items=[]))
    except Exception as e:
        # If __path__ is unavailable or walk fails
        reports.append(ModuleReport(module=f"{pkg_name}.__walk__", error=repr(e), items=[]))

    return reports

# ---------- AST Mode (no import) ----------
import ast

class AstVisitor(ast.NodeVisitor):
    def __init__(self, include_private: bool):
        self.include_private = include_private
        self.items: List[Item] = []

    def visit_FunctionDef(self, node: ast.FunctionDef):
        if should_keep(node.name, self.include_private):
            sig = "(parameters unknown via AST)"
            # best-effort build signature string
            try:
                params = []
                for a in node.args.args:
                    params.append(a.arg)
                if node.args.vararg:
                    params.append("*" + node.args.vararg.arg)
                for a in node.args.kwonlyargs:
                    params.append(a.arg)
                if node.args.kwarg:
                    params.append("**" + node.args.kwarg.arg)
                sig = "(" + ", ".join(params) + ")"
            except Exception:
                pass
            doc = ast.get_docstring(node) or ""
            head = " ".join((doc.strip().splitlines() or [""])[:2]).strip()
            self.items.append(Item(
                name=node.name,
                kind="function",
                signature=sig,
                doc_head=head,
                defined_in=None,
                line_no=getattr(node, "lineno", None)
            ))
        self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef):
        if should_keep(node.name, self.include_private):
            # Constructors' signatures are not trivial from AST; omit
            doc = ast.get_docstring(node) or ""
            head = " ".join((doc.strip().splitlines() or [""])[:2]).strip()
            self.items.append(Item(
                name=node.name,
                kind="class",
                signature="(inspect via import for full signature)",
                doc_head=head,
                defined_in=None,
                line_no=getattr(node, "lineno", None)
            ))
        self.generic_visit(node)

def is_package_dir(path: str) -> bool:
    return os.path.isdir(path) and os.path.isfile(os.path.join(path, "__init__.py"))

def walk_ast(pkg_name: str) -> List[ModuleReport]:
    # Locate the package directory by importing just to find __file__ (not executing submodules)
    reports: List[ModuleReport] = []
    try:
        mod = importlib.import_module(pkg_name)
        base_file = getattr(mod, "__file__", None)
        if not base_file:
            return [ModuleReport(module=pkg_name, error="Cannot locate package file", items=[])]
        base_dir = os.path.dirname(base_file)
    except Exception as e:
        return [ModuleReport(module=pkg_name, error=f"Could not import root to locate files: {e!r}", items=[])]

    # Walk the directory for .py files
    for root, dirs, files in os.walk(base_dir):
        # Respect package boundaries (skip hidden dirs, __pycache__)
        dirs[:] = [d for d in dirs if not d.startswith(".") and d != "__pycache__"]
        for f in files:
            if not f.endswith(".py"):
                continue
            full = os.path.join(root, f)
            rel = os.path.relpath(full, base_dir)
            # Compute module name
            modname_parts = [pkg_name] + rel[:-3].split(os.sep)  # strip .py
            if modname_parts[-1] == "__init__":
                modname_parts = modname_parts[:-1]
            module_name = ".".join(modname_parts)

            try:
                with open(full, "r", encoding="utf-8") as fh:
                    src = fh.read()
                tree = ast.parse(src, filename=full)
                visitor = AstVisitor(include_private=args.include_private)
                visitor.visit(tree)
                # attach file path for each item
                for it in visitor.items:
                    it.defined_in = full
                reports.append(ModuleReport(module=module_name, items=visitor.items))
            except Exception as e:
                reports.append(ModuleReport(module=module_name, error=repr(e), items=[]))

    return reports

def to_markdown(meta: Dict[str, Any], reports: List[ModuleReport]) -> str:
    header = [
        f"# Package Catalog: `{meta.get('package', 'unknown')}`",
        "",
        f"- **Distribution**: {meta.get('distribution', 'unknown')}  ",
        f"- **Version**: {meta.get('version', 'unknown')}  ",
        f"- **Location**: {meta.get('package_dir', meta.get('location', 'unknown'))}",
        "",
        "---",
        ""
    ]
    parts = ["\n".join(header)]
    for rep in sorted(reports, key=lambda r: r.module):
        parts.append(f"## Module: `{rep.module}`")
        if rep.error:
            parts.append(f"> ❗ Error: `{rep.error}`\n")
            continue
        if not rep.items:
            parts.append("_No public functions/classes found._\n")
            continue
        parts.append("| Name | Kind | Signature | Doc (first line) | File | Line |")
        parts.append("|------|------|-----------|------------------|------|------|")
        for it in rep.items:
            sig = it.signature.replace("|", "\\|")
            doc = (it.doc_head or "").replace("|", "\\|")
            file_disp = (it.defined_in or "").replace("|", "\\|")
            line_disp = it.line_no if it.line_no is not None else ""
            parts.append(f"| `{it.name}` | {it.kind} | `{sig}` | {doc} | {file_disp} | {line_disp} |")
        parts.append("")  # spacing
    return "\n".join(parts)

def main(args):
    pkg = args.package
    meta = get_package_metadata(pkg)

    if args.ast:
        reports = walk_ast(pkg)
    else:
        reports = walk_import(pkg, include_private=args.include_private)

    # Serialize JSON
    j = {
        "meta": meta,
        "reports": [asdict(r) for r in reports],
    }
    json_text = json.dumps(j, indent=2)

    # Build Markdown
    md_text = to_markdown(meta, reports)

    # Output to stdout
    if not args.quiet:
        print(json_text)

    # Optional file saves
    if args.json:
        with open(args.json, "w", encoding="utf-8") as f:
            f.write(json_text)
    if args.md:
        with open(args.md, "w", encoding="utf-8") as f:
            f.write(md_text)

    # If user only wants Markdown on stdout
    if args.markdown_stdout:
        sys.stdout.write(md_text)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Scan a Python package (default ent_pycore) and produce JSON/Markdown catalogs."
    )
    parser.add_argument("--package", "-p", default="ent_pycore", help="Package name to scan")
    parser.add_argument("--ast", action="store_true",
                        help="Use AST-only mode (no imports, safer, less precise signatures)")
    parser.add_argument("--include-private", action="store_true",
                        help="Include names starting with underscore")
    parser.add_argument("--json", help="Path to save JSON output")
    parser.add_argument("--md", help="Path to save Markdown output")
    parser.add_argument("--markdown-stdout", action="store_true",
                        help="Print Markdown catalog to stdout (in addition to JSON unless --quiet)")
    parser.add_argument("--quiet", action="store_true",
                        help="Do not print JSON to stdout")
    args = parser.parse_args()
    main(args)
```

---

### Tips for big distributions

* If importing submodules triggers heavy side-effects, use **`--ast`** first to get the structure without execution, then selectively re-run **without `--ast`** for the few modules you care about (use `--package ent_pycore.subpkg` to narrow scope).
* For “namespace” packages or C-extensions, AST mode will skip compiled modules; import mode will list them (signatures may still be limited).

---

If you run this and share either **`out.json`** or **`out.md`**, I’ll turn it into:

* a **Function Catalog**,
* a **Category → Use Case** map,
* and **production-ready recipes** (e.g., circuit breaker, retries/backoff, caching, telemetry hooks) using `ent_pycore`.


2:11 PM
>>>>>>>>>>>>>>>>>

Got it — let’s break this down systematically.

You’re writing a **zero-shot instruction prompt** whose goal is:
➡️ Guide the most capable AI agent to deeply analyze the internal Python library `ent_pycore`, understand its functions, and propose logic/designs that can be built using it.

---

## 🔎 Analysis of Your Current Prompt

**Your version:**

> Pip install ent_pycore.
> We have ent_pycore inbuilt library in our org, and I want to analyse all the functions in the library ent_pycore and help me to achieve this technically and see what all logic is possible with the internaly python library.
> Add if I have missed anything like Like-a Google engineer do or someone work in billion dollar startup.
> And I want you to think like high performing software engineer.

### Characteristics

1. **Direct but unstructured** – you’re stating the goal, but not guiding *how* the agent should analyze, organize, or present results.
2. **High ambition tone** – asking it to “think like a Google engineer / billion-dollar startup” sets quality expectations, but it’s vague.
3. **Task ambiguity** – "analyze all functions" could mean: generate documentation, usage examples, dependencies, design best practices, or even building new patterns (circuit breaker, retry, etc.).
4. **Missing structured output request** – you don’t specify how the answer should be organized (tables, code snippets, summaries, examples).
5. **No reasoning chain request** – you want deep reasoning, but the current text doesn’t explicitly demand step-by-step thought.

---

## ⚡ Suggestions to Improve Prompt

Here’s how to make it sharper for **zero-shot instruction tasks**:

### 1. Explicit Task Breakdown

Instead of just “analyze all functions,” tell the agent *how* to analyze:

* Extract all function names and signatures.
* Categorize by purpose (e.g., networking, retry, circuit-breaker, monitoring).
* Show possible use cases.
* Suggest best practices for enterprise-scale systems.

### 2. Require Reasoning Chain

Ask the agent to:

* Explain reasoning behind classification.
* Compare different function design patterns.
* Show why a certain function is suitable for a use case.

### 3. Structured Output Format

Specify you want output in sections/tables, for clarity:

* **Function Catalog** (name, signature, short description).
* **Category/Use Case Mapping**.
* **Design Patterns Possible** (e.g., circuit breaker, retry logic, caching).
* **Code Examples** (with explanations).
* **Improvement Suggestions** (what is missing, how a Google-level engineer would extend it).

### 4. Strengthen the "high-performance engineer" framing

Instead of “think like Google engineer,” say:

> “Think like a senior engineer at a FAANG-scale company: prioritize scalability, fault-tolerance, and clean abstractions.”

---

## 📝 Refined Prompt (Improved Version)

Here’s a stronger **zero-shot instruction prompt** you could use:

---

**Prompt:**
You are acting as a **high-performing senior software engineer at a FAANG-scale company**.

We have an internal Python library called **`ent_pycore`**. Your task is to:

1. **Analyze all functions** in the library and extract their purpose, signatures, and dependencies.
2. **Categorize functions** by use case (e.g., retries, error handling, monitoring, networking, data processing).
3. **Propose enterprise-grade design patterns** (e.g., circuit breaker, retry with backoff, observability hooks) that can be built using these functions.
4. **Show code examples** that demonstrate how to combine these functions into robust, production-ready solutions.
5. **Explain reasoning step-by-step**, highlighting trade-offs and why a design is scalable or resilient.
6. **Suggest improvements** or missing capabilities — what else a world-class engineering team would add.

**Output format:**

* Section 1: Function Catalog (table of functions with short description).
* Section 2: Categorization & Use Cases.
* Section 3: Proposed Designs & Patterns (with code).
* Section 4: Step-by-Step Reasoning.
* Section 5: Improvement & Future Enhancements.

---

👉 This way, the agent knows:

* the **goal** (analyze & apply functions),
* the **method** (structured reasoning),
* the **output style** (tables + code + explanation),
* the **quality bar** (world-class engineer thinking).

---

Would you like me to **rewrite your exact original wording** into this improved form (keeping your style but sharpening structure), or do you prefer a **more concise minimalistic version** you can drop directly into your system as a one-liner?


Short answer: I can’t install your internal package here or reach your private registry. I also can’t see `ent_pycore` unless you share the code (repo/zip/wheel) or paste docs/stubs.

That said, I can still help in two solid ways:

## Option A — You provide artifacts, I analyze now

Share **any one** of these and I’ll proceed immediately:

* The repo (or a zip/tarball) of `ent_pycore`
* A wheel (`.whl`) or sdist (`.tar.gz`)
* Generated docs or `.pyi` stubs
* Even a single module you care about most

### My analysis plan (once I have it)

1. **Inventory:** Parse modules → list functions/classes, signatures, docstrings.
2. **Categorize:** Group by capability (I/O, retries, CB, caching, telemetry, auth, utils).
3. **Pattern map:** Identify what enterprise patterns are already enabled (circuit breaker, exponential backoff, bulkhead, idempotency, tracing) and what’s missing.
4. **Usage recipes:** Produce production-grade examples (with error handling, logging, metrics, configuration via env/flags, test doubles).
5. **Gaps & proposals:** Concrete PR-style suggestions (APIs to add, breaking-change risks, test matrix, SLO/observability hooks).

I’ll deliver:

* A **Function Catalog table**
* **Design patterns** you can implement today using `ent_pycore`
* **Sample code** (prod-ready) + **pytest** tests
* A short **“What a FAANG-level team would add next”** section

## Option B — Run this locally, then paste results back

If you can run Python in your environment, this script will generate a JSON inventory I can analyze without needing the code itself:

```python
# save as scan_ent_pycore.py and run: python scan_ent_pycore.py
import importlib, inspect, json, pkgutil, sys

PKG = "ent_pycore"
mod = importlib.import_module(PKG)

def walk(pkg_name):
    seen = set()
    for m in pkgutil.walk_packages(mod.__path__, mod.__name__ + "."):
        name = m.name
        if name in seen: 
            continue
        seen.add(name)
        try:
            module = importlib.import_module(name)
        except Exception as e:
            yield {"module": name, "error": repr(e)}
            continue
        items = []
        for n, obj in inspect.getmembers(module):
            if inspect.isfunction(obj) or inspect.isclass(obj):
                try:
                    sig = str(inspect.signature(obj))
                except Exception:
                    sig = "(signature unavailable)"
                doc = (inspect.getdoc(obj) or "").splitlines()[0:3]
                items.append({
                    "name": n,
                    "kind": "class" if inspect.isclass(obj) else "function",
                    "signature": sig,
                    "doc_head": " ".join(doc)
                })
        yield {"module": name, "items": items}

result = list(walk(PKG))
print(json.dumps(result, indent=2))
```

* Run it where `ent_pycore` is installed.
* Paste the JSON output here. I’ll turn it into the Function Catalog + patterns + code examples.

## Option C — Prompt for an internal agent that *can* install

If you have an internal runner/agent with network access, use this zero-shot prompt there:

> **Role:** Senior Staff Engineer.
> **Task:** Install and analyze the internal Python library `ent_pycore`.
> **Steps:**
>
> 1. `pip install ent_pycore` (using the corporate index if required).
> 2. Enumerate modules, functions, classes, and signatures; capture first-line docstrings.
> 3. Categorize capabilities (retries/backoff, circuit breaker, caching, I/O, telemetry, auth, utils).
> 4. Propose enterprise patterns implementable with current APIs (circuit breaker, bulkhead, idempotency, request hedging, structured logging/metrics/traces).
> 5. Produce production-ready Python examples + pytest tests for each pattern.
> 6. Identify gaps and propose API changes (backwards-compatible where possible), include observability & configurability (env vars/flags).
>    **Output format:**
>
> * Section 1: Function Catalog (table: module, name, kind, signature, doc_head).
> * Section 2: Category → Functions mapping.
> * Section 3: Design patterns with code + tests.
> * Section 4: Reasoning & trade-offs.
> * Section 5: Gaps & roadmap (MVP, nice-to-have, risks).
>   **Quality bar:** FAANG-scale reliability, clarity, and testability; align with SLOs, tracing, and error-budget thinking.

---

Pick the path that fits your environment. If you can share the repo/zip or the JSON inventory, I’ll dive straight into the analysis and produce the catalog, designs, and code.




9:38
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Got it—your goal is a **zero-shot, do-it-all prompt** that reliably guides a capable agent to (1) understand your repo + `ent_pycore`, (2) design a production-grade **circuit breaker** for critical LLM calls, and (3) deliver code + tests + rationale at a “Google-ish/billion-$ startup” bar.

Below is a tight upgrade: first, a quick critique so you know what’s missing, then a **drop-in prompt template** you can paste into your system with strong structure, guardrails, and outputs.

---

# What to improve (fast critique)

1. **Ambiguity about inputs & scope**
   “30+ scripts” and “analyze all functions” is open-ended. Agents wander. Anchor scope: target directories, file patterns, key entrypoints.

2. **Tooling/Capabilities not explicit**
   If the agent can inspect files, run code, or only reason statically? Say it. If it must infer `ent_pycore` APIs, tell it how (read docs, grep usage).

3. **Outcome format not enforced**
   You want design, code, and tests. Force **structured sections** + acceptance criteria.

4. **Prod-readiness criteria missing**
   Define SLIs/SLOs, failure modes, fallback policy, telemetry, config flags, and rollback plan.

5. **Test strategy underspecified**
   Require unit + integration tests (happy/timeout/error/open/half-open/closed), fault injection, and deterministic seed.

6. **Reviewability**
   Ask for a **PR-style patch** (diffs), `README.md` updates, and CHANGELOG notes.

---

# Upgraded Zero-Shot Prompt (paste this as your instruction)

> **Role**: You are a senior platform/SRE-minded software engineer designing a production-grade **circuit breaker** for mission-critical LLM API calls. Think rigorously, write reviewable code, and justify decisions with measurable reliability goals.
>
> **Context**:
>
> * Codebase: ~30+ scripts in a GitHub repo (Python).
> * Internal library: `ent_pycore` (installed via `pip install ent_pycore`). Treat it as first-class; prefer its primitives over ad-hoc code.
> * Objective: Protect user experience during LLM outages/timeouts while meeting reliability goals and preserving throughput under normal load.
>
> **Repository Scope to Analyze**:
>
> * Focus on files under `src/` and `services/` containing LLM calls (search terms: `openai`, `llm`, `completion`, `chat`, `inference`, `client.*create`, `requests.post` to model endpoints).
> * Identify: call sites, retry logic, timeouts, error handling, and any existing rate limiting/backoff.
> * Map the call-graph for top 2 critical entrypoints (by traffic or business priority).
>
> **What you must deliver** (strict structure):
>
> 1. **Executive Summary (≤10 bullets)**
>
>    * Problem statement, constraints, proposed pattern (circuit breaker), expected impact.
> 2. **Requirements & Reliability Goals**
>
>    * Define SLIs (success rate, end-to-end latency p90/p95, fallback hit-rate).
>    * Propose SLOs (e.g., success ≥ 99.5%, p95 latency ≤ 2.5s under nominal load).
>    * Non-functionals: observability, configurability, blast-radius limits.
> 3. **Design (with diagrams-in-text)**
>
>    * Circuit breaker states (Closed, Open, Half-open) and transitions.
>    * **Trip conditions** (e.g., rolling window error-rate > X%, consecutive timeouts > N, upstream 5xx > M).
>    * **Recovery probe** policy (half-open sample size, jittered probe interval).
>    * **Fallbacks** in priority order (cached response → narrowed model → stub/template → user-facing graceful degradation).
>    * **Configuration** (env vars or config file): timeouts, thresholds, window size, max concurrent probes, allowlist endpoints.
>    * **Thread/async model** and concurrency safety.
> 4. **Implementation Plan (PR-Checklist)**
>
>    * New module(s) (paths, filenames) and changes to existing files.
>    * Backward compatibility and kill-switch/feature flag (`CB_ENABLED`).
>    * Telemetry events (names, fields) and log levels.
> 5. **Code**
>
>    * A self-contained `circuit_breaker.py` (or package) using **`ent_pycore`** where appropriate.
>    * Adapters/wrappers for current LLM clients (minimal surface change at call sites).
>    * Config loader + sane defaults.
> 6. **Tests**
>
>    * Unit tests covering: Closed→Open, Open→Half-open, Half-open→Closed; timeout bursts; mixed failures; fallback correctness; concurrency.
>    * Integration test with a stubbed LLM endpoint (fast, deterministic).
>    * Property-style test for rolling-window math.
> 7. **Operations**
>
>    * Dashboards (metrics list), alerts (burn-rate style), SLO doc snippet.
>    * Runbook: how to enable/disable, tune thresholds, interpret metrics, rollback plan.
> 8. **STAR Rationale**
>
>    * Situation, Task, Actions (trade-offs), Results (expected metrics deltas, e.g., p95 ↓, error budget burn ↓).
> 9. **Patch Output**
>
>    * Present changes as a **single PR-style diff** (filenames + unified diff).
>    * Include `README.md` section “Circuit Breaker Overview” and `CHANGELOG.md` entry.
>
> **Constraints & Guardrails**:
>
> * **No breaking public APIs** unless adapter provided.
> * Prefer **idempotent** operations; document any non-idempotent side-effects.
> * All constants are configurable (env or `config.yaml`), with safe defaults.
> * Add **metrics** (counter/gauge/histogram) and **structured logs** at state transitions.
> * Write code in **Python 3.10+**, type-annotated, lint-clean (ruff/flake8), and documented (docstrings).
>
> **Assumptions re: `ent_pycore`** (adapt if real APIs differ):
>
> * If `ent_pycore` has utilities for:
>
>   * **circuit breakers**, **rate limiters**, **rolling windows**, **metrics/logging**, or **feature flags**, **use them** and show how.
>   * If unknown, **introspect by reading its package modules** and **grep existing repo usage**; then state the chosen functions/classes and why.
>
> **Exact Output Format** (important):
>
> * Use the section headers 1–9 above.
> * All code blocks must be runnable and complete.
> * Tests must run via `pytest -q`. Provide a short command list to execute locally.
>
> **Acceptance Criteria** (the work is “done” when):
>
> * Unit + integration tests pass locally.
> * A demo script shows tripping and recovery behavior deterministically.
> * Metrics names + labels are listed and mapped to dashboards/alerts.
> * A kill-switch exists and works (`CB_ENABLED=false`).
> * Risk analysis + rollback steps are documented.

---

## Ready-to-use “micro” variant (if you need a shorter prompt)

> Design and implement a production-grade **circuit breaker** for our LLM calls in this Python repo, prioritizing: (1) reliability SLOs, (2) graceful degradation, (3) observability, (4) configurability. Analyze `src/` and `services/` for LLM call sites; prefer **`ent_pycore`** primitives. Deliver: (1) executive summary, (2) SLIs/SLOs, (3) design with states/thresholds/fallbacks, (4) PR-style patch (code + tests + config), (5) ops guide (metrics, alerts, runbook), (6) STAR rationale. Code must be typed, lint-clean, pytest-covered, include a kill-switch, and expose metrics + logs at state transitions.

---

## Optional code skeleton (helps the agent land consistently)

```python
# src/platform/circuit_breaker.py
from dataclasses import dataclass
from typing import Callable, Any, Tuple
import time
# from ent_pycore.observability import metrics, log
# from ent_pycore.controls import RollingWindow, FeatureFlag, CircuitState  # hypothetical

@dataclass
class CBConfig:
    error_rate_threshold: float = 0.25
    min_samples: int = 50
    rolling_window_sec: int = 60
    open_cooldown_sec: int = 20
    half_open_probe_count: int = 5
    request_timeout_sec: float = 8.0
    enabled: bool = True

class CircuitBreaker:
    # implement Closed/Open/Half-open with rolling window and timed cooldown
    ...
```

Tests should simulate: steady success, burst failures→trip, cooldown→probe→recover, and verify metrics/logs.

---

## Quick reasoning chain the agent should follow (baked into the structure above)

1. Identify hot paths → quantify failure/latency patterns.
2. Choose thresholds grounded in SLIs/SLOs.
3. Implement breaker with safe defaults + feature flag.
4. Wrap LLM calls; add fallbacks for degraded mode.
5. Add metrics/logs; write tests (unit/integration/fault-injection).
6. Provide PR diff + runbook + STAR results.

---

If you want, share a couple of filenames that actually call the LLM (e.g., `services/llm_client.py`, `src/handlers/generate.py`) and I’ll tailor the design thresholds and a concrete wrapper that uses your real `ent_pycore` primitives.


9:15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

awesome problem. here’s a sharper, production-ready way to brief a capable agent—plus a filled-in version for your two use cases, and a quick sanity checklist.

# 1) what’s missing in your current prompt (and why it matters)

* **Clear objective & scope**: say exactly what “done” looks like (PRs, tests, dashboards, runbook).
* **Assumptions policy**: forbid clarifying questions; require the agent to state assumptions & proceed.
* **Non-functional constraints**: SLOs, latency budgets, token/cost budgets, concurrency limits, PII/compliance.
* **Observability & rollback**: metrics, alerts, chaos tests, feature flags, rollback steps.
* **Testing matrix**: unit, integ, fault-injection, load, resilience.
* **STAR narrative**: ask for an explicit Situation→Task→Action→Result write-up.
* **Review criteria**: a checklist/rubric the PE team can use to accept/reject.
* **Change safety**: canary plan, blast radius, and backout commands.

---

# 2) zero-shot instruction prompt (template)

> copy/paste and fill <> where needed. this keeps the agent decisive and output-oriented.

**ROLE & MINDSET**
Act as a senior software engineer/SRE at a high-scale company. Do not ask questions. If information is missing, make conservative, clearly labeled assumptions and proceed.

**OBJECTIVE**
Design and implement a **circuit breaker** for the specified services to protect user experience, control LLM spend, and prevent cascading failures. Deliver production-grade code and an end-to-end plan.

**CODEBASE CONTEXT**

* Repo: `<org>/<repo>` (monorepo; primary languages: `<languages>`)
* Services in scope: `<service names / paths>`
* Environment: Dev/UAT/Prod with feature flags via `<flag system>`
* Dependencies: LLM API `<provider(s)>`, DB `<type>`, queue `<type>`

**CONSTRAINTS & SLOs**

* User-facing latency P95 ≤ `<X ms>`; error budget `<Y %>` per 30 days
* LLM spend: max `<tokens/min>` and `<$/day>`; shed load if exceeded
* Timeouts: `<client timeout>`; retries: capped with exponential backoff + jitter
* Concurrency caps: `<value>` per instance; global rate limit `<value>`
* PII/compliance: no PII in logs; enable structured logging

**DELIVERABLES**

1. **Design doc** (≤2 pages) with: state machine, thresholds, fallback paths, backpressure, and recovery policy. Include Mermaid diagram.
2. **Implementation**: PR(s) that add circuit breaker middleware/util(s) + configuration (env-driven).
3. **Tests**: unit + integration + fault-injection (simulate timeouts, 5xx, time-window failure %).
4. **Observability**: metrics names, dashboards, and alert rules.
5. **Runbook**: ops steps, feature-flag toggles, rollback commands.
6. **STAR** summary of the work (Situation, Task, Actions, Results).
7. **Review note for PE**: risks, tradeoffs, and how to validate locally & in canary.

**EVALUATION / ACCEPTANCE CRITERIA**

* Breaker transitions correctly (Closed→Open→Half-Open) using rolling window failure rate or consecutive errors.
* No request storms: retries are bounded, with jitter; concurrency & rate limits enforced.
* For write-paths: durability guaranteed (queue + idempotency key); zero data loss on transient DB outage.
* Observability: metrics and alerts fire in controlled chaos tests; dashboards show breaker state & rates.
* Cost guardrails: breaker opens on spend spikes or token rate breach.
* All tests pass; canary shows ≤ `<delta>` regression on latency/error rate.

**PROCESS**

* Make and list assumptions.
* Produce: (a) design doc, (b) code diff, (c) tests, (d) ops assets, (e) STAR summary.
* Output everything inline, with code blocks and filenames.

---

# 3) filled-in prompt for your two use cases

**ROLE & MINDSET**
Act as a senior SWE/SRE. Do not ask questions. If details are missing, assume safe defaults and record them.

**IN-SCOPE USE CASES**
A) **LLM Completion (read path)** – Core UX; timeouts/outages break workflows and burn error budget.
B) **Feedback/Analytics Write (write path)** – Compliance/audit-critical; DB outages risk data loss.

**OBJECTIVE**
Design and implement circuit breakers for A & B that:

* protect UX and SLOs,
* cap cost & token usage,
* prevent cascading failures,
* preserve/queue writes and alert on failures.

**CODEBASE**

* Repo: `<org>/genai-platform`
* Services: `api/llm_gateway` (TypeScript/Node) and `svc/telemetry_writer` (Python/FastAPI)
* Env: Dev/UAT/Prod; feature flags via `FF_CIRCUIT_BREAKER_{LLM,WRITE}`

**KEY CONSTRAINTS**

* SLOs: P95 latency ≤ 1200 ms for LLM completions; monthly availability ≥ 99.9%
* LLM budgets: ≤ 60k tokens/min per tenant; daily spend cap `<$>` configurable
* Timeouts: upstream LLM 2.0 s; client 2.5 s; retry: max 2 with exp backoff (100–600 ms) + jitter
* Concurrency: 64 per instance; global qps per tenant set via token bucket
* DB writes: at-least-once via durable queue (e.g., SQS/Kafka) with idempotency keys
* No PII in logs; trace with `trace_id`, `tenant_id`, `breaker_state`

**DELIVERABLES**

1. **Design doc** (≤2 pages) including Mermaid diagrams of both breakers.
2. **Code**

   * `api/llm_gateway`: CircuitBreaker middleware (Closed/Open/Half-Open), fallback: cached/template response for non-critical prompts, plus shed-load when cost/token budgets breach.
   * `svc/telemetry_writer`: CircuitBreaker guarding DB client; on Open, enqueue to durable queue; consumer drains with idempotency & DLQ.
3. **Tests**

   * Unit: state transitions, rolling window failure %, budget gating, retry caps.
   * Integration: fault-injection (LLM 5xx/timeout spikes; DB connection refused).
   * Load/chaos: verify alerts, ensure no thundering herd, measure P95.
4. **Observability** (Prometheus/Grafana or equivalent)

   * Metrics: `breaker_state{service}`, `breaker_open_total`, `failure_rate`, `timeout_total`, `retry_total`, `queued_writes`, `dlq_depth`, `token_rate`, `spend_rate_usd_per_min`
   * Alerts:

     * LLM: breaker Open > 1 min OR failure_rate ≥ 50% over 2 min OR token_rate/spend breach for 3 consecutive mins.
     * Writes: queue lag > 2 min OR DLQ depth > 0 OR Open > 30 s.
5. **Runbook**: flag toggles, raising limits, draining DLQ, recovering DB, backout steps.
6. **STAR**: concise narrative of the problem and results.
7. **PE Review Note**: risk, tradeoffs (e.g., hedging cost vs latency), and validation commands.

**ACCEPTANCE CRITERIA**

* LLM: Under induced 50% upstream failure for 2 min, breaker opens within 5–10 s, caps retries, P95 ≤ 1.5× baseline, error rate ≤ budget, spend spike is contained.
* Writes: On DB outage for 5 min, 0 lost records; queue builds then drains to 0 within 10 min post-recovery; duplicates suppressed via idempotency key.
* All tests pass; dashboards accurate; alerts verified in chaos runs.

**OUTPUT FORMAT**
Produce in this order:

1. Assumptions list
2. Design doc (with Mermaid diagrams)
3. Code diffs (filenames + code blocks)
4. Test code & how to run
5. Observability spec (metrics, dashboards JSON if applicable)
6. Runbook & rollback
7. STAR summary
8. PE review checklist mapping

---

# 4) pocket designs (for the agent to implement)

**LLM breaker (TypeScript sketch)**

```ts
// api/llm_gateway/middleware/circuitBreaker.ts
type State = "CLOSED" | "OPEN" | "HALF_OPEN";
export function createBreaker(opts: {
  failureThresholdPct: number; // e.g., 50 over window
  windowSecs: number;          // e.g., 20
  openCooldownMs: number;      // e.g., 5000
  halfOpenProbe: number;       // e.g., 5 requests
  maxRetries: number;          // e.g., 2 w/ jittered backoff
  tokenBudgetPerMin: number;   // per-tenant
  spendBudgetPerMinUSD: number;
}) { /* implement rolling counters, budgets, and state machine */ }
```

**Write-path breaker + queue (Python sketch)**

```python
# svc/telemetry_writer/breaker.py
class CircuitBreaker:
    # CLOSED -> OPEN (on failure rate or consecutive errors)
    # OPEN -> HALF_OPEN after cooldown
    # HALF_OPEN -> CLOSED on probe success; else OPEN
    ...
# On OPEN: push record to durable queue with idempotency_key = sha256(tenant_id|event_id|ts)
# Consumer: reads queue, writes with upsert on idempotency_key; on permanent failure -> DLQ.
```

**Mermaid (example)**

```mermaid
stateDiagram-v2
    [*] --> CLOSED
    CLOSED --> OPEN: failure_rate>=threshold OR budget_breach
    OPEN --> HALF_OPEN: cooldown elapsed
    HALF_OPEN --> CLOSED: probeSuccess>=N
    HALF_OPEN --> OPEN: probeFailure>0
```

---

# 5) review checklist (paste into PR)

* [ ] Assumptions documented; no open questions.
* [ ] Breaker thresholds, windows, cooldowns are config-driven.
* [ ] Retries capped + jitter; no unbounded fan-out.
* [ ] Cost/token budgets enforced; load-shedding in place.
* [ ] Write-path durability: queue, idempotency, DLQ; zero data loss in tests.
* [ ] Metrics/alerts wired; dashboards render breaker state & lag.
* [ ] Chaos/fault-injection scripts included; alerts verified.
* [ ] Feature-flagged; canary & rollback steps documented.

If you want, I can now generate the exact design doc + code stubs + tests following this prompt format.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Awesome brief. Here’s a battle-tested, zero-shot prompt you can drop into your agent—plus add-ons and a tiny example of the expected answer shape. It’s written to elicit Google-level design depth without forcing the model to reveal private reasoning.

---

# Zero-Shot Instruction Prompt (Final)

**Role & Context**
You are a principal software engineer designing **production-grade circuit breakers** for a large LLM application with ~30 Python scripts (micro-tools, services, CLIs). Assume an elite engineering bar (Google/billion-dollar startup).

**Inputs**

* Repository snapshot (files, tree, or links)
* Runtime context: Python 3.x, Streamlit/FastAPI workers, LLM provider(s) (e.g., Gemini/OpenAI), pptx rendering, caching/queue layers (if any)

**Primary Objective**

1. **Discover the top two business-critical use cases** in the repo and justify why they’re top priority (traffic, revenue/user impact, SLO/SLA/error-budget risk, fan-out).
2. **Design circuit breakers for each use case** with detailed, production-ready guidance.

**Deliverables (must follow exactly)**
A. **Bird’s-eye map (1–2 pages max)**

* System diagram of major scripts/services and call graph to external dependencies (LLM API, storage, DB, network).
* Hot paths and shared libraries that concentrate risk.
* Error taxonomy: timeouts, 5xx/429, transport errors, parser/JSON errors, quota/exhaustion, idempotency risks.

B. **Top-2 Use Cases (selection rationale)**

* Table with: UseCase, CriticalityReason, SLI/SLO at risk (p95 latency, success rate), Current Fail Modes, Estimated Blast Radius.

C. **Circuit Breaker Design per Use Case (deep dive)**
For each use case, provide:

1. **Trip conditions**

   * Sliding window size (N calls or T seconds), failure-rate threshold (%), consecutive-error threshold, and latency p95/p99 threshold.
   * Error classes included/excluded (e.g., count 5xx/timeout; exclude 4xx user errors).
2. **States & transitions**

   * Closed → Open → Half-Open with probe counts, decay, cool-down.
3. **Retry/backoff**

   * Max attempts, exponential backoff + full jitter, per-call **retry budget** and **global budget** to protect upstream.
4. **Fallbacks & graceful degradation**

   * LLM: cached answer/template, smaller/cheaper model, summarization mode, or “draft + user confirm”.
   * PPT build: minimal theme, skip images, reduce slide count.
5. **Idempotency & dedupe**

   * Idempotency keys, request hashing for safe retries.
6. **Observability**

   * Metrics: `circuit_state{usecase}`, `trips_total`, `rejections_total`, `probe_success_total`, `fallback_invocations`, `retry_attempts`.
   * Logs: structured, no PII; Traces: span links across retries/fallbacks.
7. **Configuration & rollout**

   * Config-as-code defaults + dynamic overrides (env/flags/config service).
   * Canary, feature flag, blast radius controls.
8. **Security & privacy**

   * Don’t log prompts/outputs; guardrails for prompt injection; secrets via env/secret manager.
9. **Testing plan**

   * Unit (fakes), fault-injection (timeouts/5xx), load + chaos (latency spikes), game-day playbook.
10. **Code artifacts**

* **Python pseudo-code** for the breaker wrapper, plus a minimal **reference implementation** using either `pybreaker` or `tenacity` for retries and a custom state store (in-memory → Redis for multi-worker).
* Integration points (decorators/middleware) and how to apply to the two hot paths without invasive changes.

D. **STAR Write-ups (one per use case)**
Use **S**ituation, **T**ask, **A**ctions (design choices, trade-offs, numbers), **R**esults (SLO improvement, error-budget burn reduction, cost/latency deltas). Keep to 8–12 bullets each.

**Style & Constraints**

* Be precise; no generic advice. Provide concrete thresholds, windows, and example numbers with rationale.
* Show reasoning **via short justifications**—do not output private chain-of-thought.
* Output sections **A–D** in order, with clear headings, tables where appropriate, and code blocks for pseudo-code.

**Repository Snapshot**

```
[Paste tree or list of scripts here]
```

---

## Add-Ons You Can Toggle (optional, if relevant)

* **Multi-tenant isolation:** per-tenant breaker state to prevent noisy-neighbor effects.
* **Client vs. server breaker:** justify placement (SDK/client, gateway, or server-side middleware).
* **Quota-aware breaker:** separate trip logic for 429s vs 5xx.
* **Adaptive thresholds:** increase/open faster during incident (burn-rate alerts).
* **Cost guardrails:** automatic downgrade to cheaper LLM/model when breaker is half-open.

---

## Example Answer Shape (concise sample for one LLM call path)

**Trip conditions**

* Window: last **200** calls or **60s**, whichever first.
* Trip if **failure_rate ≥ 15%** (timeouts, 5xx, DNS, TLS) **OR** if **p95 latency > 4s** for 3 consecutive windows **OR** **≥5** consecutive timeouts.

**States**

* **Open:** 30s cool-down (flag + timestamp).
* **Half-Open:** allow 5 probes at 2s spacing; close on ≥4/5 success with p95 < 2.5s; else reopen 60s.

**Retry & Backoff**

* Max 2 retries per call, **exponential backoff with full jitter**: 300–700ms, then 600–1400ms. Global retry budget 10% of RPS.

**Fallbacks**

* Serve cached slide outline (24h TTL) or **fallback to smaller model**; if both fail, render minimal PPT with headers and placeholders.

**Observability**

* Metrics: `circuit_state{usecase="llm_generate"}`, `trips_total`, `fallback_invocations`, `probe_success_total`.
* Trace: one span per attempt, link retries.

**Config**

* Env defaults, dynamic overrides via `CONFIG_BREAKERS.yaml` or Redis. Feature flag: `cb_llm_generate_v1`.

**Python wrapper (pseudo-code)**

```python
def with_breaker(usecase):
    def deco(fn):
        def wrapped(*args, **kwargs):
            state = breaker_store.get_state(usecase)
            if state.is_open() and not state.allow_probe():
                metrics.inc("rejections_total", usecase)
                return fallback(usecase, *args, **kwargs)

            try:
                resp = retry_with_backoff(fn, *args, **kwargs)
                breaker_store.record_success(usecase, latency_ms(resp))
                return resp
            except TransientUpstreamError as e:
                breaker_store.record_failure(usecase, e)
                if breaker_store.should_trip(usecase):
                    breaker_store.open(usecase)
                    alert("circuit_tripped", usecase)
                return fallback(usecase, *args, **kwargs)
        return wrapped
    return deco
```

---

## Why this prompt works

* **Concrete outputs** (A–D) prevent hand-wavy answers.
* **Numbers + thresholds** force engineering rigor.
* **STAR** ensures stakeholder-friendly reporting.
* **Privacy note** avoids chain-of-thought leakage while still demanding justifications.
* **Code + rollout** makes it immediately actionable.

If you want, I can adapt this to your **ppt-ai-generator** repo specifically (LLM slide generation + PPT build as the two hot paths) and pre-fill suggested thresholds and metrics.
