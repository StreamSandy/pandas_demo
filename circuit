>>>>>>>>>>>>>>>>>>>>>>
10:30 PM 14-cot2025

Got it — you want (1) a sharper zero-shot instruction prompt that reliably elicits the full solution from an agent, and (2) a concrete, minimal FastAPI example showing a circuit breaker plus “chaos” testing with step-by-step commands. Here you go.

# 1) Upgraded zero-shot instruction prompt (copy-paste ready)

**Title:** Build a FastAPI endpoint with a circuit breaker + chaos test plan

**Objective:**
Create a small FastAPI app with one endpoint that calls a simulated downstream service wrapped in a circuit breaker. Provide a *fully reproducible* chaos test plan showing behavior with and without failures.

**Constraints & rules**

* Python 3.10+. Use only: `fastapi`, `uvicorn`, `pybreaker`, `httpx` (no heavy frameworks).
* Keep code in a single file `app.py`. Use `requirements.txt`.
* Circuit breaker settings must be easy to tweak (threshold, timeout).
* Logging must clearly show breaker state transitions (CLOSED → OPEN → HALF_OPEN → CLOSED).
* “Chaos mode” must be toggled by an env var `CHAOS_MODE=on` and a failure rate env var `CHAOS_FAIL_PCT` (0–100). No external DBs.
* Provide both: simple cURL loop **and** a Chaos Toolkit experiment file to run the same scenario.

**Deliverables**

1. `app.py` (FastAPI + circuit breaker + simple “downstream” function).
2. `requirements.txt`.
3. `chaos-experiment.json` (Chaos Toolkit experiment that flips chaos on, sends traffic, and asserts breaker opens).
4. A **Step-by-step** test script (shell commands) for:

   * baseline (no chaos)
   * chaos on (trigger OPEN state)
   * recovery after `reset_timeout` (HALF_OPEN → CLOSED)

**Functional details**

* Endpoint: `GET /work` calls a helper `call_downstream()` wrapped by circuit breaker.
* When `CHAOS_MODE=on`, `call_downstream()` randomly throws an exception at `CHAOS_FAIL_PCT` probability.
* Circuit breaker config: `fail_max=3`, `reset_timeout=10` (changeable via env: `CB_FAIL_MAX`, `CB_RESET_TIMEOUT`).
* Return JSON indicating: breaker state, request id, and whether downstream succeeded.

**Acceptance criteria**

* With `CHAOS_MODE=off`: >95% success over 20 requests, breaker never opens.
* With `CHAOS_MODE=on` and `CHAOS_FAIL_PCT>=80`: breaker transitions to OPEN after 3 consecutive failures and starts short-circuiting within the same run.
* After `reset_timeout`, one request tries HALF_OPEN; on success breaker returns to CLOSED.

**Output format**

* Provide files inline as fenced code blocks with filenames in comments at top.
* Provide the exact shell commands to run everything from a clean directory.
* Keep explanations concise and operational (no fluff).

---

# 2) Working example (FastAPI + pybreaker + chaos)

> Note: In Python there isn’t a “chaos-monkey” like Spring’s. We typically use **Chaos Toolkit** for experiments and run “fault injection” inside our code via flags/env vars. Below is a tiny, reproducible pattern.

```python
# app.py
import os, random, time, uuid, logging
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pybreaker import CircuitBreaker, CircuitBreakerListener, STATE_CLOSED, STATE_OPEN, STATE_HALF_OPEN

# ---- Config via env ----
FAIL_MAX = int(os.getenv("CB_FAIL_MAX", "3"))             # consecutive failures before OPEN
RESET_TIMEOUT = int(os.getenv("CB_RESET_TIMEOUT", "10"))  # seconds before HALF_OPEN probe
CHAOS_MODE = os.getenv("CHAOS_MODE", "off").lower() == "on"
CHAOS_FAIL_PCT = int(os.getenv("CHAOS_FAIL_PCT", "0"))    # 0..100
CHAOS_DELAY_MS = int(os.getenv("CHAOS_DELAY_MS", "0"))    # optional latency

# ---- Logging ----
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("app")

class LogListener(CircuitBreakerListener):
    def state_change(self, cb, old_state, new_state):
        log.warning(f"[CB] state change: {old_state.name} -> {new_state.name}")
    def failure(self, cb, exc):
        log.error(f"[CB] failure recorded: {type(exc).__name__}: {exc}")

breaker = CircuitBreaker(fail_max=FAIL_MAX, reset_timeout=RESET_TIMEOUT, listeners=[LogListener()])

app = FastAPI(title="CB + Chaos demo")

def call_downstream() -> dict:
    # optional latency
    if CHAOS_DELAY_MS > 0:
        time.sleep(CHAOS_DELAY_MS / 1000.0)
    # random failure under chaos
    if CHAOS_MODE and random.randint(1,100) <= CHAOS_FAIL_PCT:
        raise RuntimeError("Injected downstream failure (chaos)")
    # pretend success
    return {"downstream": "ok"}

@breaker
def guarded_call():
    return call_downstream()

@app.get("/work")
def work():
    rid = str(uuid.uuid4())[:8]
    try:
        result = guarded_call()
        state = breaker.current_state
        return JSONResponse({"request_id": rid, "ok": True, "result": result, "cb_state": state.name})
    except Exception as e:
        state = breaker.current_state
        # When OPEN, pybreaker raises CircuitBreakerError immediately (short-circuit)
        log.exception(f"Request {rid} failed in state {state.name}")
        raise HTTPException(status_code=503 if state == STATE_OPEN else 500,
                            detail={"request_id": rid, "ok": False, "error": str(e), "cb_state": state.name})

@app.get("/health")
def health():
    return {"status": "up", "cb_state": breaker.current_state.name,
            "chaos": {"enabled": CHAOS_MODE, "fail_pct": CHAOS_FAIL_PCT, "delay_ms": CHAOS_DELAY_MS},
            "config": {"fail_max": FAIL_MAX, "reset_timeout": RESET_TIMEOUT}}
```

```text
# requirements.txt
fastapi
uvicorn
pybreaker
httpx
```

### Run it (baseline first)

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app:app --reload --port 8085
```

New terminal (baseline, no chaos):

```bash
# Expect all success, breaker stays CLOSED
for i in {1..10}; do curl -s http://localhost:8085/work | jq '.cb_state'; done
# Check health/config
curl -s http://localhost:8085/health | jq
```

### Turn on “chaos mode” and trigger OPEN

Stop the server and restart with chaos env:

```bash
export CHAOS_MODE=on
export CHAOS_FAIL_PCT=90        # very failure-prone
export CB_FAIL_MAX=3            # after 3 consecutive failures => OPEN
export CB_RESET_TIMEOUT=10      # 10s before HALF_OPEN probe
uvicorn app:app --reload --port 8085
```

Drive traffic to provoke failures (the first few will be 500; after 3 consecutive failures, calls short-circuit to 503 with `cb_state":"OPEN"`):

```bash
for i in {1..15}; do curl -s http://localhost:8085/work | jq '{ok:.ok, state:.cb_state}'; sleep 0.3; done
# Tail logs to SEE transitions: CLOSED -> OPEN
# (Look for: "[CB] state change: CLOSED -> OPEN")
```

### Observe recovery to HALF_OPEN → CLOSED

Wait 10s (reset timeout), then send a few more requests:

```bash
sleep 10
for i in {1..10}; do curl -s http://localhost:8085/work | jq '{ok:.ok, state:.cb_state}'; sleep 0.3; done
# If a HALF_OPEN probe succeeds, breaker returns to CLOSED.
# With CHAOS_FAIL_PCT=90, you may still see it flip OPEN again; reduce to, say, 10 to demonstrate stable recovery:
export CHAOS_FAIL_PCT=10
```

> Note on “sliding window”: **pybreaker** trips on *consecutive* failures (`fail_max`) rather than a rolling ratio window. If you specifically need a *windowed error rate*, you can layer a small counter over N recent requests and raise a synthetic exception to the breaker when the rolling failure rate exceeds a threshold. (Happy to add that if you want it.)

---

## Optional: Chaos Toolkit experiment

This experiment: (1) starts with steady state (breaker CLOSED), (2) turns chaos on (high fail%), (3) sends bursts to trip the breaker, (4) asserts state OPEN appears in responses/logs, then (5) lowers fail% and validates recovery.

```json
{
  "version": "1.0.0",
  "title": "FastAPI Circuit Breaker Chaos Experiment",
  "description": "Trip breaker under high failure rate, then recover.",
  "steady-state-hypothesis": {
    "title": "Breaker starts CLOSED",
    "probes": [{
      "type": "probe",
      "name": "health-closed",
      "tolerance": { "type": "jsonpath", "path": "$.cb_state", "expect": "CLOSED" },
      "provider": {
        "type": "http",
        "timeout": 3,
        "url": "http://localhost:8085/health",
        "method": "GET"
      }
    }]
  },
  "method": [
    {
      "type": "action",
      "name": "enable-chaos-high",
      "provider": {
        "type": "python",
        "module": "os",
        "func": "putenv",
        "arguments": { "key": "CHAOS_FAIL_PCT", "value": "90" }
      }
    },
    {
      "type": "action",
      "name": "send-traffic-to-trip",
      "provider": {
        "type": "process",
        "path": "bash",
        "arguments": ["-lc", "for i in {1..20}; do curl -s http://localhost:8085/work >/dev/null; done"]
      }
    },
    {
      "type": "probe",
      "name": "breaker-open-check",
      "tolerance": { "type": "regex", "pattern": "OPEN" },
      "provider": {
        "type": "process",
        "path": "bash",
        "arguments": ["-lc", "curl -s http://localhost:8085/work"]
      }
    },
    {
      "type": "action",
      "name": "lower-chaos",
      "provider": {
        "type": "python",
        "module": "os",
        "func": "putenv",
        "arguments": { "key": "CHAOS_FAIL_PCT", "value": "10" }
      }
    },
    {
      "type": "action",
      "name": "wait-reset-timeout",
      "provider": {
        "type": "process",
        "path": "bash",
        "arguments": ["-lc", "sleep ${CB_RESET_TIMEOUT:-10}"]
      }
    },
    {
      "type": "probe",
      "name": "breaker-recovers",
      "tolerance": { "type": "regex", "pattern": "CLOSED" },
      "provider": {
        "type": "process",
        "path": "bash",
        "arguments": ["-lc", "for i in {1..10}; do curl -s http://localhost:8085/work; done | tail -n1"]
      }
    }
  ],
  "rollbacks": []
}
```

Run:

```bash
pip install chaostoolkit
chaos run chaos-experiment.json
```

---

## If you specifically need “IML” and “profile” like Spring

Those are IntelliJ/Spring concepts. In Python, prefer:

* `pyproject.toml` or `requirements.txt` for dependencies (above).
* Use env var “profiles” (e.g., `PROFILE=chaos`) to toggle behavior inside `app.py`. Example:

  ```bash
  export PROFILE=chaos
  export CHAOS_MODE=on
  ```
* If you’re using PyCharm/IntelliJ, you can set these env vars in a **Run/Debug Configuration** instead of `.iml`.

---

## Quick recap test script

```bash
# 1) Clean run
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
unset CHAOS_MODE CHAOS_FAIL_PCT CB_FAIL_MAX CB_RESET_TIMEOUT
uvicorn app:app --port 8085 &

# baseline
for i in {1..10}; do curl -s http://localhost:8085/work | jq -r '.cb_state'; done

# 2) Enable chaos and trip breaker
export CHAOS_MODE=on
export CHAOS_FAIL_PCT=90
export CB_FAIL_MAX=3
export CB_RESET_TIMEOUT=10
pkill -f "uvicorn app:app" || true
uvicorn app:app --port 8085 &

for i in {1..15}; do curl -s http://localhost:8085/work | jq -r '.cb_state'; sleep 0.2; done

# 3) Recover
sleep 10
export CHAOS_FAIL_PCT=10
for i in {1..10}; do curl -s http://localhost:8085/work | jq -r '.cb_state'; sleep 0.2; done
```

---

If you want me to swap **consecutive-failure** tripping for a **rolling-window error-rate** (true “sliding window”), say the window size and threshold (e.g., 20 requests / 50% errors) and I’ll extend the code to maintain a deque and raise synthetic failures to the breaker when the rolling error rate breaches your target.



>>>>>>>>>>>>>>>>>>>>>>
10:40 PM 13oct:

Excellent — this is already a strong prompt foundation. To help you refine it into a **truly effective zero-shot instruction task** (so that any capable agent can solve it end-to-end, with clarity, reasoning depth, and structure), let’s analyze its **characteristics**, **gaps**, and then present a **final improved version**.

---

## 🧠 1. Analysis of Your Current Prompt

### ✅ Strengths

* Specifies a **real enterprise context** (custom library `ent_pycore`).
* Clearly ties to an **existing codebase** and file (`circuit_breaker.py`).
* Mentions **MongoDB reuse** (`src/utils/envloader.py`) — great for reuse & consistency.
* Includes **principles** from a principal engineer (callback chaining, exponential backoff).
* Encourages clean code principles (small functions, exception handling).
* Requests **before/after report**, test cases, and deployment — a very strong completion checklist.

### ⚠️ Opportunities for Improvement

| Category                | Observation                                                                                                   | Recommendation                                                                                               |
| ----------------------- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Goal Clarity**        | Multiple tasks mixed (analysis, design, implementation, deployment).                                          | Structure prompt into clear *phases* with outputs.                                                           |
| **Reasoning Depth**     | Agent not explicitly told to *reason before coding*.                                                          | Add explicit “think → plan → code → test → report” stages.                                                   |
| **Ambiguity**           | Phrases like “help me to achieve this technically” are open-ended.                                            | Replace with measurable deliverables (e.g., “Provide function/class design, Mongo schema, deployment YAML”). |
| **Evaluation Criteria** | No explicit success metrics (how do we know it’s “good”?)                                                     | Add acceptance criteria for correctness, reliability, and cleanliness.                                       |
| **Context Reuse**       | Mentions “callback chain” and “manual open” but not how Mongo toggles or exponential backoff should interact. | Add short examples or pseudocode expectations.                                                               |
| **Tone and Structure**  | All in one paragraph; dense.                                                                                  | Break into clear sections with headers for a zero-shot agent to parse logically.                             |

---

## ⚙️ 2. Improved Zero-Shot Instruction Prompt

Below is a **rewritten version** that keeps all your context intact but adds structure, reasoning, and clarity — formatted for an LLM or autonomous agent to execute in **zero-shot** mode (i.e., without prior tuning).

---

### 🔹 **Zero-Shot Instruction Prompt — Circuit Breaker Design Task**

**ROLE & CONTEXT**

You are a **Senior Software Engineer** working on our enterprise reliability framework.
We have an internal Python distribution called **`ent_pycore`**, which includes an in-house circuit breaker library.
Your goal is to design and implement an improved **Circuit Breaker** logic using that library as a base.

Reference code location:

```
venv\Lib\Site-Packages\pycore\stability\circuit_breaker.py
```

We have around **30+ existing scripts** across:

```
src/
tests/
```

---

### 🧩 **OBJECTIVE**

Design, implement, and deliver a **production-grade Circuit Breaker system** for the top-priority use case (defined below) using only the logic patterns available in our enterprise Python distribution (`ent_pycore`).

Your solution must:

1. Reuse the `ent_pycore.stability.circuit_breaker` foundation — no rewriting from scratch.
2. Integrate MongoDB as the **configuration backend** for all circuit breaker parameters.
3. Support **manual circuit control** (open/close) via MongoDB flag updates.
4. Implement **exponential backoff** during half-open retries.
5. Follow **clean code** and **function abstraction** principles.
6. Provide **tests, deployment script, and review report** for the PE (Principal Engineer).

---

### ⚙️ **TECHNICAL CONTEXT**

* MongoDB credentials and connection helpers are already available in:

  ```
  src/utils/envloader.py
  ```

  Reuse those helper functions for all Mongo interactions.

* Create a new MongoDB **collection and schema** to store circuit breaker configurations:

  * `collection`: `circuit_breakers`
  * Example fields:

    ```json
    {
      "name": "llm_api_cb",
      "enabled": true,
      "failure_rate_pct": 50,
      "rolling_window_sec": 60,
      "half_open_probe_count": 3,
      "backoff": { "mode": "exponential", "base_ms": 200, "max_ms": 5000, "jitter": true },
      "manual_override": { "state": "closed", "reason": null },
      "updated_at": "2025-10-13T10:00:00Z"
    }
    ```

---

### 🧠 **TASK FLOW**

1. **Analyze**

   * Review all functions and classes in `ent_pycore.stability.circuit_breaker.py`.
   * Summarize their role (e.g., state tracking, thresholding, exception management).
   * Identify extension points for Mongo integration and backoff logic.

2. **Design**

   * Propose a function or class wrapper `with_circuit_breaker()` that:

     * Wraps any callable.
     * Loads parameters from Mongo.
     * Supports callback chaining (`chain_invoke`).
     * Uses exponential backoff for half-open retries.
   * Define the MongoDB schema (collection + documents) used for storing breaker config and state.
   * Show how manual open/close is controlled via MongoDB updates.

3. **Implement**

   * Modify or extend code within `src/` as needed (e.g., `src/stability/circuit_wrapper.py`).
   * Show **before and after code snippets** for all files changed.
   * Implement reusable helper for backoff delay computation:

     ```python
     def compute_backoff_ms(base, attempt, max_ms, jitter=True):
         delay = min(base * (2 ** (attempt - 1)), max_ms)
         return random.uniform(0, delay) if jitter else delay
     ```

4. **Test**

   * Create or extend unit tests under `tests/`:

     * State transitions: CLOSED → OPEN → HALF_OPEN → CLOSED
     * Manual Mongo override (open/close)
     * Exponential backoff validation
     * Callback chaining order
     * Parameter reload (hot config)

5. **Deploy**

   * Provide MongoDB deployment script or JSON seed (for local and prod setup).
   * Example:

     ```bash
     mongosh "$MONGO_URI" --eval '
     db.circuit_breakers.updateOne(
       { name: "llm_api_cb" },
       { $set: { enabled: true, failure_rate_pct: 50, ... } },
       { upsert: true }
     )'
     ```

6. **Report**

   * Produce a final report with:

     * Summary of modifications
     * Before/After diffs
     * Code snippets
     * Mongo deployment details
     * PE review summary

---

### 💡 **ENGINEERING GUIDELINES**

* Prefer **exceptions** to returning error codes.
* Functions must be **small** and **single-purpose**.
* Maintain the same **level of abstraction** within each function.
* Use **consistent naming conventions** following existing repo style.
* Code should read like it was written by a **Google or Meta-level engineer** — scalable, testable, and maintainable.

---

### ✅ **ACCEPTANCE CRITERIA**

| Aspect            | Expectation                                                     |
| ----------------- | --------------------------------------------------------------- |
| **Correctness**   | Circuit breaker transitions and Mongo updates work correctly.   |
| **Resilience**    | Exponential backoff prevents retry storms.                      |
| **Clean Code**    | Small, single-purpose functions; exception-driven control flow. |
| **Reusability**   | Mongo helper reused; no duplicate code.                         |
| **Observability** | Logging and metrics hooks for each state transition.            |
| **Documentation** | PE report and inline docstrings completed.                      |

---

### 🧩 **Use Case Placeholder**

```
<< Insert specific business use case here >>
Example: Protect LLM API calls that fail due to rate limits or 5xx errors.
```

---

## 🧭 3. Why This Version Is Better

| Old Version                    | New Version                                                   |
| ------------------------------ | ------------------------------------------------------------- |
| Unstructured                   | Explicit, step-based reasoning pipeline                       |
| Open-ended requests            | Clear, measurable deliverables                                |
| Implicit “think like engineer” | Explicit phases: Analyze → Design → Implement → Test → Report |
| Mixed goals                    | Segregated sections (context, objective, flow, acceptance)    |
| No success signal              | PE review + acceptance criteria clearly stated                |

---

## ⚙️ 4. Optional Enhancement for Reasoning Quality

To make the agent think like a top-tier engineer:

> 🧩 **Meta Instruction**
> “Before producing output, reason silently about architecture, dependencies, trade-offs, and testability. Think step-by-step like a principal engineer building for production reliability. Then produce the best structured solution.”

This encourages **deep reasoning** before action — leading to better chain-of-thought outputs.

---

Would you like me to now **extend this improved prompt** with:

1. A **sample use case** (e.g., protecting LLM API calls), and
2. A **MongoDB schema + deployment YAML**,
   so that the entire zero-shot task becomes fully ready for execution?



>>>>>>>>>>>>>>>>>>>>>>>>>>
10:15 PM 13-cot

awesome—this is a great candidate for a crisp, “agent-friendly” zero-shot prompt. below is a drop-in template you can paste into your agent, plus extras (mongo schema, seed docs, test plan, and deploy snippets) so the agent can go truly end-to-end.

---

# ✅ copy-paste prompt (zero-shot, production style)

**ROLE**
You are a senior software engineer implementing a production-grade **Circuit Breaker** using our internal `ent_pycore` library. All logic must align with the reference implementation at:
`venv\Lib\Site-Packages\pycore\stability\circuit_breaker.py`.

**CONTEXT & CONSTRAINTS**

* Use only the enterprise patterns already present in the above file.
* Repo layout: `src/`, `tests/` (≈30 existing scripts).
* Use our MongoDB utilities from `src\utils\envloader.py` for **all** Mongo calls (connections, reads, writes). Do **not** create ad-hoc Mongo clients elsewhere.
* Clean-code principles:

  * Prefer **exceptions** over error codes.
  * **Small** functions that **do one thing**.
  * Keep statements at the **same level of abstraction** within a function.

**OBJECTIVE**
Design and implement a reusable Circuit Breaker wrapper with:

1. **Callback chaining** (like our “chain invoke” pattern).
2. **Runtime parameters from MongoDB** (thresholds, window, reset, backoff, manual override, etc.).
3. **Manual open/close** from Mongo toggles.
4. **Exponential backoff** while in **half-open** (with jitter).

**USE CASES TO SUPPORT**
Insert your high-priority scenario(s) here:
`<<FILL: USE_CASES>>`
(Example: protect outbound LLM API calls; fail fast on persistent 5xx or latency > p95; allow manual open during downstream incident.)

---

## DELIVERABLES (produce in this exact order)

### 1) Reasoning Summary (2–4 short paragraphs)

* Summarize `ent_pycore` circuit-breaker functions/classes discovered in `pycore\stability\circuit_breaker.py`.
* State the design you’ll implement and why (trade-offs: latency vs reliability, maintainability, failure modes).

### 2) Proposed API (thin wrapper)

* Provide a minimal public interface that **does one thing**: run a callable under circuit-breaker protection with chain-invoke support.
* Exact signature (adapt if needed but keep spirit):

```python
def with_circuit_breaker(
    name: str,
    operation: Callable[[], T],
    on_success: Optional[Callable[[T], Any]] = None,
    on_failure: Optional[Callable[[Exception], Any]] = None,
    chain: Optional[list[Callable[[T], T]]] = None,
    context: Optional[dict] = None,
) -> T:
    ...
```

* All tunables (thresholds, cooldown, half-open probe count, backoff base/max/jitter, manual_override) must be **read at call time** from Mongo via `envloader.py`.

### 3) Implementation (new/changed files)

List every file **created or modified** and include code blocks. Keep functions small and cohesive.

**a) `src/stability/circuit_wrapper.py` (NEW)**

* Responsibilities: parameter fetch, state read/write, breaker transitions, backoff schedule, chain execution, metrics hooks.
* Use exceptions; no error codes.
* Ensure uniform abstraction per function.

**b) `src/utils/envloader.py` (EXISTING)**

* Show **only** any additions you make (helper that reads/writes a single CB doc by `name`). If unchanged, say “no changes”.

**c) `src/stability/mongo_models.py` (NEW)**

* Data access helpers (read/write “circuit_breakers” collection), typed dicts / pydantic model (if used in your codebase).

**d) Brief DIFFS section**

* For each changed file: *Before vs After* snippets (only relevant hunks).

### 4) MongoDB schema & sample docs

* Collection: `circuit_breakers`
* Doc shape (example):

```json
{
  "_id": "cb:llm_primary",
  "name": "llm_primary",
  "enabled": true,
  "manual_override": { "state": "closed", "reason": null, "until": null },
  "thresholds": {
    "failure_rate_pct": 50,
    "rolling_window_sec": 60,
    "min_calls_in_window": 20
  },
  "timeouts": { "op_timeout_ms": 8000, "reset_timeout_ms": 30000 },
  "half_open": { "probe_count": 3 },
  "backoff": { "mode": "exponential", "base_ms": 200, "max_ms": 5000, "jitter": true },
  "metrics_keys": { "success": "cb.llm.success", "failure": "cb.llm.failure", "state": "cb.llm.state" },
  "updated_at": "2025-10-13T10:00:00Z",
  "notes": "Default policy for primary LLM"
}
```

* Provide **seed** commands (see “Deployment” below).

### 5) Tests (`tests/test_circuit_wrapper.py`)

* Cover transitions: closed→open (on threshold), open→half-open (after reset), half-open→closed (on probe success), half-open→open (on probe failure).
* Validate: exponential backoff, Mongo toggles (manual open), chain callbacks order, exception propagation, parameter reload per call.
* Use fixtures to stub Mongo via `envloader.py` layer (not a raw client).
* Make tests deterministic (freeze time & seed RNG; mock jitter).

### 6) PE-ready report (concise)

* One page: problem, design, diffs, test matrix (cases & expected), rollback plan, and ops runbook link.

---

## ACCEPTANCE CRITERIA (hard checks)

* **Correctness**: Transitions match design; half-open uses exponential backoff + jitter; parameters hot-reload from Mongo each call.
* **Operability**: Manual override via Mongo takes effect **without deploy**.
* **Clean Code**: Small functions, clear names, exception-first control flow, consistent abstraction layers.
* **Tests**: ≥90% coverage on `src/stability/circuit_wrapper.py`; all transitions tested.
* **Docs**: Before/After diffs and a one-page PE report supplied.

---

## IMPLEMENTATION HINTS (what to produce)

### Example wrapper skeleton

```python
# src/stability/circuit_wrapper.py
from typing import Callable, TypeVar, Optional, Any
from src.utils.envloader import get_cb_config, get_cb_state, set_cb_state, utc_now

T = TypeVar("T")

class CircuitState(str):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

def with_circuit_breaker(
    name: str,
    operation: Callable[[], T],
    on_success: Optional[Callable[[T], Any]] = None,
    on_failure: Optional[Callable[[Exception], Any]] = None,
    chain: Optional[list[Callable[[T], T]]] = None,
    context: Optional[dict] = None,
) -> T:
    """
    Does one thing: executes `operation` under circuit-breaker protection.
    Raises exceptions; never returns error codes.
    """
    cfg = get_cb_config(name)            # from Mongo
    state = get_cb_state(name)           # from Mongo
    now = utc_now()

    # Manual override (operability > everything)
    mo = cfg.get("manual_override", {})
    if mo and mo.get("state") in {CircuitState.OPEN, CircuitState.CLOSED, CircuitState.HALF_OPEN}:
        state["state"] = mo["state"]

    # Gate by state
    if state["state"] == CircuitState.OPEN:
        if now < state.get("next_try_at", now):
            raise RuntimeError(f"circuit {name} is OPEN")
        # move to half-open probe
        state["state"] = CircuitState.HALF_OPEN
        state["remaining_probes"] = cfg["half_open"]["probe_count"]

    try:
        result = operation()
        # chain invoke
        if chain:
            for fn in chain:
                result = fn(result)

        # success handling
        _record_success(name, cfg, state, now)
        set_cb_state(name, state)
        if on_success:
            on_success(result)
        return result

    except Exception as exc:
        # failure handling
        _record_failure(name, cfg, state, now, exc)

        if state["state"] == CircuitState.CLOSED and _should_open(cfg, state):
            state["state"] = CircuitState.OPEN
            state["next_try_at"] = now + _reset_delta(cfg)

        elif state["state"] == CircuitState.HALF_OPEN:
            # exponential backoff for next probe window
            delay_ms = _next_backoff_ms(cfg, state)
            state["state"] = CircuitState.OPEN
            state["next_try_at"] = now + _ms(delay_ms)

        set_cb_state(name, state)
        if on_failure:
            on_failure(exc)
        raise  # prefer exceptions
```

*(Keep helpers like `_record_success/_record_failure/_should_open/_next_backoff_ms/_reset_delta/_ms` tiny and single-purpose.)*

---

## MONGODB: minimal schema helpers

```python
# src/stability/mongo_models.py
from typing import Optional, Dict, Any
from src.utils.envloader import mongo_get_one, mongo_upsert

CB_COLL = "circuit_breakers"
STATE_COLL = "circuit_states"

def load_cb_config(name: str) -> Dict[str, Any]:
    doc = mongo_get_one(CB_COLL, {"name": name, "enabled": True})
    if not doc:
        raise RuntimeError(f"CB config not found or disabled: {name}")
    return doc

def load_cb_state(name: str) -> Dict[str, Any]:
    return mongo_get_one(STATE_COLL, {"name": name}) or {"name": name, "state": "closed"}

def save_cb_state(name: str, state: Dict[str, Any]) -> None:
    mongo_upsert(STATE_COLL, {"name": name}, state)
```

If your `envloader.py` already wraps these, re-export simple functions:

```python
# src/utils/envloader.py (additions only if needed)
def get_cb_config(name: str): return load_cb_config(name)
def get_cb_state(name: str):  return load_cb_state(name)
def set_cb_state(name: str, state: dict): save_cb_state(name, state)
```

---

## TEST PLAN (table you can paste into the PE report)

| Case                    | Initial State | Stimulus                        | Expectation                                    |
| ----------------------- | ------------- | ------------------------------- | ---------------------------------------------- |
| 1 Closed→Closed         | closed        | success                         | counters reset, remains closed                 |
| 2 Closed threshold trip | closed        | N failures in window            | state=open, next_try_at set                    |
| 3 Open cool-down        | open          | call before next_try_at         | raises “OPEN”                                  |
| 4 Open→Half-open probe  | open          | after reset_timeout             | transitions to half_open, remaining_probes=cfg |
| 5 Half-open success     | half_open     | probe success × probe_count     | state=closed                                   |
| 6 Half-open failure     | half_open     | probe fail                      | backoff computed (exp + jitter), state=open    |
| 7 Manual override       | any           | manual_override=“open” in Mongo | denies calls until cleared                     |
| 8 Parameter hot-reload  | closed        | change backoff in Mongo         | next call uses new backoff                     |
| 9 Chain callbacks       | closed        | chain=[f1,f2]                   | f1 then f2, result threaded                    |
| 10 Exception path       | any           | operation raises                | exception re-raised; on_failure invoked        |

---

## END-TO-DEPLOYMENT (Mongo)

### Seed a config (example)

```bash
mongosh "$MONGO_URI" --eval '
db.circuit_breakers.updateOne(
  { name: "llm_primary" },
  { $set: {
      name: "llm_primary",
      enabled: true,
      manual_override: { state: "closed", reason: null, until: null },
      thresholds: { failure_rate_pct: 50, rolling_window_sec: 60, min_calls_in_window: 20 },
      timeouts: { op_timeout_ms: 8000, reset_timeout_ms: 30000 },
      half_open: { probe_count: 3 },
      backoff: { mode: "exponential", base_ms: 200, max_ms: 5000, jitter: true },
      metrics_keys: { success: "cb.llm.success", failure: "cb.llm.failure", state: "cb.llm.state" },
      updated_at: new Date()
  }},
  { upsert: true }
);
'
```

### Docker-compose (local)

```yaml
version: "3.9"
services:
  mongo:
    image: mongo:7
    ports: ["27017:27017"]
    volumes: ["./.data/mongo:/data/db"]
  app:
    build: .
    environment:
      MONGO_URI: "mongodb://mongo:27017/app"
    depends_on: [mongo]
```

### K8s (minimal)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: app }
spec:
  replicas: 2
  selector: { matchLabels: { app: app } }
  template:
    metadata: { labels: { app: app } }
    spec:
      containers:
      - name: app
        image: your/image:tag
        env:
        - name: MONGO_URI
          valueFrom:
            secretKeyRef: { name: mongo-secret, key: uri }
```

---

## WHY THIS PROMPT WORKS

* **Sequenced thinking**: understand → design → implement → test → report.
* **Clear success bar**: acceptance criteria + test matrix.
* **Single-responsibility**: tiny helpers; exception-first flow.
* **Operability**: manual override via Mongo; hot-reloadable params; backoff with jitter.
* **PE-ready**: diffs + one-page report checklist.

---

### want me to tailor this to your exact `envloader.py` helpers (names & signatures) and emit the full `tests/test_circuit_wrapper.py` file too?



>>>>>>>>>>>>>>>>>>>>>>>>>>

11: 00 AM / 120ct

Excellent — you’re already very close to a strong **zero-shot instruction** prompt.
Let’s first **analyze** what you have, then I’ll propose an **improved version** and finally explain **why it’s better** for agent reasoning.

---

## 🧠 1. Analysis of Current Prompt

### **Strengths**

* Clear technical scope — “design circuit breaker using ent_pycore”
* References concrete assets: `circuit_breaker.py`, `src/`, `tests/`
* Specifies expected outputs: modified scripts, before/after diff, explanation, test cases, PE-ready report
* Invokes reasoning depth — “think like a high-performing software engineer / Google engineer”

### **Weaknesses**

1. **Ambiguous structure**

   * The agent receives multiple overlapping goals (analyze library, design logic, implement MongoDB integration, add test cases) without hierarchy or sequencing.
2. **Too much implicit context**

   * The agent doesn’t know the target runtime, repo structure, or constraints (e.g. CI/CD, env vars, schema of MongoDB parameters).
3. **No evaluation signal**

   * The agent isn’t told how success or completeness will be judged (e.g., “design must support manual override”, “half-open state must backoff exponentially”, etc.)
4. **Instruction verbosity**

   * Long sentences combine analysis, design, and reporting requests in one flow, which can confuse the reasoning chain.
5. **No reasoning checkpoints**

   * There are no explicit “think first, then do” stages (reflection, design sketch, code, explanation).

---

## ⚙️ 2. Recommended Prompt Structure

Below is an improved version — rewritten for clarity, reasoning depth, and zero-shot agent comprehension.

---

### 🧩 **Improved Zero-Shot Instruction Prompt**

**ROLE & CONTEXT**
You are a senior software engineer designing a fault-tolerant **Circuit Breaker** system using our internal library `ent_pycore`.
All logic must align with existing enterprise code standards and be derived only from the reference implementation in `circuit_breaker.py`.

**OBJECTIVE**
Analyze, design, and implement a production-ready Circuit Breaker solution for the high-priority LLM API use case.
The implementation must follow clean-code and design principles:

* Prefer exceptions to error codes
* Each function does one thing
* Small, cohesive functions
* Maintain a consistent abstraction level

---

### **TASK FLOW**

1. **Understand the environment**

   * Review all modules in `ent_pycore`, especially `circuit_breaker.py`.
   * Summarize available classes/functions related to circuit breaking.

2. **Design proposal**

   * Propose how to wrap the circuit breaker into a reusable **function or class** that supports:

     * Configurable parameters (fetched dynamically from MongoDB)
     * Manual circuit open capability
     * Exponential backoff in half-open state
     * Callback chaining for dependent invocations (like the “chain invoke call” pattern)

3. **Implementation**

   * Provide complete Python code implementing the above.
   * Show which scripts were **modified** or **created** (`src/`, `tests/`).
   * Use realistic variable names and exception handling patterns.

4. **Testing**

   * Write unit tests for each circuit breaker transition (closed → open → half-open → closed).
   * Mock MongoDB parameter retrieval for reproducibility.

5. **Reporting**

   * Provide a concise report with:

     * Before vs After code snippets
     * Explanation of design decisions
     * MongoDB schema and configuration details
     * Deployment checklist for the MongoDB-based circuit breaker

---

### **ADDITIONAL CONSIDERATIONS**

* Code must be modular, production-grade, and ready for deployment.
* Use exponential backoff strategy with clear retry logic.
* Ensure all configuration parameters are dynamically loaded from MongoDB (e.g., thresholds, timeouts, reset interval).
* Demonstrate how manual override can be triggered through MongoDB flag change.
* Include best-practice reasoning like a Google or Meta engineer.

**OUTPUT FORMAT**

* 🧠 Reasoning Summary
* 🏗️ Code Implementation (with function/class)
* 🧪 Test Cases
* 📝 Before/After Comparison
* 📊 MongoDB Schema + Deployment Notes

---

## 💡 3. Why This Works Better

| Issue in Old Prompt      | Improvement in New Prompt                                                       |
| ------------------------ | ------------------------------------------------------------------------------- |
| Multiple nested goals    | Structured into five clear phases                                               |
| Implicit assumptions     | Explicit context and deliverables                                               |
| No reasoning checkpoints | Includes stepwise “think → design → implement → test → report” flow             |
| No evaluation criteria   | Adds measurable success signals (manual override, backoff, MongoDB integration) |
| Overlong sentences       | Modular sections with bullet clarity                                            |
| Missing output structure | Defines exact sections for response                                             |

---

## 🚀 4. Optional Enhancement (for Billion-Dollar Startup Quality)

You can prepend this meta-instruction to further strengthen zero-shot reasoning:

> **“You must first reason silently about architecture and dependencies before producing any output. Think about trade-offs (latency, resilience, maintainability), then generate the most optimal and readable solution.”**

This encourages **chain-of-thought reflection** before generation — helping the agent produce better technical depth and structure.

---

Would you like me to extend this improved prompt to include a **sample MongoDB schema + deployment YAML** as part of the “end-to-deployment code”? That would make it completely end-to-end enterprise-ready.


>>>>>>>>>>>>>>>>>>>>>>>>

7:45 AM /12-oct:
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Awesome—this is a strong start. Below is a tightened, production-grade zero-shot instruction prompt you can paste into your agent, plus a compact acceptance checklist, output schemas, and a minimal example wrapper signature/pseudocode so the agent has zero ambiguity.

---

# Improved zero-shot instruction prompt (v1.0)

**Role & Context**
You are a Senior Platform Engineer embedded in our LLM platform team. Our org ships Python services that already depend on our internal library `ent_pycore`. The only allowed implementation for circuit breaking is the code and public APIs already present in `ent_pycore/circuit_breaker.py`. Do **not** introduce new third-party circuit-breaker libs.

**Primary Objective**
Design and integrate a circuit breaker for the highest-priority use case(s) listed under **Use Cases**, using only the enterprise logic available in `ent_pycore/circuit_breaker.py`. Wrap the breaker usage in a reusable function that accepts a callback (chain-invoke style). Fetch breaker parameters from MongoDB, support a **manual open/close override**, and ensure the **half-open** state uses **exponential backoff** with jitter.

**Inputs Provided**

* Source repo structure: `src/`, `tests/` (30+ scripts).
* Internal library file: `ent_pycore/circuit_breaker.py` (the single source of truth).
* List of modules referencing circuit-breaker behavior (you will analyze these).
* **Use Cases** (fill these in):

  ```
  << USE_CASES_GO_HERE >>
  ```

**Hard Requirements (from Principal Engineer)**

1. Provide a wrapper that can be called like chain-invoke, e.g. `invoke_with_cb(name, callback, *args, **kwargs)`.
2. All breaker parameters (`fail_max`, `reset_timeout`, `success_threshold`, backoff settings, etc.) must be loaded from **MongoDB** at call time (with sensible caching) and support **manual override** (`open`/`closed`/`auto`).
3. **Half-open** must use **exponential backoff** (with jitter) before trial calls; on failure in half-open, increase backoff; on success reaching `success_threshold`, close the breaker.

**Design Constraints**

* Use only `ent_pycore/circuit_breaker.py` public APIs. If a needed hook isn’t public, propose a minimal, safe change there (with before/after diff).
* Thread-safe and async-safe (where applicable).
* Minimal overhead; avoid global state; support multi-tenant keys (e.g., by breaker `name`).
* Observability: emit logs and metrics for state transitions, open/close counts, backoff delays, and fallback invocations.
* Error taxonomy: distinguish remote timeouts, 5xx, rate-limits, and client errors; only count **retriable** failures against the breaker.

**MongoDB Schema (authoritative)**
Collection: `cb_configs`
Document key: `{ name: <string> }`
Example document:

```json
{
  "name": "llm.completions",
  "mode": "auto",                // "auto" | "open" | "closed"
  "fail_max": 5,
  "reset_timeout_seconds": 60,
  "success_threshold": 2,
  "backoff": {
    "strategy": "exponential_jitter",
    "base_seconds": 0.5,
    "max_seconds": 30,
    "jitter_fraction": 0.2
  },
  "tags": ["prod", "tenant:global"],
  "updated_by": "pe@company",
  "updated_at": "2025-10-10T12:00:00Z"
}
```

**Backoff Definition**
For attempt `n` in half-open (starting at 1):
`delay = min(max_seconds, base_seconds * 2^(n-1))`
Apply jitter: `delay = delay * (1 ± jitter_fraction)` using random uniform.

**Tasks & Deliverables**

1. **Code Analysis**

   * Enumerate all relevant functions, classes, and extension points in `ent_pycore/circuit_breaker.py`.
   * Map how current repo scripts call external APIs and where the breaker wrapper should be inserted.

2. **Design & Implementation**

   * Implement `invoke_with_cb(name, callback, *args, **kwargs)` that:

     * Loads config from MongoDB (with short-TTL in-process cache).
     * Honors `mode` (`open` → immediately raise; `closed` → normal; `auto` → state machine).
     * Uses the ent_pycore breaker, configuring `fail_max`, `reset_timeout`, `success_threshold`.
     * Implements **half-open** trial gating using exponential backoff with jitter.
     * Allows a **fallback** callable (optional) for graceful degradation (document clearly).
     * Emits logs/metrics on: open → half-open → closed transitions; backoff delays; fallback use.
   * If ent_pycore lacks a hook you need, propose a **minimal** change to `circuit_breaker.py` (with unified diff).

3. **Tests** (`pytest`)

   * Unit tests for: normal, open, half-open flow; manual modes; Mongo config reload; backoff growth; success threshold closing; retriable vs non-retriable error counting.
   * Property tests: backoff monotonicity and cap.
   * Chaos tests: injected 5xx/timeouts to confirm breaker opens and recovers.

4. **Before/After Report for PE**

   * Table of files added/modified.
   * Before/After code snippets (small, focused).
   * Rationale for each change (1-3 lines).
   * Rollout plan, config guardrails, and a quick rollback path.

5. **Usage Examples**

   * Show replacing existing direct calls with `invoke_with_cb(...)` (include one with fallback).

**Output Format (single markdown response)**

* `# Summary` (≤10 lines)
* `# Analysis of ent_pycore/circuit_breaker.py` (API list + where you’ll hook)
* `# Design` (state machine, backoff, config load, manual override)
* `# Code Changes` (per-file summary + unified diffs)
* `# Tests` (pytest code blocks)
* `# Usage Examples` (before/after)
* `# Observability` (metrics/logs to emit; names)
* `# Rollout & Runbook` (flags, config, rollback)
* `# Risks & Mitigations`

**Quality Bar (“like a Google engineer”)**

* Deterministic, idempotent wrapper; zero global hidden state.
* Clear separation: config load, policy calc, breaker call, fallback.
* Tight diffs, self-contained tests, docs inline.
* No new external deps; no hidden side-effects.

**Acceptance Criteria (must pass)**

* Manual override: `open` always short-circuits; `closed` forces normal; `auto` follows state machine.
* Half-open honors exponential backoff with jitter; closes only after `success_threshold` consecutive successes.
* Only retriable failures increment breaker counters.
* Metrics/logs present for all state transitions and backoff events.
* All tests green.

---

## Minimal wrapper signature & pseudocode (for clarity to the agent)

```python
# src/cb_wrapper.py
from typing import Callable, TypeVar, Optional, Any, Dict
from ent_pycore.circuit_breaker import CircuitBreaker, CircuitOpenError
from .mongo import get_cb_config  # you will implement
from .obs import record_metric, log_event  # you will implement

T = TypeVar("T")

def invoke_with_cb(name: str,
                   callback: Callable[..., T],
                   *args: Any,
                   fallback: Optional[Callable[[Exception], T]] = None,
                   **kwargs: Any) -> T:
    """
    Invoke `callback` protected by enterprise CircuitBreaker configured from MongoDB.

    - Loads config by `name` (cached).
    - Honors manual mode: "open"|"closed"|"auto".
    - In half-open, applies exponential backoff with jitter before trial attempts.
    - Emits metrics/logs for transitions, backoff delays, and fallback invocations.
    """
    cfg = get_cb_config(name)  # {mode, fail_max, reset_timeout_seconds, success_threshold, backoff{...}}

    if cfg["mode"] == "open":
        raise CircuitOpenError(f"Breaker '{name}' manually opened")

    # Configure breaker from ent_pycore
    cb = CircuitBreaker(
        name=name,
        fail_max=cfg["fail_max"],
        reset_timeout=cfg["reset_timeout_seconds"],
        success_threshold=cfg["success_threshold"]
    )

    # If cfg["mode"] == "closed", skip state checks but still count metrics
    if cfg["mode"] == "closed":
        try:
            result = callback(*args, **kwargs)
            record_metric("cb.closed.success", 1, {"name": name})
            return result
        except Exception as e:
            record_metric("cb.closed.error", 1, {"name": name, "type": type(e).__name__})
            if fallback: return fallback(e)
            raise

    # Auto mode with half-open exponential backoff when needed
    # (Agent will implement: read cb.state(), compute backoff delay if HALF_OPEN, sleep, then attempt)
    return cb.call(callback, *args, **kwargs)  # Agent augments half-open behavior per design
```

> The agent must flesh out `get_cb_config`, the half-open backoff wait, classification of retriable errors, and observability helpers, **without** diverging from `ent_pycore`’s intended usage.

---

## “Before/After” diff template the agent should use

```diff
# File: src/service/llm_client.py
- resp = http_client.post(LLM_URL, json=payload, timeout=8)
+ from cb_wrapper import invoke_with_cb
+ resp = invoke_with_cb(
+     name="llm.completions",
+     callback=lambda: http_client.post(LLM_URL, json=payload, timeout=8),
+     fallback=lambda e: {"status":"degraded","choices":[{"text":"Service temporarily unavailable"}]}
+ )
```

---

## Test plan checklist (agent must include code)

* Unit: open/closed/auto; manual override; Mongo reload; success_threshold closing; non-retriable errors ignored; backoff growth + cap + jitter bounds.
* Chaos: injected 5xx/timeouts → breaker opens; then recovers after `reset_timeout` with backoff and success threshold.
* Property: backoff monotonicity; jitter within ±fraction.
* Concurrency: multiple threads/processes using same breaker `name` behave correctly.

---

## Observability (names the agent must emit)

* Counters: `cb.open.total`, `cb.half_open.total`, `cb.closed.total`, `cb.short_circuits.total`, `cb.fallback.total`
* Histograms: `cb.backoff.seconds`, `cb.latency.seconds`
* Logs: state transitions with `name`, `old_state`, `new_state`, `reason`

---

## Quick critique of your original prompt & what we fixed

* **Ambiguity removed**: explicitly constrained the implementation to `ent_pycore/circuit_breaker.py` and banned new deps.
* **PE asks embedded**: wrapper+callback, Mongo-backed config with manual override, half-open exponential backoff.
* **Evaluation ready**: acceptance criteria + tests + observability naming.
* **Output structure**: a single markdown spec with diffs and rollout steps so a PE can review fast.
* **Data model**: provided a concrete Mongo schema and backoff definition to avoid interpretation gaps.

---

If you want, paste your actual `<< USE_CASES >>` and I’ll tailor the acceptance tests and example snippets to those flows.



10:58 PM /06oct
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Got it—your current prompt has the right intent, but it’s a bit open-ended and leaves the agent guessing about scope, artifacts, and what “good” looks like. Below is a tighter, drop-in **zero-shot instruction template** you can paste into your runbook. It bakes in: clear goals, strict outputs, “before/after” diffs with *line-level* commentary, explicit references to `ent_pycore.circuit_breaker`, realistic data walkthroughs, tests, and a self-audit checklist “like a Google engineer.”

---

# Upgraded Zero-Shot Instruction (copy/paste)

## Role

You are a **principal-level software engineer** improving reliability for an LLM backend. Think rigorously, justify trade-offs, and produce production-ready code and tests.

## Context

* We have an internal library **`ent_pycore`** with a module **`circuit_breaker.py`** (source code is provided in this task).
* Our service code lives in a repo with `src/` and `tests/`.
* File to modify: **`src/agent.py`** (LLM calling logic).
* Goal: Add a robust **circuit breaker** around calls to external LLM APIs using **`ent_pycore.circuit_breaker`**.

## Inputs (you will be given)

1. The current `src/agent.py`.
2. The raw source of `ent_pycore/circuit_breaker.py`.
3. Execution environment constraints (Python version, framework, logging).
4. Any repo conventions (lint/format/test commands).

## Deliverables

1. **Design brief (≤1 page)**

   * Why a circuit breaker here, failure modes, chosen config (fail threshold, reset timeout, success threshold), fallback strategy, observability.
2. **Code changes**

   * A **Before/After diff** for *each changed file* with **line-by-line explanations** of what the code does and why.
   * New/modified files must include header comments showing ownership, docstrings, and links if applicable.
3. **APIs used from `ent_pycore.circuit_breaker`**

   * List the exact classes/functions you used; for each, include signature, parameters, return types, and how/where they’re invoked.
4. **Real-time data walkthrough** *(mandatory)*

   * Walk through a realistic call sequence showing state transitions (**CLOSED → OPEN → HALF_OPEN → CLOSED**), with concrete example values: timestamps, counters, latency (ms), exceptions (e.g., `TimeoutError`, HTTP 500), and fallback payloads.
   * For every non-trivial line you changed or added, state: **(a)** what data the line sees at runtime, **(b)** expected side-effects, **(c)** how errors propagate or are suppressed.
5. **Test plan & tests**

   * Unit tests: normal path, repeated failures opening the breaker, backoff/reset, HALF_OPEN probe success/failure, metrics/log assertions, fallback path.
   * Include **table-driven tests** and one **property-based** check for idempotence or retry limits.
   * Provide commands to run locally (`pytest -q`) and expected outputs.
6. **Operational readiness**

   * Metrics (list exact metric names, labels), logs (message templates), alerts (firing conditions), config toggles/env vars, and safe defaults.
   * Rollout plan + rollback guardrails.
7. **Risks & mitigations (concise)**

   * E.g., false opens under intermittent timeouts, thundering herd on HALF_OPEN, metric cardinality bloat—plus mitigations.

## Circuit Breaker Requirements

* **States**: `CLOSED`, `OPEN`, `HALF_OPEN`.
* **Config (defaults unless overridden by env):**

  * `fail_max`: 5 consecutive failures to OPEN
  * `reset_timeout`: 60s before HALF_OPEN
  * `success_threshold`: 2 successful probes to return to CLOSED
  * **Failure classification**: count `TimeoutError`, 5xx HTTP, `RateLimitError`; exclude 4xx except 429.
* **Fallback**: Return a safe degraded response: `{"text":"Service temporarily unavailable","mode":"degraded","request_id":<uuid>}` and set `X-CB-State` header in server responses when applicable.
* **Observability**:

  * Counters: `cb_failures_total{call="llm"}`, `cb_fallback_total{call="llm"}`
  * Gauges: `cb_state{call="llm"}` (0=CLOSED,1=OPEN,2=HALF_OPEN)
  * Histograms: latency of upstream call and fallback path
  * Logs: structured JSON with `cb_state`, `fail_count`, `cooldown_remaining_ms`, `request_id`.

## Tasks

1. **Audit** `agent.py` call path(s) to the LLM. Identify exact call sites, retries, and timeouts. Remove duplicate retry logic if the breaker already handles it (avoid retry storms).
2. **Integrate** the breaker from `ent_pycore.circuit_breaker`:

   * Instantiate with explicit config + env overrides.
   * Wrap only the *outer* LLM call (not individual lower-level sockets).
   * Implement a **single** fallback function that is fast, deterministic, and observable.
3. **Thread-safety**: Ensure the breaker instance is shared appropriately (module-level singleton or DI container) without cross-tenant bleed if multi-tenant.
4. **Idempotency**: Do not change side effects for already-sent upstream requests on fallback.
5. **Backpressure**: When OPEN, short-circuit immediately—do **not** queue.
6. **Half-open probes**: Limit concurrent probes (e.g., semaphore or built-in limiter if available).

## Output Format (strict)

Return a single Markdown document with these sections (in order):

1. **Summary**
2. **Design Brief**
3. **Changed Files (Diff + Line-by-Line Explanations)**

   * For each file: *Before/After* fenced code blocks, then an **Explanation Table** with columns: `Line`, `Runtime Data Example`, `Behavior`, `Why`.
4. **ent_pycore APIs Used** (signatures + rationale)
5. **Real-Time Data Walkthrough** (state timeline with concrete values)
6. **Test Plan & Commands** (include code)
7. **Operational Readiness** (metrics/logs/alerts/config)
8. **Risks & Mitigations**
9. **Appendix: Config & Env Vars**

## Self-Audit Checklist (fail the task if any box unchecked)

* [ ] Every non-trivial changed line has a **runtime data example**.
* [ ] All breaker states and transitions are demonstrated with timestamps.
* [ ] Fallback path validated with tests and metrics.
* [ ] `ent_pycore` APIs referenced with exact signatures and links to code sections provided.
* [ ] Diff compiles; `pytest -q` passes locally.

## Style & Constraints

* Pythonic, typed where useful; clear docstrings.
* Avoid hidden global state and double retries.
* Keep public behavior backward compatible unless justified.

---

## Mini Example: What the “Real-Time Data Walkthrough” Should Look Like

**Timeline (excerpt):**

* `t=00:00.000` State=CLOSED, `fail_count=0`. Call `llm()` → **TimeoutError @ 3000ms** → record failure (1/5).
* `t=00:00.120` CLOSED (4 more failures: HTTP 502, 500, 500, 429) → reaches 5/5 → **transition to OPEN**.
* `t=00:00.650` OPEN: next request short-circuited in **~1ms**, fallback returns `{"mode":"degraded"}`; metrics: `cb_fallback_total++`.
* `t=00:60.700` After `reset_timeout=60s`, allow **HALF_OPEN** single probe; probe succeeds twice under `success_threshold=2` → **back to CLOSED**.

**Line-by-line example (for a changed line):**

* `resp = cb.call(lambda: llm_client.complete(prompt, timeout=2.5))`

  * *Runtime data*: `prompt_len=224`, `timeout=2.5s`, `cb_state=HALF_OPEN`, `trial=1/2`
  * *Behavior*: executes guarded call; if success → `success_count++`; if failure → `OPEN` again
  * *Why*: ensure controlled probe during recovery; prevents stampedes.

---

## “Like a Google Engineer” Enhancements (baked in)

* Explicit **acceptance criteria**, **observability**, and **rollback** plan.
* **Failure taxonomy** documented; **cardinality-safe** metrics.
* Clear **ownership** in file headers and PR description template.
* **Risk table** with quantified blast radius and detection channels.

---

# Ultra-Concise Variant (for quick runs)

**Goal:** Add `ent_pycore.circuit_breaker` around LLM calls in `src/agent.py`.

**Do:**

* Provide **Before/After** diffs with **line-level runtime examples**.
* Show state transitions with timestamps & counters.
* List `ent_pycore` APIs used (signature + where/how).
* Add unit tests: open/close/half-open/fallback/metrics.
* Output sections: Summary, Diffs+Explanations, APIs Used, Walkthrough, Tests, Ops, Risks.

**Breaker Defaults:** `fail_max=5`, `reset_timeout=60s`, `success_threshold=2`. Failures = timeout, 5xx, 429. Fallback returns deterministic degraded payload.

**Fail the task if:** any changed line lacks a runtime example or tests don’t cover fallback and HALF_OPEN.

---

# Why this is better

* **Removes ambiguity** (explicit inputs, outputs, and config).
* **Forces concrete reasoning** (line-level runtime data + state timeline).
* **Operationalizes** the work (metrics/logs/alerts/rollout).
* **Testable** (clear cases + commands).
* **Review-ready** for PEs (diffs + rationale).

If you want, I can tailor this to your exact `agent.py` and a sample `ent_pycore.circuit_breaker` signature you’re using.



11:00 pm
>>>>>>>>>>>>>>>>>>>>

Love it. Here’s a copy-paste, battle-tested zero-shot prompt that makes a top agent produce a step-by-step, code-anchored explainer of every circuit breaker in your repo—covering design, failure detection, fallbacks, observability, edge cases, and PE-grade recommendations.

⸻

Zero-Shot Prompt — “Explain Every Circuit Breaker in Our Codebase (Step-by-Step, PE-Ready)”

Role
Act as a Staff/Principal engineer at a billion-dollar startup: concise, accurate, SRE-aligned, and documentation-first.

Context
	•	We use an in-house circuit breaker pattern (e.g., CircuitBreaker(fail_max=5, reset_timeout=60, success_threshold=2)), possibly via ent_pycore or local equivalents.
	•	Codebase layout: src/ (code), tests/ (pytest).
	•	Primary files likely include src/circuit_breaker.py and call sites across services/clients.
	•	Objective: explain, not redesign—produce a precise, code-referenced walkthrough of how each breaker works, how failure is detected, and what the fallback is.

Inputs You Can Assume
	•	Repo root: <<REPO_ROOT>> (default: current working tree)
	•	Key modules: <<PRIMARY_FILES_OR_GLOBS>> (e.g., src/**/circuit_breaker*.py, src/**/*client*.py, src/**/*service*.py)
	•	Runtime config sources in order of precedence (override if code differs): ENV → config file → code defaults.
	•	If ent_pycore is a distribution/namespace, use AST + safe import guards to find its exported CB pieces.

Task
Produce a single Markdown report that explains every circuit breaker instance and its behavior, with code pointers and runnable reproductions.

Method (required)
	1.	Discover all CBs
	•	Statically (AST) find CircuitBreaker(...) and wrappers; include custom factories or decorators.
	•	For each instance, resolve thresholds/timeouts/success criteria and where they come from (ENV, config, defaults).
	2.	Trace usage
	•	Build a concise call graph from entrypoint → CB → upstream dependency (e.g., LLM API).
	•	Note sync vs async, thread/process safety, and whether the breaker is per-dependency or global.
	3.	Explain behavior step-by-step
	•	Show state machine (Closed → Open → Half-Open), with what triggers transitions (consecutive failures vs error-rate window), and timers (cool-down/reset).
	•	Clarify what counts as “failure”: exception types, status codes, timeouts, circuit open rejections.
	•	Describe fallbacks (cached value, degraded response, null-object, fast-fail with user-friendly message).
	4.	Observability mapping
	•	List emitted metrics/logs/traces, names & labels; sample queries (Prometheus) and suggested alerts (flapping, open too long, probe failures).
	5.	Edge cases & hazards
	•	Double-wrapping, async + blocking mix, global state, multi-process workers, config hot-reload, high-cardinality metrics, retry storms.
	6.	Mini reproduction harness
	•	Provide simple, runnable code snippets or pytest examples that trigger: normal success, consecutive failures to Open, Half-Open probe success/fail, timeout path.
	7.	Prioritized recommendations
	•	P0/P1 suggestions to harden behavior, improve observability, or fix pitfalls—without changing public APIs unless strongly justified.

Output Format (strict, in order)
	1.	# Executive Summary
	•	One paragraph: how many CBs found, where, and headline risks or confidence level.
	2.	# Inventory of Circuit Breakers (table)

ID	Module:Line	Breaker Var	Upstream/Callsite	Policy (fail_max / reset_timeout / success_threshold)	Failure Signals Counted	Fallback



	3.	# State Machine & Semantics
	•	Mermaid stateDiagram-v2 with transitions & timers.
	•	Bulleted rules for what opens/closes; how Half-Open probes work; what’s ignored (e.g., 4xx vs 5xx).
	4.	# Per-Breaker Deep Dive (repeat per CB instance)
	•	Path & code refs: src/.../file.py:L123 (show small, focused snippet).
	•	Config resolution: ENV keys, config keys, defaults; resolved effective values.
	•	Call flow (Mermaid sequence diagram) from endpoint/function → CB → upstream.
	•	Failure taxonomy: exceptions/status codes/timeouts that trip the CB.
	•	Fallback behavior: exact return shape/examples; user impact.
	•	Concurrency model: thread/process safety notes; async peculiarities.
	•	Instrumentation: metric/log names & sample entries.
	5.	# Reproduction Recipes
	•	Pytest examples (or curl/script) to demonstrate:
	•	normal path,
	•	N consecutive failures → Open,
	•	cool-down → Half-Open,
	•	success_threshold successes → Closed,
	•	timeout vs error differentiation.
	•	Show expected outputs and /cb/state (if available).
	6.	# Observability & Alerting
	•	Metrics list, low-cardinality labels, example PromQL (or pseudo):
	•	open duration SLI, state transition counter, probe success rate.
	•	Alert sketches: breaker flapping, breaker stuck Open, sudden rise in open rejections.
	7.	# Edge Cases & Known Gaps
	•	E.g., double wrap, per-tenant breakers, retries causing thundering herd, multi-worker desync.
	8.	# Recommendations (P0/P1/P2)
	•	Actionable, scoped changes; risk & effort notes; suggested owners.
	9.	# Appendix
	•	AST/grep queries used, config key map, longer code excerpts if needed.

Acceptance Criteria
	•	Every circuit breaker in the codebase is listed with module:line references and resolved config values.
	•	Clear, step-by-step narrative of how failure is detected and exactly what fallback returns.
	•	Includes state diagram + sequence diagrams, runnable reproduction snippets, and concrete alert ideas.
	•	Uses precise language (no vague “may/should” unless truly unknown; then call it out as an assumption).
	•	No third-party CB design theory unless tied back to our code.

House Rules
	•	Be concise but specific. Quote only the minimal code needed to explain.
	•	Do not expose secrets; keep example logs/metrics PII-safe.
	•	If something is unknown or inferred, label it “Assumption” and proceed.

Optional Inputs (fill if known)
	•	<<ENV_KEYS>> for CB config (e.g., CB_FAIL_MAX, CB_RESET_TIMEOUT, CB_SUCCESS_THRESHOLD).
	•	<<UPSTREAMS>> names and SLIs/SLOs to anchor recommendations.
	•	Whether the repo exposes /cb/state//metrics; if not, note how to emulate.

⸻

Pro tips to strengthen your prompt even more
	•	Pin scope: add explicit file globs or entrypoints the agent must analyze.
	•	Give SLOs: target success rate/latency/error budget window to contextualize fallbacks.
	•	Name env keys: so config resolution is deterministic.
	•	Ask for risk-ranked recos with estimated effort (S/M/L) to help prioritization.
	•	Require runnable recipes: reviewers can verify behavior in minutes.

If you want, paste your repo paths and ENV keys and I’ll pre-fill the placeholders for you.


8:00 PM
>>>>>>>>>>>>>>>

Awesome—here’s a tight, copy-paste-ready **zero-shot prompt** you can give to the “most capable agent.” It forces runnable code, tests, and docs, all **under `src/temp/livetest`**, to A/B test your circuit breaker **before vs after** via FastAPI on localhost.

---

# Zero-Shot Prompt — FastAPI “Before vs After” Circuit Breaker (localhost, under `src/temp/livetest`)

**Role**
Act as a Staff/Principal backend & platform engineer. Ship minimal, production-grade, fully runnable code with tests and docs.

**Context**
We’re evaluating a Python **circuit breaker (CB)** for an LLM API call. Current usage pattern in our app:

```python
cb = CircuitBreaker(fail_max=5, reset_timeout=60, success_threshold=2)

def call_llm_api(...):
    return cb.call(llm_api, ...)
```

We need a **dummy FastAPI service** to compare behavior **before (no CB)** vs **after (with CB)** on **localhost**.

**Hard constraints**

* **All files must live under:** `src/temp/livetest/...` (use these exact paths).
* Use **FastAPI + uvicorn**.
* **Do not** use any third-party circuit-breaker libraries. Implement a small CB that mirrors the above semantics (Closed → Open → Half-Open with `fail_max`, `reset_timeout`, `success_threshold`).
* Keep dependencies minimal (FastAPI, pydantic, httpx (if needed), prometheus-client (optional), pytest).
* Python **3.10+**. No ellipses in code; provide complete files.

**What to build**

1. **Endpoints**

   * `GET /v1/before/complete` → calls simulated upstream **without** CB (control).
   * `GET /v1/after/complete` → same call **with** CB.
   * `GET /simulate/upstream` → simulated upstream that can:

     * return success,
     * return 5xx,
     * or timeout/sleep.
       Control via query params or ENV: `fail_rate` (0.0–1.0), `consec_fail` (int), `timeout_ms` (int).
   * `GET /cb/state` → JSON with `state` (`closed|open|half_open`), counters (attempts/success/failure/opens), timestamps.
   * `GET /healthz` → 200 OK.
   * `GET /metrics` → Prometheus metrics (optional but preferred).

2. **Config (ENV with sane defaults)**

   * `CB_FAIL_MAX=5`, `CB_RESET_TIMEOUT=60`, `CB_SUCCESS_THRESHOLD=2`
   * `UPSTREAM_FAIL_RATE=0.0`, `UPSTREAM_CONSEC_FAIL=0`, `UPSTREAM_TIMEOUT_MS=0`
   * `SERVER_HOST=127.0.0.1`, `SERVER_PORT=8080`
   * Respect ENV → .env → defaults precedence (simple, documented).

3. **Code organization (exact paths)**

```
src/temp/livetest/requirements.txt
src/temp/livetest/requirements-dev.txt            # optional (pytest, coverage, ruff/black)
src/temp/livetest/app/__init__.py
src/temp/livetest/app/config.py
src/temp/livetest/app/circuit_breaker.py
src/temp/livetest/app/upstream.py
src/temp/livetest/app/main.py
src/temp/livetest/tests/test_cb_api.py
src/temp/livetest/Makefile
src/temp/livetest/README.md
src/temp/livetest/.env.example
# Optional:
# src/temp/livetest/Dockerfile
# src/temp/livetest/pyproject.toml (ruff/black), mypy.ini
```

4. **Circuit breaker requirements (must)**

   * States: **Closed → Open → Half-Open**.
   * **Open** on either `fail_max` consecutive failures **or** (if you implement rate windows) an error rate threshold—choose one consistent policy and document it (default: consecutive failures only).
   * **Half-Open** allows limited probes; **Close** after `success_threshold` consecutive successes; otherwise **Open** again.
   * Thread/process-safe state changes (use locks for shared state).
   * Optional bounded retry with jitter **only** inside the “after” path; avoid retry storms; document the policy.
   * Structured JSON logs for state transitions (no secrets/PII).
   * Minimal, low-cardinality Prometheus metrics if `/metrics` is included.

5. **Tests (pytest)**

   * Use FastAPI TestClient or httpx AsyncClient.
   * Cover:

     * Success path (no failures).
     * Failing upstream: breaker **opens** by/before `fail_max`.
     * Cool-down elapses → **half-open**; on `success_threshold` successes → **closed** again.
     * Timeout behavior vs explicit 5xx.
     * `/cb/state` accuracy.
     * (Nice-to-have) simple concurrency test (parallel calls) showing safe transitions.
   * Include coverage run (target ≥ **85%** for CB module).

6. **Dev UX (localhost)**

   * `Makefile` targets: `run`, `test`, `lint`, `fmt`, `coverage`.
   * `README.md` quickstart for **localhost only** (no Docker required), plus optional Docker instructions.
   * Example `curl` commands to trigger scenarios:

     * random failures: `curl 'http://127.0.0.1:8080/v1/after/complete?fail_rate=0.7'`
     * consecutive failures then success: `...&consec_fail=5`
     * timeouts: `...&timeout_ms=1200`
   * Show how to read `/cb/state` and `/metrics`.

**Acceptance criteria**

* With `UPSTREAM_CONSEC_FAIL=5`, calling `/v1/after/complete` opens the breaker **by/before the 5th failure**, stays **open** for `CB_RESET_TIMEOUT` seconds, then transitions to **half-open**.
* During **half-open**, after `CB_SUCCESS_THRESHOLD` successful probes, the breaker **closes**.
* `/v1/before/complete` reflects raw upstream behavior (control).
* `/cb/state` returns correct state & counters; logs include transition entries.
* `pytest` passes locally; coverage target met.
* Service runs on **[http://127.0.0.1:8080](http://127.0.0.1:8080)** with `make run` or the README command.

**Output format (strict; in this order)**

1. **Project tree** (exact paths rooted at `src/temp/livetest/`)
2. `requirements.txt` (+ optional `requirements-dev.txt`)
3. `app/config.py`
4. `app/circuit_breaker.py`
5. `app/upstream.py`
6. `app/main.py`
7. `tests/test_cb_api.py` (and any helpers)
8. `Makefile`
9. `README.md`
10. `.env.example`
11. *(Optional)* `Dockerfile` and any lint/format config

> Provide **full file contents** for each (no placeholders or ellipses).

**House rules**

* No new complex deps; keep it lean.
* Small, readable functions; docstrings + type hints.
* Consistent logging format; redact sensitive values.
* If you must make an assumption, **state it in README** and proceed.

**Starter defaults (use unless overridden)**

* `CB_FAIL_MAX=5`, `CB_RESET_TIMEOUT=60`, `CB_SUCCESS_THRESHOLD=2`
* `SERVER_HOST=127.0.0.1`, `SERVER_PORT=8080`

---

## Bonus (things “a Google-level engineer” wouldn’t miss)

* Clear **state transition diagram** (Mermaid) in the README.
* **Alerting ideas** for `/metrics`: breaker flapping, open too long.
* **Operational switches**: env flag to disable CB, and a safe default when config is absent.
* Minimal **security hygiene**: no secrets in logs; avoid high-cardinality labels; validate/limit user-supplied params.

Want me to generate the **reference code** for this prompt next so you can run it immediately?

6:50PM
>>>>>>>>>>>>>>>>>>>>>.

Got it—great raw brief. Here’s a sharper, copy-paste-ready zero-shot prompt that sets the “most capable agent” up for success, plus a quick checklist of why these upgrades matter.

# What to improve (why your current prompt underdelivers)

* **Scope + guardrails:** Be explicit about allowed code changes (which files, what not to touch), dependencies (only in-house libs), and non-goals.
* **Inputs & assumptions:** Tell the agent exactly what you’re providing (repo paths, module list, target SLOs) and how to proceed if something’s missing.
* **Deliverables & formats:** Require deterministic outputs: a design ADR, a unified diff, test files, coverage target, and a PE-facing report.
* **Quality bars:** Bake in acceptance criteria, failure modes to simulate, and observability requirements.
* **Reasoning signal (without chain-of-thought):** Ask for decision records and trade-off summaries instead of hidden step-by-step thoughts.
* **Enterprise specifics:** Thread/process safety, sync/async, config precedence, feature flagging, rollback plan, and security/compliance notes.
* **ent_pycore reality check:** If it’s a distribution/namespace package, include a fallback way to discover submodules.

---

# Upgraded Zero-Shot Prompt (drop-in)

**System / Role**
You are a senior platform engineer embedded on a high-performing team. Operate like a Staff/Principal engineer at a billion-dollar startup: pragmatic, security-minded, SRE-aligned, and documentation-first.

**Context**

* We must implement/extend a **circuit breaker** using our **in-house library `ent_pycore`**, reusing the enterprise patterns already present in **`src/circuit_breaker.py`**.
* Codebase layout: `src/` (code), `tests/` (pytest). About 30+ scripts exist.
* You will receive the **use case(s)** (see `<<USE_CASES>>`) and a **list of ent_pycore modules relevant to circuit breaking**. Primary file of record: **`src/circuit_breaker.py`**.
* **Non-goals:** Do not introduce third-party CB libraries. Do not change public APIs outside the designated module(s) unless explicitly justified and documented.

**Inputs you have**

1. `<<USE_CASES>>`
2. List of relevant `ent_pycore` modules/subpackages and the file `src/circuit_breaker.py`.
3. SLO targets & constraints (fill if known; otherwise propose defaults and justify):

   * Target success rate: `<<SUCCESS_RATE>>`
   * p95 latency: `<<P95_MS>> ms`
   * Error budget window: `<<WINDOW>>`

**If imports are tricky (ent_pycore is a distribution/namespace)**

* Prefer **in-repo references** already used in `src/circuit_breaker.py`.
* If discovery is required, attempt safe introspection (e.g., `importlib.metadata` for top-level names, guarded imports). If still unknown, **document assumptions** and proceed with the best-aligned design to `src/circuit_breaker.py`.

**Task**
Design and implement a resilient, observable circuit breaker for the provided use case(s), **only** by extending/refactoring existing enterprise logic from `src/circuit_breaker.py` and related in-house modules.

**Functional requirements (must)**

* Support **Closed → Open → Half-Open** states with configurable thresholds:

  * **Open** when failure rate or consecutive failures exceed thresholds within a rolling window.
  * **Half-Open** with limited probe requests and success criteria to close.
* **Sync & Async** call paths (if codebase supports async): ensure consistent behavior and thread/process safety.
* **Timeouts + retries with jitter** integrated with breaker state (no retry storms; respect backoff).
* **Config precedence:** env → service config → sane defaults. Hot-reload safe if present in repo patterns.
* **Observability:** counters/timers for attempts, successes, failures, state transitions; log correlation IDs; OpenTelemetry hooks if used; minimal cardinality labels.
* **Feature flags/kill switch:** enable/disable breaker quickly at runtime (enterprise pattern).
* **Fallbacks / graceful degradation:** return cached/partial/placeholder responses where the use case allows.
* **Security/compliance:** no secrets in logs; PII-safe telemetry; adhere to existing logging levels and scrubbers.

**Non-functional requirements (must)**

* Deterministic behavior under concurrency; no race conditions on state transitions.
* Avoid global mutable state unless protected; prefer scoped breakers per external dependency.
* Clear, minimal public surface; internal helpers documented and tested.
* No new external dependencies.

**Design & Implementation steps (what to produce)**

1. **Short ADR (Architecture Decision Record)**

   * Problem statement, constraints, options considered, and the chosen approach with trade-offs.
   * State machine diagram (ASCII or Mermaid).
2. **Proposed config schema** with defaults and examples (YAML/ENV).
3. **Code changes** limited to `src/circuit_breaker.py` and necessary adjacent internal modules.
4. **Tests (pytest)** in `tests/` covering:

   * Threshold crossings, half-open probes, backoff/jitter, timeouts vs errors, concurrency, async (if applicable), idempotent retries, feature flag on/off, and flapping prevention.
   * Include property-based tests (if Hypothesis is already used in repo; else keep to pytest).
   * **Coverage target:** ≥ 90% for changed lines; report coverage summary.
5. **Observability plan**: emitted metrics/logs/traces with names, types, and example queries/dashboards.
6. **PE-facing change report** (see “Final deliverables” format) with before/after snippets and rationale.
7. **Rollback plan**: flag/Config switch + revert commands; documented blast radius and owner runbook pointers.

**Acceptance criteria (verify via tests & local runs)**

* Under `<<FAILURE_PATTERN_1>>` (e.g., 5xx burst), breaker **opens** within `<<N>>` requests and **stays open** for the cool-down interval.
* Under `<<RECOVERY_PATTERN>>`, half-open allows `<<M>>` probes; on `>=<<K>>` successful probes, breaker **closes**.
* p95 latency stays within `<<P95_MS>> ms` during normal load; does not exceed `<<DEGRADED_P95_MS>> ms` during Open state with fallback.
* Metrics/logs contain **state_transition** events and can drive alerts (burn-rate or SLO alerts).
* No data races in concurrent test harness (document evidence or include a concurrency test).

**Output formatting (strict)**
Return a **single Markdown document** with the following top-level sections **in order**:

1. `# ADR – Circuit Breaker for <<USE_CASE_NAME>>`
2. `# Design`

   * State machine diagram
   * Config schema
   * Thread/async model
   * Failure & recovery policy
3. `# Patch (Unified Diff)`

   * Only include diffs for files you modify/create. Use unified diff format with paths rooted at repo root (e.g., `src/circuit_breaker.py`, `tests/test_circuit_breaker.py`).
4. `# Test Plan & Coverage`

   * Test matrix table; commands to run; expected coverage summary
5. `# Observability`

   * Metrics/logs/traces spec; sample alert rules or queries
6. `# PE Review Notes`

   * Before/after code snippets (small, focused), rationale, trade-offs, risk, rollback
7. `# TODO / Follow-ups`

   * Small backlog items explicitly out of scope

**House rules**

* Do **not** add third-party deps.
* Preserve repository coding style and lint rules.
* Prefer small, composable functions; thorough docstrings.
* If an assumption is made (e.g., missing SLOs), **state it clearly** and continue.

**Self-check rubric (brief)**

* Correctness (states/thresholds/time)
* Concurrency safety
* Observability completeness
* Test depth & coverage
* Configurability & ops ergonomics
* Simplicity & maintainability

---

## Optional starter you can fill now

* `<<USE_CASES>>`: e.g., “Protect LLM completion API calls to `provider_X` from cascading failures/timeouts; fallback to cached answer or graceful message.”
* Threshold sketch: “Open on ≥50% error rate over last 20 requests or ≥5 consecutive failures; half-open after 30s with 3 probes; close on 3/3 success.”

---

### Why this works

* Forces **enterprise-grade outputs** (ADR, diff, tests, observability, rollback) your PE expects.
* Constrains the agent to **reuse in-house patterns** (no scope creep).
* Makes results **auditable and runnable** (unified diff + pytest commands).
* Anticipates **ent_pycore** import quirks with a safe fallback approach.

If you want, paste your actual `<<USE_CASES>>`, SLOs, and any ent_pycore module list you have, and I’ll slot them into the template for you.


7:15
>>>>>>>>>>>>>>>>

Great start. Your prompt has the right intent (“design a circuit breaker using the in-house library + update tests + PR for review”), but it’s missing the scaffolding that lets a capable agent act deterministically: concrete inputs/outputs, constraints, acceptance criteria, and a step plan.

Below I’ll (1) pinpoint the gaps, (2) give you a copy-paste upgraded prompt template tailored to your situation (ent_pycore + LLM circuit breaker), and (3) add a short checklist you can keep using across tasks.

---

# 1) What’s strong vs. what’s missing

**Strong**

* Clear objective: design a circuit breaker for a top-priority use case.
* Mentions internal lib (`ent_pycore`) and existing 30+ scripts.
* Requires tests and a review-ready output.

**Missing / ambiguous**

* No explicit input artifacts (where is the JSON, which repo branch?).
* No target language/runtime/test framework versions.
* No SLOs/thresholds (how to decide open/half-open/closed? which error taxonomy?).
* No output format (PR layout, folders, filenames).
* No acceptance criteria or evaluation plan (benchmarks, chaos tests, pass/fail).
* No guardrails (observability, idempotency, config flags, rollback).
* No UX/behavior definition during degrade mode.
* No security/compliance constraints.

---

# 2) Copy-paste upgraded zero-shot instruction (fill the placeholders)

> **Title:** Design & integrate an LLM circuit breaker using `ent_pycore` + test + PR
> **Audience:** Staff/Principal-level agent; produce review-ready work, not notes.

**Role & Mission**
You are a senior backend engineer. Your job: design and implement a production-grade circuit breaker for our highest-priority use case using our internal library **`ent_pycore`**. You must integrate it into the existing codebase, add tests, and produce a review-ready PR.

**Context**

* We depend on an LLM Completion API for core user workflows. Outages/timeouts currently cause cascading failures and SLO breaches.
* We have ~30 existing Python scripts in this repo that call the LLM.
  `REPO:` `<link or path>` (branch: `<branch>`).
* We have **JSON manifests** listing all available `ent_pycore` functions (module path, name, signature, docstring).
  `ARTIFACTS:` `<path/to/functions_index.json>` `<path/to/any_additional.json>`
  If runtime import of `ent_pycore` is not possible (e.g., distribution-style package), **work from the JSON** to map capabilities.

**Top-Priority Use Case**
LLM Completion call (sync path).

* **Trigger:** user submit → backend → LLM.
* **Pain:** timeouts/5xx spikes cause request storms and poor UX.
* **Goal:** protect upstream and preserve UX via circuit breaker + graceful degrade.
  Fill any details you need from code and manifests; where unknown, propose sensible defaults and call them out.

**Non-Negotiable Requirements**

* **Language/Runtime:** Python `X.Y`, pytest `Z`.
* **Observability:** emit metrics/traces/logs for: state transitions, failure counts, rolling windows, short-circuits, half-open probes, fallback activations.
* **Config:** thresholds/timeouts must be **configurable** (env or YAML), with sane defaults.
* **Safety:** idempotent operations, no sensitive data in logs, follow org logging policy.
* **Compatibility:** do not break existing public functions; add feature-flag `CB_ENABLED` default `false` (we’ll canary).
* **Docs:** create/update a `README.md` and a runbook section.

**Design Constraints & Targets**

* **States:** Closed → Open → Half-Open with exponential backoff + jitter for re-probe.
* **Trip policy:** open when any of:

  * rolling error-rate ≥ **`ERROR_RATE_THRESHOLD`** over last **`N`** calls,
  * consecutive failures ≥ **`CONSEC_FAILS`**,
  * p95 latency ≥ **`P95_LATENCY_MS`** for **`M`** windows.
* **Recovery:** allow **`K`** trial calls in Half-Open; if success-rate ≥ **`RECOVERY_RATE`**, close else re-open.
* **Partitioning:** maintain breakers per **service + tenant (or API key)** to avoid noisy-neighbor effects.
* **Fallbacks:** when open, do (in order):

  1. return cached last-good response (if TTL valid),
  2. route to a cheaper/backup model (configurable),
  3. return a minimal “degraded” response with user-safe messaging.
* **SLO alignment:** define & report: availability, p95 latency, error budget burn.

**Inputs You Can Use**

1. `functions_index.json` describing `ent_pycore` callable surface.
2. Source tree under `<repo path>`.
3. Test data samples under `<tests/data>` (or create fixtures if absent).

**Tasks (do them in order)**

1. **Inventory Mapping:** Parse the JSON and produce a table: module path, function name, signature, doc summary, probable category (metrics/timing/retries/circuit-breaker helpers). Identify the most relevant `ent_pycore` utilities for:

   * rolling windows, counters, timers, atomic state, locks, monotonic time, retry/backoff, config.
     If something is missing, propose minimal adapters.
2. **Design Spec (short):**

   * Architecture diagram (ASCII or Mermaid) showing call path and breaker boundaries.
   * Config schema (`config/circuit_breaker.yml`) with defaults and comments.
   * Error taxonomy (timeout, 429/5xx, connection errors) and which should count toward tripping.
   * Observability plan (metrics names, labels, sampling for traces).
3. **Implementation:**

   * Add a `circuit_breaker/` module with a thin, reusable wrapper (`@with_circuit_breaker` or context manager) that integrates with `ent_pycore`.
   * Wire it into the top-priority LLM call sites via dependency injection / wrapper.
   * Respect feature flag `CB_ENABLED`.
4. **Tests:**

   * Unit tests for state transitions, thresholds, backoff/jitter, half-open probes, partitioning.
   * Property-based or table-driven tests for rolling windows.
   * Integration tests that simulate spike of timeouts/5xx and verify correct degrade behavior and metrics.
   * Basic chaos test (loop: fail N% randomly) to validate stability and observability.
5. **Docs & Ops:**

   * Update `README.md` with quickstart, config, and examples.
   * Add `RUNBOOK.md` section: how to flip the flag, safe rollback, and dashboard links.
   * Provide a one-page PR description with risks and canary plan.

**Deliverables (exact output format)**

* `design/circuit_breaker_spec.md` (≤ 2 pages, includes thresholds table & Mermaid diagram)
* `circuit_breaker/` Python module (well-documented)
* Modified call sites under `<paths>` guarded by `CB_ENABLED`
* `config/circuit_breaker.yml` (defaults + comments)
* `tests/test_circuit_breaker_*.py` (pytest)
* `docs/RUNBOOK.md#circuit-breaker`
* **PR text** (markdown): summary, screenshots of metrics if possible, rollout plan, rollback plan.

**Acceptance Criteria**

* All tests pass in CI.
* In a 5-minute fault-injection run (≥50% timeouts), breaker must:

  * open within **X seconds** or **Y failed calls**,
  * cap outbound calls to ≤ **RPS limit** while open,
  * emit metrics: `cb_state{service,tenant}`, `cb_open_total`, `cb_short_circuits_total`, `cb_half_open_probe_total`, `cb_fallback_total`.
* With `CB_ENABLED=false`, behavior is unchanged (backward compatibility).

**Output Style**

* Provide concise artifacts and code blocks.
* Show rationale as **bulleted summaries only** (no raw chain-of-thought).
* Where you make assumptions, call them out at the top of each file as `ASSUMPTIONS`.

**If Something Blocks You**

* If `ent_pycore` cannot be imported, operate solely from the JSON manifest and generate shims with TODOs.
* If critical data is missing, propose defaults and proceed; list gaps in the PR.

---

### (Optional) Ready-to-use skeletons the agent can emit

**Mermaid (architecture)**

```mermaid
flowchart LR
Client-->App[App Service]
App-->CB{Circuit Breaker}
CB -->|Closed/Half-Open| LLM[LLM API]
CB -.Open/Short-circuit.-> Fallback[Cache/Backup Model/Degraded]
App-->Obs[Metrics/Tracing Logs]
```

**Example config (`config/circuit_breaker.yml`)**

```yaml
cb_enabled: false
window_size: 100            # rolling window calls
error_rate_threshold: 0.25  # 25%+ errors trip
consecutive_failures: 10
latency_p95_ms_threshold: 3000
half_open_max_probes: 5
recovery_success_rate: 0.8
open_state_backoff_ms: [1000, 2000, 4000, 8000]  # with jitter
partition_keys: ["service", "tenant"]
fallbacks:
  enable_cache: true
  cache_ttl_s: 300
  enable_backup_model: true
  backup_model_name: "gpt-lite"
```

**Pytest outline**

```python
def test_trips_on_error_rate(...): ...
def test_half_open_probes_and_reclose(...): ...
def test_short_circuit_and_fallback_cache(...): ...
def test_partitioning_isolated_states(...): ...
def test_flag_off_is_noop(...): ...
```

---

# 3) Quick quality checklist (use this every time)

* Inputs specified (repo, branch, JSON paths)?
* Runtime/toolchain versions fixed?
* SLOs/thresholds defined & justified?
* Error taxonomy + which count toward tripping?
* Configurable with safe defaults? Feature flag?
* Observability: metrics names, labels, traces, logs?
* Tests: unit + integration + chaos; pass/fail criteria?
* Canary & rollback plan documented?
* Backward compatibility guaranteed?
* Security & privacy logging rules respected?

---

If you want, paste your actual JSON manifest path + repo path and I’ll tailor the thresholds and emit a first pass of `config/circuit_breaker.yml`, the metrics schema, and a pytest starter file that matches your tree.


5:44 PM
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


Great call — if `ent_pycore` is a *distribution* with many subpackages, you’ll want a sturdier scanner. Here’s a **drop-in upgraded script** with:

* **Two scan modes**:

  1. **Import mode (default)** — imports modules and uses `inspect` (fast, accurate signatures, but may run import side-effects).
  2. **AST mode (`--ast`)** — parses `.py` files without importing (safer; signatures may be partial).
* **Top-level package scan** (not just submodules).
* **Public/private filtering** (hide names starting with `_` unless `--include-private`).
* **Markdown + JSON outputs** (to stdout and optional files).
* **Package metadata** (version, location, distribution name).
* **Graceful error capture** per module.

> Run: `python scan_ent_pycore.py`
> Safer (no imports): `python scan_ent_pycore.py --ast`
> Save files: `python scan_ent_pycore.py --json out.json --md out.md`

---

### `scan_ent_pycore.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scan a Python package (default: ent_pycore) and produce:
- JSON inventory of modules, functions, classes, signatures, first-line docs
- Markdown catalog (grouped by module)
Supports two modes:
  * Import-based inspection (default)
  * AST-only scanning (--ast) without importing modules
"""

import argparse
import importlib
import inspect
import json
import os
import pkgutil
import sys
import textwrap
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any

# Python 3.8+: importlib.metadata is stdlib (fallback for older)
try:
    from importlib import metadata as ilmd  # type: ignore
except Exception:
    ilmd = None  # pragma: no cover

@dataclass
class Item:
    name: str
    kind: str  # "function" | "class"
    signature: str
    doc_head: str
    defined_in: Optional[str] = None  # file path if known
    line_no: Optional[int] = None

@dataclass
class ModuleReport:
    module: str
    error: Optional[str] = None
    items: List[Item] = None  # type: ignore

def get_package_metadata(pkg_name: str) -> Dict[str, Any]:
    meta = {"package": pkg_name}
    try:
        mod = importlib.import_module(pkg_name)
        meta["location"] = getattr(mod, "__file__", None)
        meta["package_dir"] = os.path.dirname(meta["location"]) if meta["location"] else None
    except Exception as e:
        meta["import_error"] = repr(e)
        return meta

    # Distribution metadata (best-effort)
    if ilmd is not None:
        try:
            # Some packages use different "distribution" names than import name
            # Try direct match; if fails, scan distributions and match top-level files
            dist_version = None
            dist_name = None
            try:
                dist_version = ilmd.version(pkg_name)
                dist_name = pkg_name
            except Exception:
                # fallback: pick first distribution whose files include the package directory name
                for dist in ilmd.distributions():
                    name = dist.metadata.get("Name") or dist.metadata.get("name")
                    if not name:
                        continue
                    try:
                        files = list(dist.files or [])
                    except Exception:
                        continue
                    for f in files:
                        parts = str(f).split("/")
                        if parts and parts[0] == pkg_name:
                            dist_name = name
                            dist_version = dist.version
                            break
                    if dist_name:
                        break
            if dist_name:
                meta["distribution"] = dist_name
            if dist_version:
                meta["version"] = dist_version
        except Exception:
            pass
    return meta

def should_keep(name: str, include_private: bool) -> bool:
    if include_private:
        return True
    return not name.startswith("_")

def doc_head(obj) -> str:
    d = inspect.getdoc(obj) or ""
    # First 1–2 lines (short)
    lines = d.strip().splitlines()
    return " ".join(lines[:2]).strip()

def inspect_module(module, include_private: bool) -> List[Item]:
    out: List[Item] = []
    for n, obj in inspect.getmembers(module):
        if not should_keep(n, include_private):
            continue
        if inspect.isfunction(obj) or inspect.isbuiltin(obj):
            k = "function"
        elif inspect.isclass(obj):
            k = "class"
        else:
            continue
        try:
            try:
                sig = str(inspect.signature(obj))
            except Exception:
                sig = "(signature unavailable)"
            try:
                srcfile = inspect.getsourcefile(obj) or inspect.getfile(obj)
            except Exception:
                srcfile = None
            try:
                _, lineno = inspect.getsourcelines(obj)
            except Exception:
                lineno = None
            out.append(Item(
                name=n,
                kind=k,
                signature=sig,
                doc_head=doc_head(obj),
                defined_in=srcfile,
                line_no=lineno
            ))
        except Exception:
            # Continue collecting even if one member fails
            continue
    return out

def walk_import(pkg_name: str, include_private: bool) -> List[ModuleReport]:
    reports: List[ModuleReport] = []

    # Include the top-level package itself
    try:
        root = importlib.import_module(pkg_name)
        items = inspect_module(root, include_private)
        reports.append(ModuleReport(module=pkg_name, items=items))
    except Exception as e:
        reports.append(ModuleReport(module=pkg_name, error=repr(e), items=[]))

    # Walk submodules
    try:
        root = importlib.import_module(pkg_name)
        for m in pkgutil.walk_packages(root.__path__, root.__name__ + "."):
            name = m.name
            try:
                mod = importlib.import_module(name)
                items = inspect_module(mod, include_private)
                reports.append(ModuleReport(module=name, items=items))
            except Exception as e:
                reports.append(ModuleReport(module=name, error=repr(e), items=[]))
    except Exception as e:
        # If __path__ is unavailable or walk fails
        reports.append(ModuleReport(module=f"{pkg_name}.__walk__", error=repr(e), items=[]))

    return reports

# ---------- AST Mode (no import) ----------
import ast

class AstVisitor(ast.NodeVisitor):
    def __init__(self, include_private: bool):
        self.include_private = include_private
        self.items: List[Item] = []

    def visit_FunctionDef(self, node: ast.FunctionDef):
        if should_keep(node.name, self.include_private):
            sig = "(parameters unknown via AST)"
            # best-effort build signature string
            try:
                params = []
                for a in node.args.args:
                    params.append(a.arg)
                if node.args.vararg:
                    params.append("*" + node.args.vararg.arg)
                for a in node.args.kwonlyargs:
                    params.append(a.arg)
                if node.args.kwarg:
                    params.append("**" + node.args.kwarg.arg)
                sig = "(" + ", ".join(params) + ")"
            except Exception:
                pass
            doc = ast.get_docstring(node) or ""
            head = " ".join((doc.strip().splitlines() or [""])[:2]).strip()
            self.items.append(Item(
                name=node.name,
                kind="function",
                signature=sig,
                doc_head=head,
                defined_in=None,
                line_no=getattr(node, "lineno", None)
            ))
        self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef):
        if should_keep(node.name, self.include_private):
            # Constructors' signatures are not trivial from AST; omit
            doc = ast.get_docstring(node) or ""
            head = " ".join((doc.strip().splitlines() or [""])[:2]).strip()
            self.items.append(Item(
                name=node.name,
                kind="class",
                signature="(inspect via import for full signature)",
                doc_head=head,
                defined_in=None,
                line_no=getattr(node, "lineno", None)
            ))
        self.generic_visit(node)

def is_package_dir(path: str) -> bool:
    return os.path.isdir(path) and os.path.isfile(os.path.join(path, "__init__.py"))

def walk_ast(pkg_name: str) -> List[ModuleReport]:
    # Locate the package directory by importing just to find __file__ (not executing submodules)
    reports: List[ModuleReport] = []
    try:
        mod = importlib.import_module(pkg_name)
        base_file = getattr(mod, "__file__", None)
        if not base_file:
            return [ModuleReport(module=pkg_name, error="Cannot locate package file", items=[])]
        base_dir = os.path.dirname(base_file)
    except Exception as e:
        return [ModuleReport(module=pkg_name, error=f"Could not import root to locate files: {e!r}", items=[])]

    # Walk the directory for .py files
    for root, dirs, files in os.walk(base_dir):
        # Respect package boundaries (skip hidden dirs, __pycache__)
        dirs[:] = [d for d in dirs if not d.startswith(".") and d != "__pycache__"]
        for f in files:
            if not f.endswith(".py"):
                continue
            full = os.path.join(root, f)
            rel = os.path.relpath(full, base_dir)
            # Compute module name
            modname_parts = [pkg_name] + rel[:-3].split(os.sep)  # strip .py
            if modname_parts[-1] == "__init__":
                modname_parts = modname_parts[:-1]
            module_name = ".".join(modname_parts)

            try:
                with open(full, "r", encoding="utf-8") as fh:
                    src = fh.read()
                tree = ast.parse(src, filename=full)
                visitor = AstVisitor(include_private=args.include_private)
                visitor.visit(tree)
                # attach file path for each item
                for it in visitor.items:
                    it.defined_in = full
                reports.append(ModuleReport(module=module_name, items=visitor.items))
            except Exception as e:
                reports.append(ModuleReport(module=module_name, error=repr(e), items=[]))

    return reports

def to_markdown(meta: Dict[str, Any], reports: List[ModuleReport]) -> str:
    header = [
        f"# Package Catalog: `{meta.get('package', 'unknown')}`",
        "",
        f"- **Distribution**: {meta.get('distribution', 'unknown')}  ",
        f"- **Version**: {meta.get('version', 'unknown')}  ",
        f"- **Location**: {meta.get('package_dir', meta.get('location', 'unknown'))}",
        "",
        "---",
        ""
    ]
    parts = ["\n".join(header)]
    for rep in sorted(reports, key=lambda r: r.module):
        parts.append(f"## Module: `{rep.module}`")
        if rep.error:
            parts.append(f"> ❗ Error: `{rep.error}`\n")
            continue
        if not rep.items:
            parts.append("_No public functions/classes found._\n")
            continue
        parts.append("| Name | Kind | Signature | Doc (first line) | File | Line |")
        parts.append("|------|------|-----------|------------------|------|------|")
        for it in rep.items:
            sig = it.signature.replace("|", "\\|")
            doc = (it.doc_head or "").replace("|", "\\|")
            file_disp = (it.defined_in or "").replace("|", "\\|")
            line_disp = it.line_no if it.line_no is not None else ""
            parts.append(f"| `{it.name}` | {it.kind} | `{sig}` | {doc} | {file_disp} | {line_disp} |")
        parts.append("")  # spacing
    return "\n".join(parts)

def main(args):
    pkg = args.package
    meta = get_package_metadata(pkg)

    if args.ast:
        reports = walk_ast(pkg)
    else:
        reports = walk_import(pkg, include_private=args.include_private)

    # Serialize JSON
    j = {
        "meta": meta,
        "reports": [asdict(r) for r in reports],
    }
    json_text = json.dumps(j, indent=2)

    # Build Markdown
    md_text = to_markdown(meta, reports)

    # Output to stdout
    if not args.quiet:
        print(json_text)

    # Optional file saves
    if args.json:
        with open(args.json, "w", encoding="utf-8") as f:
            f.write(json_text)
    if args.md:
        with open(args.md, "w", encoding="utf-8") as f:
            f.write(md_text)

    # If user only wants Markdown on stdout
    if args.markdown_stdout:
        sys.stdout.write(md_text)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Scan a Python package (default ent_pycore) and produce JSON/Markdown catalogs."
    )
    parser.add_argument("--package", "-p", default="ent_pycore", help="Package name to scan")
    parser.add_argument("--ast", action="store_true",
                        help="Use AST-only mode (no imports, safer, less precise signatures)")
    parser.add_argument("--include-private", action="store_true",
                        help="Include names starting with underscore")
    parser.add_argument("--json", help="Path to save JSON output")
    parser.add_argument("--md", help="Path to save Markdown output")
    parser.add_argument("--markdown-stdout", action="store_true",
                        help="Print Markdown catalog to stdout (in addition to JSON unless --quiet)")
    parser.add_argument("--quiet", action="store_true",
                        help="Do not print JSON to stdout")
    args = parser.parse_args()
    main(args)
```

---

### Tips for big distributions

* If importing submodules triggers heavy side-effects, use **`--ast`** first to get the structure without execution, then selectively re-run **without `--ast`** for the few modules you care about (use `--package ent_pycore.subpkg` to narrow scope).
* For “namespace” packages or C-extensions, AST mode will skip compiled modules; import mode will list them (signatures may still be limited).

---

If you run this and share either **`out.json`** or **`out.md`**, I’ll turn it into:

* a **Function Catalog**,
* a **Category → Use Case** map,
* and **production-ready recipes** (e.g., circuit breaker, retries/backoff, caching, telemetry hooks) using `ent_pycore`.


2:11 PM
>>>>>>>>>>>>>>>>>

Got it — let’s break this down systematically.

You’re writing a **zero-shot instruction prompt** whose goal is:
➡️ Guide the most capable AI agent to deeply analyze the internal Python library `ent_pycore`, understand its functions, and propose logic/designs that can be built using it.

---

## 🔎 Analysis of Your Current Prompt

**Your version:**

> Pip install ent_pycore.
> We have ent_pycore inbuilt library in our org, and I want to analyse all the functions in the library ent_pycore and help me to achieve this technically and see what all logic is possible with the internaly python library.
> Add if I have missed anything like Like-a Google engineer do or someone work in billion dollar startup.
> And I want you to think like high performing software engineer.

### Characteristics

1. **Direct but unstructured** – you’re stating the goal, but not guiding *how* the agent should analyze, organize, or present results.
2. **High ambition tone** – asking it to “think like a Google engineer / billion-dollar startup” sets quality expectations, but it’s vague.
3. **Task ambiguity** – "analyze all functions" could mean: generate documentation, usage examples, dependencies, design best practices, or even building new patterns (circuit breaker, retry, etc.).
4. **Missing structured output request** – you don’t specify how the answer should be organized (tables, code snippets, summaries, examples).
5. **No reasoning chain request** – you want deep reasoning, but the current text doesn’t explicitly demand step-by-step thought.

---

## ⚡ Suggestions to Improve Prompt

Here’s how to make it sharper for **zero-shot instruction tasks**:

### 1. Explicit Task Breakdown

Instead of just “analyze all functions,” tell the agent *how* to analyze:

* Extract all function names and signatures.
* Categorize by purpose (e.g., networking, retry, circuit-breaker, monitoring).
* Show possible use cases.
* Suggest best practices for enterprise-scale systems.

### 2. Require Reasoning Chain

Ask the agent to:

* Explain reasoning behind classification.
* Compare different function design patterns.
* Show why a certain function is suitable for a use case.

### 3. Structured Output Format

Specify you want output in sections/tables, for clarity:

* **Function Catalog** (name, signature, short description).
* **Category/Use Case Mapping**.
* **Design Patterns Possible** (e.g., circuit breaker, retry logic, caching).
* **Code Examples** (with explanations).
* **Improvement Suggestions** (what is missing, how a Google-level engineer would extend it).

### 4. Strengthen the "high-performance engineer" framing

Instead of “think like Google engineer,” say:

> “Think like a senior engineer at a FAANG-scale company: prioritize scalability, fault-tolerance, and clean abstractions.”

---

## 📝 Refined Prompt (Improved Version)

Here’s a stronger **zero-shot instruction prompt** you could use:

---

**Prompt:**
You are acting as a **high-performing senior software engineer at a FAANG-scale company**.

We have an internal Python library called **`ent_pycore`**. Your task is to:

1. **Analyze all functions** in the library and extract their purpose, signatures, and dependencies.
2. **Categorize functions** by use case (e.g., retries, error handling, monitoring, networking, data processing).
3. **Propose enterprise-grade design patterns** (e.g., circuit breaker, retry with backoff, observability hooks) that can be built using these functions.
4. **Show code examples** that demonstrate how to combine these functions into robust, production-ready solutions.
5. **Explain reasoning step-by-step**, highlighting trade-offs and why a design is scalable or resilient.
6. **Suggest improvements** or missing capabilities — what else a world-class engineering team would add.

**Output format:**

* Section 1: Function Catalog (table of functions with short description).
* Section 2: Categorization & Use Cases.
* Section 3: Proposed Designs & Patterns (with code).
* Section 4: Step-by-Step Reasoning.
* Section 5: Improvement & Future Enhancements.

---

👉 This way, the agent knows:

* the **goal** (analyze & apply functions),
* the **method** (structured reasoning),
* the **output style** (tables + code + explanation),
* the **quality bar** (world-class engineer thinking).

---

Would you like me to **rewrite your exact original wording** into this improved form (keeping your style but sharpening structure), or do you prefer a **more concise minimalistic version** you can drop directly into your system as a one-liner?


Short answer: I can’t install your internal package here or reach your private registry. I also can’t see `ent_pycore` unless you share the code (repo/zip/wheel) or paste docs/stubs.

That said, I can still help in two solid ways:

## Option A — You provide artifacts, I analyze now

Share **any one** of these and I’ll proceed immediately:

* The repo (or a zip/tarball) of `ent_pycore`
* A wheel (`.whl`) or sdist (`.tar.gz`)
* Generated docs or `.pyi` stubs
* Even a single module you care about most

### My analysis plan (once I have it)

1. **Inventory:** Parse modules → list functions/classes, signatures, docstrings.
2. **Categorize:** Group by capability (I/O, retries, CB, caching, telemetry, auth, utils).
3. **Pattern map:** Identify what enterprise patterns are already enabled (circuit breaker, exponential backoff, bulkhead, idempotency, tracing) and what’s missing.
4. **Usage recipes:** Produce production-grade examples (with error handling, logging, metrics, configuration via env/flags, test doubles).
5. **Gaps & proposals:** Concrete PR-style suggestions (APIs to add, breaking-change risks, test matrix, SLO/observability hooks).

I’ll deliver:

* A **Function Catalog table**
* **Design patterns** you can implement today using `ent_pycore`
* **Sample code** (prod-ready) + **pytest** tests
* A short **“What a FAANG-level team would add next”** section

## Option B — Run this locally, then paste results back

If you can run Python in your environment, this script will generate a JSON inventory I can analyze without needing the code itself:

```python
# save as scan_ent_pycore.py and run: python scan_ent_pycore.py
import importlib, inspect, json, pkgutil, sys

PKG = "ent_pycore"
mod = importlib.import_module(PKG)

def walk(pkg_name):
    seen = set()
    for m in pkgutil.walk_packages(mod.__path__, mod.__name__ + "."):
        name = m.name
        if name in seen: 
            continue
        seen.add(name)
        try:
            module = importlib.import_module(name)
        except Exception as e:
            yield {"module": name, "error": repr(e)}
            continue
        items = []
        for n, obj in inspect.getmembers(module):
            if inspect.isfunction(obj) or inspect.isclass(obj):
                try:
                    sig = str(inspect.signature(obj))
                except Exception:
                    sig = "(signature unavailable)"
                doc = (inspect.getdoc(obj) or "").splitlines()[0:3]
                items.append({
                    "name": n,
                    "kind": "class" if inspect.isclass(obj) else "function",
                    "signature": sig,
                    "doc_head": " ".join(doc)
                })
        yield {"module": name, "items": items}

result = list(walk(PKG))
print(json.dumps(result, indent=2))
```

* Run it where `ent_pycore` is installed.
* Paste the JSON output here. I’ll turn it into the Function Catalog + patterns + code examples.

## Option C — Prompt for an internal agent that *can* install

If you have an internal runner/agent with network access, use this zero-shot prompt there:

> **Role:** Senior Staff Engineer.
> **Task:** Install and analyze the internal Python library `ent_pycore`.
> **Steps:**
>
> 1. `pip install ent_pycore` (using the corporate index if required).
> 2. Enumerate modules, functions, classes, and signatures; capture first-line docstrings.
> 3. Categorize capabilities (retries/backoff, circuit breaker, caching, I/O, telemetry, auth, utils).
> 4. Propose enterprise patterns implementable with current APIs (circuit breaker, bulkhead, idempotency, request hedging, structured logging/metrics/traces).
> 5. Produce production-ready Python examples + pytest tests for each pattern.
> 6. Identify gaps and propose API changes (backwards-compatible where possible), include observability & configurability (env vars/flags).
>    **Output format:**
>
> * Section 1: Function Catalog (table: module, name, kind, signature, doc_head).
> * Section 2: Category → Functions mapping.
> * Section 3: Design patterns with code + tests.
> * Section 4: Reasoning & trade-offs.
> * Section 5: Gaps & roadmap (MVP, nice-to-have, risks).
>   **Quality bar:** FAANG-scale reliability, clarity, and testability; align with SLOs, tracing, and error-budget thinking.

---

Pick the path that fits your environment. If you can share the repo/zip or the JSON inventory, I’ll dive straight into the analysis and produce the catalog, designs, and code.




9:38
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Got it—your goal is a **zero-shot, do-it-all prompt** that reliably guides a capable agent to (1) understand your repo + `ent_pycore`, (2) design a production-grade **circuit breaker** for critical LLM calls, and (3) deliver code + tests + rationale at a “Google-ish/billion-$ startup” bar.

Below is a tight upgrade: first, a quick critique so you know what’s missing, then a **drop-in prompt template** you can paste into your system with strong structure, guardrails, and outputs.

---

# What to improve (fast critique)

1. **Ambiguity about inputs & scope**
   “30+ scripts” and “analyze all functions” is open-ended. Agents wander. Anchor scope: target directories, file patterns, key entrypoints.

2. **Tooling/Capabilities not explicit**
   If the agent can inspect files, run code, or only reason statically? Say it. If it must infer `ent_pycore` APIs, tell it how (read docs, grep usage).

3. **Outcome format not enforced**
   You want design, code, and tests. Force **structured sections** + acceptance criteria.

4. **Prod-readiness criteria missing**
   Define SLIs/SLOs, failure modes, fallback policy, telemetry, config flags, and rollback plan.

5. **Test strategy underspecified**
   Require unit + integration tests (happy/timeout/error/open/half-open/closed), fault injection, and deterministic seed.

6. **Reviewability**
   Ask for a **PR-style patch** (diffs), `README.md` updates, and CHANGELOG notes.

---

# Upgraded Zero-Shot Prompt (paste this as your instruction)

> **Role**: You are a senior platform/SRE-minded software engineer designing a production-grade **circuit breaker** for mission-critical LLM API calls. Think rigorously, write reviewable code, and justify decisions with measurable reliability goals.
>
> **Context**:
>
> * Codebase: ~30+ scripts in a GitHub repo (Python).
> * Internal library: `ent_pycore` (installed via `pip install ent_pycore`). Treat it as first-class; prefer its primitives over ad-hoc code.
> * Objective: Protect user experience during LLM outages/timeouts while meeting reliability goals and preserving throughput under normal load.
>
> **Repository Scope to Analyze**:
>
> * Focus on files under `src/` and `services/` containing LLM calls (search terms: `openai`, `llm`, `completion`, `chat`, `inference`, `client.*create`, `requests.post` to model endpoints).
> * Identify: call sites, retry logic, timeouts, error handling, and any existing rate limiting/backoff.
> * Map the call-graph for top 2 critical entrypoints (by traffic or business priority).
>
> **What you must deliver** (strict structure):
>
> 1. **Executive Summary (≤10 bullets)**
>
>    * Problem statement, constraints, proposed pattern (circuit breaker), expected impact.
> 2. **Requirements & Reliability Goals**
>
>    * Define SLIs (success rate, end-to-end latency p90/p95, fallback hit-rate).
>    * Propose SLOs (e.g., success ≥ 99.5%, p95 latency ≤ 2.5s under nominal load).
>    * Non-functionals: observability, configurability, blast-radius limits.
> 3. **Design (with diagrams-in-text)**
>
>    * Circuit breaker states (Closed, Open, Half-open) and transitions.
>    * **Trip conditions** (e.g., rolling window error-rate > X%, consecutive timeouts > N, upstream 5xx > M).
>    * **Recovery probe** policy (half-open sample size, jittered probe interval).
>    * **Fallbacks** in priority order (cached response → narrowed model → stub/template → user-facing graceful degradation).
>    * **Configuration** (env vars or config file): timeouts, thresholds, window size, max concurrent probes, allowlist endpoints.
>    * **Thread/async model** and concurrency safety.
> 4. **Implementation Plan (PR-Checklist)**
>
>    * New module(s) (paths, filenames) and changes to existing files.
>    * Backward compatibility and kill-switch/feature flag (`CB_ENABLED`).
>    * Telemetry events (names, fields) and log levels.
> 5. **Code**
>
>    * A self-contained `circuit_breaker.py` (or package) using **`ent_pycore`** where appropriate.
>    * Adapters/wrappers for current LLM clients (minimal surface change at call sites).
>    * Config loader + sane defaults.
> 6. **Tests**
>
>    * Unit tests covering: Closed→Open, Open→Half-open, Half-open→Closed; timeout bursts; mixed failures; fallback correctness; concurrency.
>    * Integration test with a stubbed LLM endpoint (fast, deterministic).
>    * Property-style test for rolling-window math.
> 7. **Operations**
>
>    * Dashboards (metrics list), alerts (burn-rate style), SLO doc snippet.
>    * Runbook: how to enable/disable, tune thresholds, interpret metrics, rollback plan.
> 8. **STAR Rationale**
>
>    * Situation, Task, Actions (trade-offs), Results (expected metrics deltas, e.g., p95 ↓, error budget burn ↓).
> 9. **Patch Output**
>
>    * Present changes as a **single PR-style diff** (filenames + unified diff).
>    * Include `README.md` section “Circuit Breaker Overview” and `CHANGELOG.md` entry.
>
> **Constraints & Guardrails**:
>
> * **No breaking public APIs** unless adapter provided.
> * Prefer **idempotent** operations; document any non-idempotent side-effects.
> * All constants are configurable (env or `config.yaml`), with safe defaults.
> * Add **metrics** (counter/gauge/histogram) and **structured logs** at state transitions.
> * Write code in **Python 3.10+**, type-annotated, lint-clean (ruff/flake8), and documented (docstrings).
>
> **Assumptions re: `ent_pycore`** (adapt if real APIs differ):
>
> * If `ent_pycore` has utilities for:
>
>   * **circuit breakers**, **rate limiters**, **rolling windows**, **metrics/logging**, or **feature flags**, **use them** and show how.
>   * If unknown, **introspect by reading its package modules** and **grep existing repo usage**; then state the chosen functions/classes and why.
>
> **Exact Output Format** (important):
>
> * Use the section headers 1–9 above.
> * All code blocks must be runnable and complete.
> * Tests must run via `pytest -q`. Provide a short command list to execute locally.
>
> **Acceptance Criteria** (the work is “done” when):
>
> * Unit + integration tests pass locally.
> * A demo script shows tripping and recovery behavior deterministically.
> * Metrics names + labels are listed and mapped to dashboards/alerts.
> * A kill-switch exists and works (`CB_ENABLED=false`).
> * Risk analysis + rollback steps are documented.

---

## Ready-to-use “micro” variant (if you need a shorter prompt)

> Design and implement a production-grade **circuit breaker** for our LLM calls in this Python repo, prioritizing: (1) reliability SLOs, (2) graceful degradation, (3) observability, (4) configurability. Analyze `src/` and `services/` for LLM call sites; prefer **`ent_pycore`** primitives. Deliver: (1) executive summary, (2) SLIs/SLOs, (3) design with states/thresholds/fallbacks, (4) PR-style patch (code + tests + config), (5) ops guide (metrics, alerts, runbook), (6) STAR rationale. Code must be typed, lint-clean, pytest-covered, include a kill-switch, and expose metrics + logs at state transitions.

---

## Optional code skeleton (helps the agent land consistently)

```python
# src/platform/circuit_breaker.py
from dataclasses import dataclass
from typing import Callable, Any, Tuple
import time
# from ent_pycore.observability import metrics, log
# from ent_pycore.controls import RollingWindow, FeatureFlag, CircuitState  # hypothetical

@dataclass
class CBConfig:
    error_rate_threshold: float = 0.25
    min_samples: int = 50
    rolling_window_sec: int = 60
    open_cooldown_sec: int = 20
    half_open_probe_count: int = 5
    request_timeout_sec: float = 8.0
    enabled: bool = True

class CircuitBreaker:
    # implement Closed/Open/Half-open with rolling window and timed cooldown
    ...
```

Tests should simulate: steady success, burst failures→trip, cooldown→probe→recover, and verify metrics/logs.

---

## Quick reasoning chain the agent should follow (baked into the structure above)

1. Identify hot paths → quantify failure/latency patterns.
2. Choose thresholds grounded in SLIs/SLOs.
3. Implement breaker with safe defaults + feature flag.
4. Wrap LLM calls; add fallbacks for degraded mode.
5. Add metrics/logs; write tests (unit/integration/fault-injection).
6. Provide PR diff + runbook + STAR results.

---

If you want, share a couple of filenames that actually call the LLM (e.g., `services/llm_client.py`, `src/handlers/generate.py`) and I’ll tailor the design thresholds and a concrete wrapper that uses your real `ent_pycore` primitives.


9:15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

awesome problem. here’s a sharper, production-ready way to brief a capable agent—plus a filled-in version for your two use cases, and a quick sanity checklist.

# 1) what’s missing in your current prompt (and why it matters)

* **Clear objective & scope**: say exactly what “done” looks like (PRs, tests, dashboards, runbook).
* **Assumptions policy**: forbid clarifying questions; require the agent to state assumptions & proceed.
* **Non-functional constraints**: SLOs, latency budgets, token/cost budgets, concurrency limits, PII/compliance.
* **Observability & rollback**: metrics, alerts, chaos tests, feature flags, rollback steps.
* **Testing matrix**: unit, integ, fault-injection, load, resilience.
* **STAR narrative**: ask for an explicit Situation→Task→Action→Result write-up.
* **Review criteria**: a checklist/rubric the PE team can use to accept/reject.
* **Change safety**: canary plan, blast radius, and backout commands.

---

# 2) zero-shot instruction prompt (template)

> copy/paste and fill <> where needed. this keeps the agent decisive and output-oriented.

**ROLE & MINDSET**
Act as a senior software engineer/SRE at a high-scale company. Do not ask questions. If information is missing, make conservative, clearly labeled assumptions and proceed.

**OBJECTIVE**
Design and implement a **circuit breaker** for the specified services to protect user experience, control LLM spend, and prevent cascading failures. Deliver production-grade code and an end-to-end plan.

**CODEBASE CONTEXT**

* Repo: `<org>/<repo>` (monorepo; primary languages: `<languages>`)
* Services in scope: `<service names / paths>`
* Environment: Dev/UAT/Prod with feature flags via `<flag system>`
* Dependencies: LLM API `<provider(s)>`, DB `<type>`, queue `<type>`

**CONSTRAINTS & SLOs**

* User-facing latency P95 ≤ `<X ms>`; error budget `<Y %>` per 30 days
* LLM spend: max `<tokens/min>` and `<$/day>`; shed load if exceeded
* Timeouts: `<client timeout>`; retries: capped with exponential backoff + jitter
* Concurrency caps: `<value>` per instance; global rate limit `<value>`
* PII/compliance: no PII in logs; enable structured logging

**DELIVERABLES**

1. **Design doc** (≤2 pages) with: state machine, thresholds, fallback paths, backpressure, and recovery policy. Include Mermaid diagram.
2. **Implementation**: PR(s) that add circuit breaker middleware/util(s) + configuration (env-driven).
3. **Tests**: unit + integration + fault-injection (simulate timeouts, 5xx, time-window failure %).
4. **Observability**: metrics names, dashboards, and alert rules.
5. **Runbook**: ops steps, feature-flag toggles, rollback commands.
6. **STAR** summary of the work (Situation, Task, Actions, Results).
7. **Review note for PE**: risks, tradeoffs, and how to validate locally & in canary.

**EVALUATION / ACCEPTANCE CRITERIA**

* Breaker transitions correctly (Closed→Open→Half-Open) using rolling window failure rate or consecutive errors.
* No request storms: retries are bounded, with jitter; concurrency & rate limits enforced.
* For write-paths: durability guaranteed (queue + idempotency key); zero data loss on transient DB outage.
* Observability: metrics and alerts fire in controlled chaos tests; dashboards show breaker state & rates.
* Cost guardrails: breaker opens on spend spikes or token rate breach.
* All tests pass; canary shows ≤ `<delta>` regression on latency/error rate.

**PROCESS**

* Make and list assumptions.
* Produce: (a) design doc, (b) code diff, (c) tests, (d) ops assets, (e) STAR summary.
* Output everything inline, with code blocks and filenames.

---

# 3) filled-in prompt for your two use cases

**ROLE & MINDSET**
Act as a senior SWE/SRE. Do not ask questions. If details are missing, assume safe defaults and record them.

**IN-SCOPE USE CASES**
A) **LLM Completion (read path)** – Core UX; timeouts/outages break workflows and burn error budget.
B) **Feedback/Analytics Write (write path)** – Compliance/audit-critical; DB outages risk data loss.

**OBJECTIVE**
Design and implement circuit breakers for A & B that:

* protect UX and SLOs,
* cap cost & token usage,
* prevent cascading failures,
* preserve/queue writes and alert on failures.

**CODEBASE**

* Repo: `<org>/genai-platform`
* Services: `api/llm_gateway` (TypeScript/Node) and `svc/telemetry_writer` (Python/FastAPI)
* Env: Dev/UAT/Prod; feature flags via `FF_CIRCUIT_BREAKER_{LLM,WRITE}`

**KEY CONSTRAINTS**

* SLOs: P95 latency ≤ 1200 ms for LLM completions; monthly availability ≥ 99.9%
* LLM budgets: ≤ 60k tokens/min per tenant; daily spend cap `<$>` configurable
* Timeouts: upstream LLM 2.0 s; client 2.5 s; retry: max 2 with exp backoff (100–600 ms) + jitter
* Concurrency: 64 per instance; global qps per tenant set via token bucket
* DB writes: at-least-once via durable queue (e.g., SQS/Kafka) with idempotency keys
* No PII in logs; trace with `trace_id`, `tenant_id`, `breaker_state`

**DELIVERABLES**

1. **Design doc** (≤2 pages) including Mermaid diagrams of both breakers.
2. **Code**

   * `api/llm_gateway`: CircuitBreaker middleware (Closed/Open/Half-Open), fallback: cached/template response for non-critical prompts, plus shed-load when cost/token budgets breach.
   * `svc/telemetry_writer`: CircuitBreaker guarding DB client; on Open, enqueue to durable queue; consumer drains with idempotency & DLQ.
3. **Tests**

   * Unit: state transitions, rolling window failure %, budget gating, retry caps.
   * Integration: fault-injection (LLM 5xx/timeout spikes; DB connection refused).
   * Load/chaos: verify alerts, ensure no thundering herd, measure P95.
4. **Observability** (Prometheus/Grafana or equivalent)

   * Metrics: `breaker_state{service}`, `breaker_open_total`, `failure_rate`, `timeout_total`, `retry_total`, `queued_writes`, `dlq_depth`, `token_rate`, `spend_rate_usd_per_min`
   * Alerts:

     * LLM: breaker Open > 1 min OR failure_rate ≥ 50% over 2 min OR token_rate/spend breach for 3 consecutive mins.
     * Writes: queue lag > 2 min OR DLQ depth > 0 OR Open > 30 s.
5. **Runbook**: flag toggles, raising limits, draining DLQ, recovering DB, backout steps.
6. **STAR**: concise narrative of the problem and results.
7. **PE Review Note**: risk, tradeoffs (e.g., hedging cost vs latency), and validation commands.

**ACCEPTANCE CRITERIA**

* LLM: Under induced 50% upstream failure for 2 min, breaker opens within 5–10 s, caps retries, P95 ≤ 1.5× baseline, error rate ≤ budget, spend spike is contained.
* Writes: On DB outage for 5 min, 0 lost records; queue builds then drains to 0 within 10 min post-recovery; duplicates suppressed via idempotency key.
* All tests pass; dashboards accurate; alerts verified in chaos runs.

**OUTPUT FORMAT**
Produce in this order:

1. Assumptions list
2. Design doc (with Mermaid diagrams)
3. Code diffs (filenames + code blocks)
4. Test code & how to run
5. Observability spec (metrics, dashboards JSON if applicable)
6. Runbook & rollback
7. STAR summary
8. PE review checklist mapping

---

# 4) pocket designs (for the agent to implement)

**LLM breaker (TypeScript sketch)**

```ts
// api/llm_gateway/middleware/circuitBreaker.ts
type State = "CLOSED" | "OPEN" | "HALF_OPEN";
export function createBreaker(opts: {
  failureThresholdPct: number; // e.g., 50 over window
  windowSecs: number;          // e.g., 20
  openCooldownMs: number;      // e.g., 5000
  halfOpenProbe: number;       // e.g., 5 requests
  maxRetries: number;          // e.g., 2 w/ jittered backoff
  tokenBudgetPerMin: number;   // per-tenant
  spendBudgetPerMinUSD: number;
}) { /* implement rolling counters, budgets, and state machine */ }
```

**Write-path breaker + queue (Python sketch)**

```python
# svc/telemetry_writer/breaker.py
class CircuitBreaker:
    # CLOSED -> OPEN (on failure rate or consecutive errors)
    # OPEN -> HALF_OPEN after cooldown
    # HALF_OPEN -> CLOSED on probe success; else OPEN
    ...
# On OPEN: push record to durable queue with idempotency_key = sha256(tenant_id|event_id|ts)
# Consumer: reads queue, writes with upsert on idempotency_key; on permanent failure -> DLQ.
```

**Mermaid (example)**

```mermaid
stateDiagram-v2
    [*] --> CLOSED
    CLOSED --> OPEN: failure_rate>=threshold OR budget_breach
    OPEN --> HALF_OPEN: cooldown elapsed
    HALF_OPEN --> CLOSED: probeSuccess>=N
    HALF_OPEN --> OPEN: probeFailure>0
```

---

# 5) review checklist (paste into PR)

* [ ] Assumptions documented; no open questions.
* [ ] Breaker thresholds, windows, cooldowns are config-driven.
* [ ] Retries capped + jitter; no unbounded fan-out.
* [ ] Cost/token budgets enforced; load-shedding in place.
* [ ] Write-path durability: queue, idempotency, DLQ; zero data loss in tests.
* [ ] Metrics/alerts wired; dashboards render breaker state & lag.
* [ ] Chaos/fault-injection scripts included; alerts verified.
* [ ] Feature-flagged; canary & rollback steps documented.

If you want, I can now generate the exact design doc + code stubs + tests following this prompt format.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Awesome brief. Here’s a battle-tested, zero-shot prompt you can drop into your agent—plus add-ons and a tiny example of the expected answer shape. It’s written to elicit Google-level design depth without forcing the model to reveal private reasoning.

---

# Zero-Shot Instruction Prompt (Final)

**Role & Context**
You are a principal software engineer designing **production-grade circuit breakers** for a large LLM application with ~30 Python scripts (micro-tools, services, CLIs). Assume an elite engineering bar (Google/billion-dollar startup).

**Inputs**

* Repository snapshot (files, tree, or links)
* Runtime context: Python 3.x, Streamlit/FastAPI workers, LLM provider(s) (e.g., Gemini/OpenAI), pptx rendering, caching/queue layers (if any)

**Primary Objective**

1. **Discover the top two business-critical use cases** in the repo and justify why they’re top priority (traffic, revenue/user impact, SLO/SLA/error-budget risk, fan-out).
2. **Design circuit breakers for each use case** with detailed, production-ready guidance.

**Deliverables (must follow exactly)**
A. **Bird’s-eye map (1–2 pages max)**

* System diagram of major scripts/services and call graph to external dependencies (LLM API, storage, DB, network).
* Hot paths and shared libraries that concentrate risk.
* Error taxonomy: timeouts, 5xx/429, transport errors, parser/JSON errors, quota/exhaustion, idempotency risks.

B. **Top-2 Use Cases (selection rationale)**

* Table with: UseCase, CriticalityReason, SLI/SLO at risk (p95 latency, success rate), Current Fail Modes, Estimated Blast Radius.

C. **Circuit Breaker Design per Use Case (deep dive)**
For each use case, provide:

1. **Trip conditions**

   * Sliding window size (N calls or T seconds), failure-rate threshold (%), consecutive-error threshold, and latency p95/p99 threshold.
   * Error classes included/excluded (e.g., count 5xx/timeout; exclude 4xx user errors).
2. **States & transitions**

   * Closed → Open → Half-Open with probe counts, decay, cool-down.
3. **Retry/backoff**

   * Max attempts, exponential backoff + full jitter, per-call **retry budget** and **global budget** to protect upstream.
4. **Fallbacks & graceful degradation**

   * LLM: cached answer/template, smaller/cheaper model, summarization mode, or “draft + user confirm”.
   * PPT build: minimal theme, skip images, reduce slide count.
5. **Idempotency & dedupe**

   * Idempotency keys, request hashing for safe retries.
6. **Observability**

   * Metrics: `circuit_state{usecase}`, `trips_total`, `rejections_total`, `probe_success_total`, `fallback_invocations`, `retry_attempts`.
   * Logs: structured, no PII; Traces: span links across retries/fallbacks.
7. **Configuration & rollout**

   * Config-as-code defaults + dynamic overrides (env/flags/config service).
   * Canary, feature flag, blast radius controls.
8. **Security & privacy**

   * Don’t log prompts/outputs; guardrails for prompt injection; secrets via env/secret manager.
9. **Testing plan**

   * Unit (fakes), fault-injection (timeouts/5xx), load + chaos (latency spikes), game-day playbook.
10. **Code artifacts**

* **Python pseudo-code** for the breaker wrapper, plus a minimal **reference implementation** using either `pybreaker` or `tenacity` for retries and a custom state store (in-memory → Redis for multi-worker).
* Integration points (decorators/middleware) and how to apply to the two hot paths without invasive changes.

D. **STAR Write-ups (one per use case)**
Use **S**ituation, **T**ask, **A**ctions (design choices, trade-offs, numbers), **R**esults (SLO improvement, error-budget burn reduction, cost/latency deltas). Keep to 8–12 bullets each.

**Style & Constraints**

* Be precise; no generic advice. Provide concrete thresholds, windows, and example numbers with rationale.
* Show reasoning **via short justifications**—do not output private chain-of-thought.
* Output sections **A–D** in order, with clear headings, tables where appropriate, and code blocks for pseudo-code.

**Repository Snapshot**

```
[Paste tree or list of scripts here]
```

---

## Add-Ons You Can Toggle (optional, if relevant)

* **Multi-tenant isolation:** per-tenant breaker state to prevent noisy-neighbor effects.
* **Client vs. server breaker:** justify placement (SDK/client, gateway, or server-side middleware).
* **Quota-aware breaker:** separate trip logic for 429s vs 5xx.
* **Adaptive thresholds:** increase/open faster during incident (burn-rate alerts).
* **Cost guardrails:** automatic downgrade to cheaper LLM/model when breaker is half-open.

---

## Example Answer Shape (concise sample for one LLM call path)

**Trip conditions**

* Window: last **200** calls or **60s**, whichever first.
* Trip if **failure_rate ≥ 15%** (timeouts, 5xx, DNS, TLS) **OR** if **p95 latency > 4s** for 3 consecutive windows **OR** **≥5** consecutive timeouts.

**States**

* **Open:** 30s cool-down (flag + timestamp).
* **Half-Open:** allow 5 probes at 2s spacing; close on ≥4/5 success with p95 < 2.5s; else reopen 60s.

**Retry & Backoff**

* Max 2 retries per call, **exponential backoff with full jitter**: 300–700ms, then 600–1400ms. Global retry budget 10% of RPS.

**Fallbacks**

* Serve cached slide outline (24h TTL) or **fallback to smaller model**; if both fail, render minimal PPT with headers and placeholders.

**Observability**

* Metrics: `circuit_state{usecase="llm_generate"}`, `trips_total`, `fallback_invocations`, `probe_success_total`.
* Trace: one span per attempt, link retries.

**Config**

* Env defaults, dynamic overrides via `CONFIG_BREAKERS.yaml` or Redis. Feature flag: `cb_llm_generate_v1`.

**Python wrapper (pseudo-code)**

```python
def with_breaker(usecase):
    def deco(fn):
        def wrapped(*args, **kwargs):
            state = breaker_store.get_state(usecase)
            if state.is_open() and not state.allow_probe():
                metrics.inc("rejections_total", usecase)
                return fallback(usecase, *args, **kwargs)

            try:
                resp = retry_with_backoff(fn, *args, **kwargs)
                breaker_store.record_success(usecase, latency_ms(resp))
                return resp
            except TransientUpstreamError as e:
                breaker_store.record_failure(usecase, e)
                if breaker_store.should_trip(usecase):
                    breaker_store.open(usecase)
                    alert("circuit_tripped", usecase)
                return fallback(usecase, *args, **kwargs)
        return wrapped
    return deco
```

---

## Why this prompt works

* **Concrete outputs** (A–D) prevent hand-wavy answers.
* **Numbers + thresholds** force engineering rigor.
* **STAR** ensures stakeholder-friendly reporting.
* **Privacy note** avoids chain-of-thought leakage while still demanding justifications.
* **Code + rollout** makes it immediately actionable.

If you want, I can adapt this to your **ppt-ai-generator** repo specifically (LLM slide generation + PPT build as the two hot paths) and pre-fill suggested thresholds and metrics.
